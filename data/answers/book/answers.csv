id,answer
1,"Choose $\mathbf{x} = (1,0,1)^T$ and compute $\mathbf{u} = \mathbf{x} + \alpha \mathbf{e}_1$, where $\alpha = -\text{sign}(1) \cdot \|\mathbf{x}\| = -\sqrt{2}$. Construct
\[
\mathbf{u} = \mathbf{x} + \alpha \mathbf{e}_1 = (1,0,1)^T - (\sqrt{2},0,0)^T = (-0.4142,0,1)^T.
\]
Construct $\mathbf{v} = \frac{\mathbf{u}}{\|\mathbf{u}\|}$. Therefore
\[
\mathbf{v} = (-0.3827,0,0.9239)^T.
\]
Compute
\[
Q_1 = I - 2\mathbf{v}\mathbf{v}^T = \begin{pmatrix} 
0.7071 & 0 & 0.7071 \\ 
0 & 1 & 0 \\ 
0.7071 & 0 & -0.7071 
\end{pmatrix}.
\]
Then compute
\[
A_1 = Q_1 A = \begin{pmatrix} 
1.4142 & 0.7071 \\ 
0 & 1 \\ 
0 & -0.7071 
\end{pmatrix}.
\]

Now choose a new vector $\mathbf{x} = (1, -0.7071)^T$ and compute $\alpha = -\text{sign}(1) \cdot \|\mathbf{x}\| = -1.2247$. Construct
\[
\mathbf{u} = \mathbf{x} + \alpha \mathbf{e}_1 = (1, -0.7071)^T - (1.2247, 0)^T = (-0.2247, -0.7071)^T.
\]
Then normalize:
\[
\mathbf{v} = \frac{\mathbf{u}}{\|\mathbf{u}\|} = (-0.3029, -0.9530)^T.
\]
Now compute
\[
V = I - 2\mathbf{v}\mathbf{v}^T = \begin{pmatrix} 
1 - 2(0.3029)^2 & -2(0.3029)(0.9530) \\
-2(0.3029)(0.9530) & 1 - 2(0.9530)^2 
\end{pmatrix}
= \begin{pmatrix}
0.8165 & -0.5774 \\
-0.5774 & -0.8165 
\end{pmatrix}.
\]

Construct the second matrix of the Householder transformation $V_1$ as:
\[ V_1 = \begin{pmatrix} 1 & 0 \\ 0 & V' \end{pmatrix} \]
to get
\[ V_1 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0.8165 & -0.5774 \\ 0 & -0.5774 & -0.8165 \end{pmatrix} \]
and then compute
\[ R = V_1 A_1 = \begin{pmatrix} 1.4142 & 0.7071 \\ 0 & 1.2247 \\ 0 & 0 \end{pmatrix} \]
We observe that $R = V_1 A_1$ is the matrix in the QR decomposition of the matrix $A$ with $Q = V_1^T V_2^T$:
\[ Q = V_1^T V_2^T = \begin{pmatrix} 0.7071 & -0.4082 & -0.5774 \\ 0 & 0.8165 & -0.5774 \\ 0.7071 & 0.4082 & 0.5774 \end{pmatrix} \]"
2,"By definition of the Inertia of $A$, we have:
\[
\text{Inertia}(A) = (\# \text{ negative eigenvalues of } A, \# \text{ zero eigenvalues of } A, \# \text{ positive eigenvalues of } A).
\]
Thus, we need to find eigenvalues of the matrix $A$, and to do this we solve the characteristic equation $\det(A - \lambda I) = 0$:
\[
\det\begin{pmatrix}
1 - \lambda & 0 & 1 \\
0 & 1 - \lambda & 0 \\
1 & 0 & 1 - \lambda \\
\end{pmatrix} = 0.
\]
Solving the above equation for $\lambda$ we get three eigenvalues:
\[
\lambda_1 = 0, \quad \lambda_2 = 1, \quad \lambda_3 = 2,
\]
which are solutions to the characteristic equation $(1 - \lambda)((1 - \lambda)^2 - 1) = 0$.
Thus, we have two positive eigenvalues and one zero eigenvalue, and therefore
\[
\text{Inertia}(A) = (\#\lambda_i < 0, \#\lambda_i = 0, \#\lambda_i > 0) = (0, 1, 2).
\]"
3,"To compute the upper triangular matrix of the matrix
\[ A = \begin{bmatrix} 1 & 0 & 1 \\ 1 & 1 & 0 \\ 0 & 1 & 1 \end{bmatrix}, \]
using Givens rotation, we perform the following steps.
To zero out element $A_{2,1} = 1$, we compute elements of the Givens matrix $G(j,k) = G(2,1)$. The Givens matrix (since $j > k$) will be
\[ G(j,k) = G(2,1) = \begin{bmatrix} c & -s & 0 \\ s & c & 0 \\ 0 & 0 & 1 \end{bmatrix}. \]
Values of $c, s$ are computed from the known $a = A(j,j) = 1$ and $b = A(j,k) = 1$ via formulas:
\[ r = \sqrt{a^2 + b^2} = \sqrt{1^2 + 1^2} = \sqrt{2}, \]
\[ c = \frac{1}{\sqrt{2}}, \]
\[ s = -\frac{1}{\sqrt{2}}. \]
The Givens matrix (recall $j > k$) will be
\[ G(2,1) = \begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 \\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 \\ 0 & 0 & 1 \end{bmatrix} \approx \begin{bmatrix} 0.7071 & 0.7071 & 0 \\ -0.7071 & 0.7071 & 0 \\ 0 & 0 & 1 \end{bmatrix}. \]
Thus,
\[ A_1 = G(2,1) \cdot A = \begin{bmatrix} 1.4142 & 0.7071 & 0.7071 \\ 0 & 0.7071 & -0.7071 \\ 0 & 1 & 1 \end{bmatrix}. \]
To zero out element $A_{13,2} = 1$, we compute elements of the Givens matrix $G(j, k) = G(3, 2)$. The Givens matrix (since $j > k$) will be
\[ G(j, k) = G(3, 2) = \begin{bmatrix} 1 & 0 & 0 \\ 0 & c & -s \\ 0 & s & c \end{bmatrix} \]
Values of $c, s$ are computed from the known $a = A(j, j) = 0.7071$ and $b = A(j, k) = 1$ via formulas:
\[ r = \sqrt{a^2 + b^2} = \sqrt{0.7071^2 + 1^2} = 1.2247, \]
\[ c = 0.5774, \]
\[ s = -0.8165. \]
The Givens matrix (recall $j > k$) will be
\[ G(3, 2) \approx \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0.5774 & 0.8165 \\ 0 & -0.8165 & 0.5774 \end{bmatrix} \]
Thus, the upper triangular matrix will be
\[ A_2 = G(3, 2) \cdot A_1 = \begin{bmatrix} 1.4142 & 0.7071 & 0.7071 \\ 0 & 1.2247 & 0.4082 \\ 0 & 0 & 1.1547 \end{bmatrix} \]"
4,"Since $r = Ax - b$ then $x = A^{-1}(r + b)$, $x = A^{-1}b$ and
\[
\frac{\|x - \hat{x}\|_2}{\|\hat{x}\|_2} \leq \|A^{-1}\|_2 \cdot \frac{\|r\|_2}{\|\hat{x}\|_2}.
\]
Applying the above estimate to the computed solution $x = [-2.001, 5.002]^T$ and noting that the exact solution is $\hat{x} = [-2, 5]^T$, we have the following error bound for the relative error:
\[
\frac{\|x - \hat{x}\|_2}{\|\hat{x}\|_2} \leq \|A^{-1}\|_2 \cdot \frac{\|r\|_2}{\|\hat{x}\|_2} = 1.6180 \cdot \frac{\sqrt{(-0.001)^2 + 0.002^2}}{\sqrt{(-2)^2 + 5^2}} \approx 6.7184 \times 10^{-4}.
\]
Here $\|A^{-1}\|_2 = 1.618$ since
\[
A^{-1} = \frac{1}{\det(A)} \bar{C}^T = \frac{1}{1} \begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix},
\]
where $\bar{C}^T$ is the transposed matrix of co-factors.

\[
\text{ans =}
\]
\[
1.6180
\]"
5,"
The Wilkinson's shift $\sigma_i$ should be chosen as an eigenvalue of the matrix
\[\begin{pmatrix}
a_{n-1,n-1} & a_{n-1,n} \\
a_{n,n-1} & a_{n,n}
\end{pmatrix}\]
which is closest to the value $a_{n,n}$ of the matrix $A_i$. Thus, the Wilkinson's shift $\sigma_i$ for the matrix
\[ A_i = \begin{pmatrix}
1 & 0 & 1 \\
0 & 1 & 1 \\
1 & 0 & 1
\end{pmatrix} \]
will be an eigenvalue of the matrix
\[\begin{pmatrix}
1 & 1 \\
0 & 1
\end{pmatrix}\]
which is closest to $a_{n,n} = 1$. Since both eigenvalues are $\lambda_1,_2 = 1,,$, then the Wilkinson's shift is $\sigma_i = 1$."
6," Multiplying the matrix \( A \) with the matrix \( B \), we have
\[
AB = \begin{pmatrix}
1 & 2 & 3 \\
0 & 1 & 2 \\
2 & 0 & 2
\end{pmatrix}
\begin{pmatrix}
1 & 1 & 2 \\
-1 & 1 & -1 \\
1& 0 & 2
\end{pmatrix}
= \begin{pmatrix}
2 & 3 & 6 \\
3 & -1 & 5 \\
4 & 2 & 8
\end{pmatrix}
\]

Since we know from part (a) that product \( AB \) is equal to
\[
AB = \begin{pmatrix}
2 & 3 & 6 \\
3 & -1 & 5 \\
4 & 2 & 8
\end{pmatrix}
\]

Now interchanging rows and columns, gives
\[
(AB)^T = \begin{pmatrix}
2 & 3 & 4 \\
3 & -1 & 2 \\
6 & 5 & 8
\end{pmatrix}
\]

Since transpose of the matrix \( A \) is
\[
A^T = \begin{pmatrix}
1 & 0 & 2 \\
0 & -1 & 0 \\
3 & 2 & 2
\end{pmatrix}
\]

and transpose of the matrix \( B \) is
\[
B^T = \begin{pmatrix}
1 & -1 & 1 \\
0 & 1 & -1 \\
2 & -1 & 2
\end{pmatrix}
\]

Then the product \( B^T A^T \) is equal to
\[
B^T A^T = \begin{pmatrix}
1 \cdot 2 + 3 \cdot 0 + 1 \cdot 2 & 0 + 1 \cdot 2 + 2 \cdot 0 & 2 + 0 + 2 \\
1 + 2 \cdot 0 + 0 \cdot -1 & 0 - 1 \cdot 0 + 2 \cdot 0 & 0 + 0 + 0 \\
2 \cdot 2 + 6 \cdot 0 + 1 \cdot 4 & 0 + 1 \cdot 4 + 4 \cdot 0 & 4 + 0 + 4
\end{pmatrix}
= \begin{pmatrix}
2 & 3 & 4 \\
3 & -1 & 2 \\
6 & 5 & 8
\end{pmatrix}
\]

Thus \( (AB)^T = B^T A^T \)."
7,"Multiplying the matrix $A$ with the matrix $B$, we have
\[ AB = \begin{pmatrix} 1+1 & 0+1 \\ 0+1 & 0+1 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}. \]
Now finding the inverse of product $AB$ as
\[ (AB)^{-1} = \frac{1}{(2)(1) - (1)(1)} \begin{pmatrix} 1 & -1 \\ -1 & 2 \end{pmatrix} = \begin{pmatrix} 1 & -1 \\ -1 & 2 \end{pmatrix}. \]
Similarly, finding inverse of $A$ and $B$, we get
\[ A^{-1} = \frac{1}{(1)(1) - (0)(-1)} \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 1 & -1 \\ 0 & 1 \end{pmatrix}, \]
and
\[ B^{-1} = \frac{1}{(1)(1) - (0)(-1)} \begin{pmatrix} 1 & 0 \\ -1 & 1 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ -1 & 1 \end{pmatrix}. \]
Then
\[ B^{-1}A^{-1} = \begin{pmatrix} 1+0 & -1+0 \\ -1+0 & 1+1 \end{pmatrix} = \begin{pmatrix} 1 & -1 \\ -1 & 2 \end{pmatrix}. \]
Thus, $(AB)^{-1} = B^{-1}A^{-1}$."
8,"The minors $M_{ij}$ of all elements $a_{ij}$ of the matrix $A$ are calculated as follows:
\begin{align*}
M_{11} &= \begin{vmatrix} 0 & 1 \\ 1 & 1 \end{vmatrix} = 0 - 1 = -1, \\
M_{12} &= \begin{vmatrix} 1 & 1 \\ 0 & 1 \end{vmatrix} = 1 - 0 = 1, \\
M_{13} &= \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix} = 1 - 0 = 1, \\
M_{21} &= \begin{vmatrix} 1 & 0 \\ 1 & 1 \end{vmatrix} = 1 - 0 = 1, \\
M_{22} &= \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix} = 1 - 0 = 1, \\
M_{23} &= \begin{vmatrix} 1 & 1 \\ 0 & 1 \end{vmatrix} = 1 - 0 = 1, \\
M_{31} &= \begin{vmatrix} 1 & 0 \\ 0 & 1 \end{vmatrix} = 1 - 0 = 1, \\
M_{32} &= \begin{vmatrix} 1 & 0 \\ 1 & 1 \end{vmatrix} = 1 - 0 = 1, \\
M_{33} &= \begin{vmatrix} 1 & 1 \\ 1 & 0 \end{vmatrix} = 0 - 1 = -1.
\end{align*}
Now using these minors we can have the cofactors of the matrix as follows:
\begin{align*}
A_{11} &= (-1)^{1+1}M_{11} = M_{11} = -1, \\
A_{12} &= (-1)^{1+2}M_{12} = -M_{12} = -1, \\
A_{13} &= (-1)^{1+3}M_{13} = M_{13} = 1, \\
A_{21} &= (-1)^{2+1}M_{21} = -M_{21} = -1, \\
A_{22} &= (-1)^{2+2}M_{22} = M_{22} = 1, \\
A_{23} &= (-1)^{2+3}M_{23} = -M_{23} = -1, \\
A_{31} &= (-1)^{3+1}M_{31} = M_{31} = 1, \\
A_{32} &= (-1)^{3+2}M_{32} = -M_{32} = -1, \\
A_{33} &= (-1)^{3+3}M_{33} = M_{33} = -1.
\end{align*}
So that the matrix of the cofactor is
\[ C = \begin{pmatrix} -1 & -1 & 1 \\ -1 & 1 & -1 \\ 1 & -1 & -1 \end{pmatrix}, \]
and the transpose of the cofactor matrix $C$ is the adjoint of $A$, that is
\[ \text{Adj}(A) = \begin{pmatrix} -1 & -1 & 1 \\ -1 & 1 & -1 \\ 1 & -1 & -1 \end{pmatrix}. \]
Now, by using the cofactor expansion along the first row, we can find the determinant of the matrix $A$ as follows:
\[ \det(A) = a_{11}A_{11} + a_{12}A_{12} + a_{13}A_{13} = -1 - 1 - 0 = -2. \]
To find the inverse of the matrix $A$, we use $A^{-1} = \frac{\text{Adj}(A)}{\det(A)}$. For example, if $A = \begin{pmatrix} -1 & -1 & 1 \\ -1 & 1 & -1 \\ 1 & -1 & -1 \end{pmatrix}$, then $A^{-1} = \begin{pmatrix} 1/2 & 1/2 & -1/2 \\ 1/2 & -1/2 & 1/2 \\ -1/2 & 1/2 & 1/2 \end{pmatrix}$."
9,"The minors $M_{ij}$ of all elements $a_{ij}$ of the matrix $A$ are calculated as follows:
\begin{align*}
M_{11} &= \begin{vmatrix} 2 & 0 \\ -2 & 1 \end{vmatrix} = 2(1) - 0(-2) = 2, \\
M_{12} &= \begin{vmatrix} -1 & 0 \\ 3 & 1 \end{vmatrix} = (-1)(1) - 0(3) = -1, \\
M_{13} &= \begin{vmatrix} -1 & 2 \\ 3 & -2 \end{vmatrix} = (-1)(-2) - 2(3) = -4, \\
M_{21} &= \begin{vmatrix} 1 & 3 \\ -2 & 1 \end{vmatrix} = 1(1) - 3(-2) = 7, \\
M_{22} &= \begin{vmatrix} 2 & 3 \\ 3 & 1 \end{vmatrix} = 2(1) - 3(3) = -7, \\
M_{23} &= \begin{vmatrix} 2 & 1 \\ 3 & -2 \end{vmatrix} = 2(-2) - 1(3) = -7, \\
M_{31} &= \begin{vmatrix} 1 & 3 \\ 2 & 0 \end{vmatrix} = 1(0) - 3(2) = -6, \\
M_{32} &= \begin{vmatrix} 2 & 3 \\ -1 & 0 \end{vmatrix} = 2(0) - 3(-1) = 3, \\
M_{33} &= \begin{vmatrix} 2 & 1 \\ -1 & 2 \end{vmatrix} = 2(2) - 1(-1) = 5.
\end{align*}
Now using these minors we can have the cofactors of the matrix as follows:

\begin{align*}
A_{11} &= (-1)^{1+1} M_{11} = M_{11} = 2 \\
A_{12} &= (-1)^{1+2} M_{12} = -M_{12} = 1 \\
A_{13} &= (-1)^{1+3} M_{13} = M_{13} = -4 \\
A_{21} &= (-1)^{2+1} M_{21} = -M_{21} = -7 \\
A_{22} &= (-1)^{2+2} M_{22} = M_{22} = -7 \\
A_{23} &= (-1)^{2+3} M_{23} = -M_{23} = 7 \\
A_{31} &= (-1)^{3+1} M_{31} = M_{31} = -6 \\
A_{32} &= (-1)^{3+2} M_{32} = -M_{32} = -3 \\
A_{33} &= (-1)^{3+3} M_{33} = M_{33} = 5 \\
\end{align*}

So that the matrix of the cofactor is 
C = \begin{pmatrix}
2 & 1 & -4 \\
-7 & -7 & 7 \\
-6 & -3 & 5
\end{pmatrix} \\
and the transpose of the cofactor matrix $C$ is the adjoint of $A$, that is 

\text{Adj}(A) = \begin{pmatrix}
2 & -7 & -6 \\
1 & -7 & -3 \\
-4 & 7 & 5
\end{pmatrix} \\

Now by using the cofactor expansion along the first row, we can find the determinant of the matrix $A$ as follows \\
\det(A) = a_{11} A_{11} + a_{12} A_{12} + a_{13} A_{13} = 4 + 1 - 12 = -7 \\
Now find the product 
\\

A \cdot \text{Adj} (A) = \text{Adj} (A) \cdot A = \begin{pmatrix}
-7 & 0 & 0 \\
0 & -7 & 0 \\
0 & 0 & -7
\end{pmatrix} 

Which can also be written as\\
$A \cdot \text{Adj}(A) = \text{Adj}(A) \cdot A = -7 \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} = \det(A) I_3$."
10,"Writing the given system in matrix form
\[
\begin{pmatrix}
4 & 1 & -3 \\
3 & 2 & -6 \\
1 & -5 & 3
\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2 \\
x_3
\end{pmatrix}
=
\begin{pmatrix}
9 \\
-2 \\
1
\end{pmatrix}
\]
gives
\[
A =
\begin{pmatrix}
4 & 1 & -3 \\
3 & 2 & -6 \\
1 & -5 & 3
\end{pmatrix}
\quad \text{and} \quad
\mathbf{b} =
\begin{pmatrix}
9 \\
-2 \\
1
\end{pmatrix}
\]
The determinant of the matrix can be calculated as
\[
|A| =
\begin{vmatrix}
4 & 1 & -3 \\
3 & 2 & -6 \\
1 & -5 & 3
\end{vmatrix}
= 4(6 - 30) + 1(-6 - 9) - 3(-15 - 2) = -60 \neq 0
\]
which shows that the given matrix $A$ is nonsingular. Then the matrices $A_1$, $A_2$, and $A_3$ can be computed as
\[
A_1 =
\begin{pmatrix}
9 & 1 & -3 \\
-2 & 2 & -6 \\
1 & -5 & 3
\end{pmatrix}, \quad
A_2 =
\begin{pmatrix}
4 & 9 & -3 \\
3 & -2 & -6 \\
1 & 1 & 3
\end{pmatrix}, \quad
A_3 =
\begin{pmatrix}
4 & 1 & 9 \\
3 & 2 & -2 \\
1 & -5 & 1
\end{pmatrix}
\]
Now Finding the determinants of the matrices $A_1$, $A_2$, and $A_3$, we get
\begin{align*}
|A_1| &= 9(6 - 30) + 1(-6 + 6) - 3(10 - 2) = -216 + 0 - 24 = -240, \\
|A_2| &= 4(-6 + 6) + 9(-6 - 9) - 3(3 + 2) = 0 - 135 - 15 = -150, \\
|A_3| &= 4(2 - 10) + 1(-2 - 3) + 9(-15 - 2) = -32 - 5 - 153 = -190.
\end{align*}
Therefore, by using Cramer's rule, we get
\begin{align*}
x_1 &= \frac{|A_1|}{|A|} = \frac{-240}{-60} = 4, \\
x_2 &= \frac{|A_2|}{|A|} = \frac{-150}{-60} = 2.5, \\
x_3 &= \frac{|A_3|}{|A|} = \frac{-190}{-60} = 3.166.
\end{align*}"""
11,"The process begins with the augmented matrix form
\[
\begin{pmatrix}
3 & 1 & 0 & | & 1.5 \\
2 & -1 & -1 & | & 2 \\
4 & 3 & 1 & | & 0 \\
\end{pmatrix}
\]
Since $a_{11} = 3 \neq 0$, we wish to eliminate the elements $a_{21}$ and $a_{31}$ by subtracting from the second and third rows the appropriate multiples of the first row. In this case, the multiples are given by $m_{21} = \frac{2}{3}$, and $m_{31} = \frac{4}{3}$. Hence
\[
\begin{pmatrix}
3 & 1 & 0 & | & 1.5 \\
0 & -\frac{5}{3} & -1 & | & 1 \\
0 & \frac{5}{3} & 1 & | & -2 \\
\end{pmatrix}
\]
As $a^{(1)}_{22} = -\frac{5}{3} \neq 0$, therefore, we eliminate entry in $a^{(1)}_{32}$ position by subtracting the multiple $m_{32} = -1$ of the second row from the third row, to get
\[
\begin{pmatrix}
3 & 1 & 0 & | & 1.5 \\
0 & -\frac{5}{3} & -1 & | & 1 \\
0 & 0 & 0 & | & -1 \\
\end{pmatrix}
\]
Obviously, the original set of equations has been transformed to an upper-triangular form. Since the third diagonal element of the obtaining upper-triangular matrix is zero, which means that the coefficient matrix of the given system is singular and therefore, the given system has no unique solution. Now expressing the set in algebraic form yields
\[
\begin{aligned}
3x_1 + x_2 &= 1.5 \\
-\frac{5}{3}x_2 - x_3 &= 1 \\
0x_3 &= -1
\end{aligned}
\]
From the third equation, we find that $0 = -1$, which is not possible, therefore, this system has no solution."
12,"The process begins with the augmented matrix form
\[
\begin{pmatrix}
1 & 1 & 1 & \vdots & 1 \\
2 & 3 & 4 & \vdots & 3 \\
4 & 9 & 16 & \vdots & 11 \\
\end{pmatrix}
\]
Since $a_{11} = 1 \neq 0$, we wish to eliminate the elements $a_{21}$ and $a_{31}$ by subtracting from the second and third rows the appropriate multiples of the first row. In this case, the multiples are given by $m_{21} = \frac{2}{1} = 2$, and $m_{31} = \frac{4}{1} = 4$. Hence
\[
\begin{pmatrix}
1 & 1 & 1 & \vdots& 1 \\
0 & 1 & 2 & \vdots & 1 \\
0 & 5 & 12 & \vdots & 7 \\
\end{pmatrix}
\]
As $a^{(1)}_{22} = 1 \neq 0$, therefore, we eliminate entry in $a^{(1)}_{32}$ position by subtracting the multiple $m_{32} = 5$ of the second row from the third row, to get
\[
\begin{pmatrix}
1 & 1 & 1 & \vdots & 1 \\
0 & 1 & 2 & \vdots & 1 \\
0 & 0 & 2 & \vdots & 2 \\
\end{pmatrix}
\]
Obviously, the original set of equations has been transformed to an upper-triangular form. Since all the diagonal elements of the obtaining upper-triangular matrix are nonzero, which means that the coefficient matrix of the given system is nonsingular and therefore, the given system has a unique solution. Now expressing the set in algebraic form yields
\begin{align*}
x_1 + x_2 + x_3 &= 1 \\
x_2 + 2x_3 &= 1 \\
2x_3 &= 2
\end{align*}
Now using backward substitution to give
\begin{align*}
2x_3 &= 2, & \text{gives } x_3 &= 1, \\
x_2 &= 1 - 2x_3 = 1 - 2, & \text{gives } x_2 &= -1, \\
x_1 &= 1 - x_2 - x_3 = 1 + 1 - 1, & \text{gives } x_1 &= 1.
\end{align*}"
13,"The augmented matrix form of the given system is
\[
\begin{pmatrix}
2 & 2 & 3 & \vdots & 0 \\
3 & k & 5 & \vdots & 0 \\
1 & 7 & 3 & \vdots & 0 \\
\end{pmatrix}
\]
Since $a_{11} = 2 \neq 0$, we wish to eliminate the elements $a_{21}$ and $a_{31}$ by subtracting from the second and third rows the appropriate multiples of the first row. In this case, the multiples are given by $m_{21} = \frac{3}{2}$, and $m_{31} = \frac{1}{2}$. Hence
\[
\begin{pmatrix}
2 & 2 & 3 & \vdots & 0 \\
0 & k-3 & \frac{1}{2} & \vdots & 0 \\
0 & 6 & \frac{3}{2} & \vdots & 0 \\
\end{pmatrix}
\]
If $a^{(1)}_{22} = k-3 = 0$, then we eliminate the entry in $a^{(1)}_{32}$ position by subtracting the multiple $m_{32} = \frac{6}{k-3}$ of the second row from the third row, to get
\[
\begin{pmatrix}
2 & 2 & 3 & \vdots & 0 \\
0 & k-3 & \frac{1}{2} & \vdots & 0 \\
0 & 0 & \frac{3k-15}{2(k-3)} & \vdots & 0 \\
\end{pmatrix}
\]
Since we know that a homogeneous system of $n$ equations in $n$ unknowns has a solution other than the trivial solution if and only if the determinant of the coefficient matrix is equal to zero, therefore, we put $\frac{3k-15}{2(k-3)} = 0$, which gives $k=5$.

Now expressing the set in algebraic form yields the underdetermined linear system
\begin{align*}
2x_1 + 2x_2 + 3x_3 &= 0, \\
2x_2 + \frac{1}{2}x_3 &= 0, \\
0x_3 &= 0,
\end{align*}
which has infinitely many solutions. If we take $x_3 = 4$, then we get $x_1 = -5$, $x_2 = -1$, $x_3 = 4$."
14,"Applying the forward elimination step of the simple Gaussian elimination on the given matrix $A$ and eliminating the elements below the first pivot (first diagonal element) to 
\[
\begin{pmatrix}
3 & 1 & -1 \\
0 & -\frac{2}{3} & \frac{14}{3} \\
0 & -\frac{16}{3} & \frac{4}{3}
\end{pmatrix}
\]
we finished with the first elimination step. The second pivot is in the (2,2) position but after eliminating the element below it we find the triangular form to be
\[
\begin{pmatrix}
3 & 1 & -1 \\
0 & -\frac{2}{3} & \frac{14}{3} \\
0 & 0 & -36
\end{pmatrix}
\]
Since the number of pivots are three, therefore, the rank of the given matrix is 3. Note that the original matrix is nonsingular as the rank of a $3 \times 3$ matrix is 3.

Now applying the forward elimination step of the simple Gaussian elimination on the given matrix $B$ and eliminating the elements below the first pivot (first diagonal element) to 
\[
\begin{pmatrix}
4 & 1 & 6 \\
0 & \frac{27}{4} & \frac{17}{2} \\
0 & -\frac{5}{4} & \frac{3}{2}
\end{pmatrix}
\]
we finished with the first elimination step. The second pivot is in the (2,2) position but after eliminating the element below it we find the triangular form to be
\[
\begin{pmatrix}
4 & 1 & 6 \\
0 & \frac{27}{4} & \frac{17}{2} \\
0 & 0 & \frac{83}{27}
\end{pmatrix}
\]
Since the number of pivots are three, therefore, the rank of the given matrix is 3. Note that this matrix is also nonsingular.

Repeating the same above procedure in finding the rank of the matrix $C$, we get,the matrices
\[
\begin{pmatrix}
17 & 46 & 7 \\
0 & -\frac{87}{17} & -\frac{4}{17} \\
0 & -\frac{174}{17} & -\frac{8}{17}
\end{pmatrix}
\]
and
\[
\begin{pmatrix}
17 & 46 & 7 \\
0 & -\frac{87}{17} & -\frac{4}{17} \\
0 & 0 & 0
\end{pmatrix}
\]
Since the number of pivots is two, therefore, the rank of the given matrix is 2. Note that the original matrix $C$ is singular as the rank of a $3 \times 3$ matrix is 2."
15,"The process begins with the augmented matrix form
\[
\begin{pmatrix}
1.001 & 1.5 & \vdots & 0 \\
2 & 3 & \vdots & 1 \\
\end{pmatrix}
\]
Since $a_{11} = 1.001 \neq 0$, we wish to eliminate the elements $a_{21}$ by subtracting from the second row the appropriate multiple of the first row. In this case, the multiple is given as
\[
m_{21} = \frac{2}{1.001} = 1.9980
\]
Hence, the matrix becomes
\[
\begin{pmatrix}
1.001 & 1.5 & \vdots & 0 \\
0 & 0.003 & \vdots & 1 \\
\end{pmatrix}
\]
Obviously, the original set of equations has been transformed to an upper-triangular form. Since all the diagonal elements of the obtained upper-triangular matrix are nonzero, which means that the coefficient matrix of the given system is nonsingular and therefore, the given system has a unique solution. Now expressing the set in algebraic form yields
\[
\begin{aligned}
1.001x_1 + 1.5x_2 &= 0 \\
0.003x_2 &= 1
\end{aligned}
\]
Now using backward substitution to give
\[
0.003x_2 = 1,
\]
gives
\[
1.001x_1 = 0 - 1.5x_2,
\]
which results in
\[
x_2 = 333.6667 \quad \text{and} \quad x_1 = -500.0000.
\]"
16,"Writing the given system in the augmented matrix form
\[
\begin{pmatrix}
1 & 4 & 1 & \vdots & 1 \\
2 & 4 & 1 & \vdots & 9 \\
3 & 5 & -2 & \vdots & 11 \\
\end{pmatrix}
\]
The first elimination step is to eliminate elements $a_{21} = 2$ and $a_{31} = 3$ by subtracting the multiples $m_{21} = 2$ and $m_{31} = 3$ of row 1 from rows 2 and 3 respectively, gives
\[
\begin{pmatrix}
1 & 4 & 1 & \vdots & 1 \\
0 & -4 & -1 & \vdots & 7 \\
0 & -7 & -5 & \vdots & 8 \\
\end{pmatrix}
\]
The second row is now divided by -4 to give
\[
\begin{pmatrix}
1 & 4 & 1 & \vdots & 1 \\
0 & 1 & 1/4 & \vdots & -7/4 \\
0 & -7 & -5 & \vdots & 8 \\
\end{pmatrix}
\]
The second elimination step is to eliminate elements in positions $a_{12} = 4$ and $a_{32} = -7$ by subtracting the multiples $m_{12} = 4$ and $m_{32} = -7$ of row 2 from rows 1 and 3 respectively, gives
\[
\begin{pmatrix}
1 & 0 & 0 & \vdots & 8 \\
0 & 1 & 1/4 & \vdots & -7/4 \\
0 & 0 & -13/4 & \vdots & -17/4 \\
\end{pmatrix}
\]
The third row is now divided by $-13/4$ to give
\[
\begin{pmatrix}
1 & 0 & 0 & \vdots & 8 \\
0 & 1 & 1/4 & \vdots & -7/4 \\
0 & 0 & 1 & \vdots & 17/13 \\
\end{pmatrix}
\]
The third elimination step is to eliminate elements in positions $a_{23} = -1$ by subtracting the multiples $m_{23} = -\frac{1}{4}$ and $m_{13} = 2$ of row 3 from row 2, gives
\[
\begin{pmatrix}
1 & 0 & 0 & \vdots & 8 \\
0 & 1 & 0 & \vdots & -\frac{27}{13} \\
0 & 0 & 1 & \vdots & \frac{17}{13} \\
\end{pmatrix}
\]
Obviously, the original set of equations has been transformed to a diagonal form. Now expressing the set in algebraic form yields
\[
\begin{aligned}
x_1 &= 8, \\
x_2 &= -\frac{27}{13}, \\
x_3 &= \frac{17}{13},
\end{aligned}
\]
which is the required solution of the given system."
17,"Consider the augmented matrix form:

\[
\begin{pmatrix}
3 & -9 & 5 & \vert & 1 & 0 & 0 \\
0 & 5 & 1 & \vert & 0 & 1 & 0 \\
-1 & 6 & 3 & \vert & 0 & 0 & 1 \\
\end{pmatrix}
\]

Using the Gauss-Jordan procedure, we get the following sequence of matrix transformations:

\[
\begin{aligned}
\begin{pmatrix}
1 & -3 & 1.6667 & \vert & 0.3333 & 0 & 0 \\
0 & 5 & 1 & \vert & 0 & 1 & 0 \\
-1 & 6 & 3 & \vert & 0 & 0 & 1 \\
\end{pmatrix}
&\rightarrow
\begin{pmatrix}
1 & -3 & 1.6667 & \vert & 0.3333 & 0 & 0 \\
0 & 5 & 1 & \vert & 0 & 1 & 0 \\
0 & 3 & 4.6667 & \vert & 0.3333 & 0 & 1 \\
\end{pmatrix}
\\[10pt]
&\rightarrow
\begin{pmatrix}
1 & -3 & 1.6667 & \vert & 0.3333 & 0 & 0 \\
0 & 1 & 0.2000 & \vert & 0 & 0.2000 & 0 \\
0 & 3 & 4.6667 & \vert & 0.3333 & 0 & 1 \\
\end{pmatrix}
\\[10pt]
&\rightarrow
\begin{pmatrix}
1 & 0 & 2.2667 & \vert & 0.3333 & 0.6000 & 0 \\
0 & 1 & 0.2000 & \vert & 0 & 0.2000 & 0 \\
0 & 0 & 4.0667 & \vert & 0.3333 & -0.6000 & 1 \\
\end{pmatrix}
\\[10pt]
&\rightarrow
\begin{pmatrix}
1 & 0 & 0 & \vert & 0.1475 & 0.9344 & -0.5574 \\
0 & 1 & 0.2000 & \vert & 0 & 0.2000 & 0 \\
0 & 0 & 1 & \vert & 0.0820 & -0.1475 & 0.2459 \\
\end{pmatrix}
\\[10pt]
&\rightarrow
\begin{pmatrix}
1 & 0 & 0 & \vert & 0.1475 & 0.9344 & -0.5574 \\
0 & 1 & 0 & \vert & -0.0164 & 0.2295 & -0.0492 \\
0 & 0 & 1 & \vert & 0.0820 & -0.1475 & 0.2459 \\
\end{pmatrix}
\end{aligned}
\]

Thus, the inverse of matrix \( A^{-1} \) is:

\[
A^{-1} =
\begin{pmatrix}
0.1475 & 0.9344 & -0.5574 \\
-0.0164 & 0.2295 & -0.0492 \\
0.0820 & -0.1475 & 0.2459 \\
\end{pmatrix}
\]"
18,"Solving the following equation to find the eigenvalues of the given matrix
\[
\begin{vmatrix}
A - \lambda I
\end{vmatrix} = -\lambda^3 + 7\lambda^2 - 14\lambda + 8 = -( \lambda - 2)( \lambda - 1)( \lambda - 4) = 0
\]
gives
\[
\lambda_1 = 4, \quad \lambda_2 = 2, \quad \lambda_3 = 1.
\]
The eigenvalues of the matrix. The corresponding eigenvectors are
\[
\mathbf{x}_1 = \begin{bmatrix} -1 & 1 &\ \frac{1}{2} \end{bmatrix}^T, \quad \mathbf{x}_2 = \begin{bmatrix} 1 & 0 & 0 \end{bmatrix}, \quad \mathbf{x}_3 = \begin{bmatrix} -1 & 1 & 1 \end{bmatrix}.
\]
Also
\[
A^T A = \begin{bmatrix}
4 & -4 & 6 \\
-4 & 14 & -14 \\
6 & -14 & 17
\end{bmatrix},
\]
and the characteristic equation of $A^T A$ is
\[
\begin{vmatrix}
A^T A - \lambda I
\end{vmatrix} = -\lambda^3 + 35\lambda^2 - 114\lambda + 64 = 0,
\]
which gives eigenvalues of $A^T A$
\[
\lambda_1 = 31.4386, \quad \lambda_2 = 2.8461, \quad \lambda_3 = 0.7153.
\]
Thus
\[
\rho(A^T A) = \sqrt{31.4386} = 5.6070 > 4 = \rho(A)."
19,"""Given a matrix $A$ with rank 2, positive singular values $\sigma_1$ and $\sigma_2$, we have $\sigma_1 > \lambda_{\text{max}} = 5$ and $\sigma_2 < \lambda_{\text{min}} = 3$. Starting with $A^T A$ and $AA^T$:

\[ A^T A = \begin{pmatrix} 25 & 20 \\ 20 & 25 \end{pmatrix}, \quad AA^T = \begin{pmatrix} 9 & 12 \\ 12 & 41 \end{pmatrix}. \]

Both have the same trace (50) and the same eigenvalues $\sigma^2_1 = 45$ and $\sigma^2_2 = 5$. The square roots are $\sigma_1 = \sqrt{45}$ and $\sigma_2 = \sqrt{5}$. Then $\sigma_1 \sigma_2 = 15$ which is the determinant of $A$.

A key step is to find the eigenvectors of $A^T A$ (with eigenvalues 45 and 5):

\[ \begin{pmatrix} 25 & 20 \\ 20 & 25 \end{pmatrix} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 45 \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \quad \begin{pmatrix} 25 & 20 \\ 20 & 25 \end{pmatrix} \begin{pmatrix} -1 \\ 1 \end{pmatrix} = 5 \begin{pmatrix} -1 \\ 1 \end{pmatrix}. \]

Then $v_1$ and $v_2$ are those (orthogonal) eigenvectors rescaled to length 1. Right singular vectors:

\[ v_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \end{pmatrix}, \quad v_2 = \frac{1}{\sqrt{2}} \begin{pmatrix} -1 \\ 1 \end{pmatrix}. \]

Left singular vectors $u_i$ are computed as follows:

Given $A\mathbf{v}_1 = \frac{3}{\sqrt{2}}\begin{pmatrix}1 \\ \frac{1}{3}\end{pmatrix} = \sqrt{45}\begin{pmatrix}\frac{1}{\sqrt{10}} \\ \frac{1}{3}\end{pmatrix} = \sigma_1 \mathbf{u}_1$ and $A\mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix}-3 \\ 1\end{pmatrix} = \sqrt{5}\begin{pmatrix}\frac{1}{\sqrt{10}} \\ -\frac{3}{1}\end{pmatrix} = \sigma_2 \mathbf{u}_2$. The division by $\sqrt{10}$ makes $\mathbf{u}_1$ and $\mathbf{u}_2$ orthonormal. Then $\sigma_1 = \sqrt{45}$ and $\sigma_2 = \sqrt{5}$ as expected. The Singular Value Decomposition is $A = U\Sigma V^T$:

$U = \frac{1}{\sqrt{10}}\begin{pmatrix}1 & -3 \\ 3 & 1\end{pmatrix}, \quad \Sigma = \begin{pmatrix}\sqrt{45} & 0 \\ 0 & \sqrt{5}\end{pmatrix}, \quad V = \frac{1}{\sqrt{2}}\begin{pmatrix}1 & -1 \\ 1 & 1\end{pmatrix}$.

Let $U$ and $V$ contain orthonormal bases for the column space and the row space, respectively (both spaces are $\mathbb{R}^2$). The real achievement is that these two bases diagonalize $A$: $AV = U \Sigma$. Then the matrix $U^T A V = \Sigma$ is diagonal. The matrix $A$ can be expressed as a combination of two rank-one matrices, columns times rows:
\[
\sigma_1 \mathbf{u}_1 \mathbf{v}_1^T + \sigma_2 \mathbf{u}_2 \mathbf{v}_2^T = \sqrt{45} \begin{pmatrix} 1 \\ 3 \end{pmatrix} \begin{pmatrix} 1 & 3 \end{pmatrix} + \sqrt{5} \begin{pmatrix} 3 \\ -1 \end{pmatrix} \begin{pmatrix} -3 & 1 \end{pmatrix} = \begin{pmatrix} 3 & 0 \\ 0 & 5 \end{pmatrix} = A.
\]"
20,"For $S$ to be orthogonal, the vectors in $S$ must be mutually orthogonal to each other. We therefore have:
\[
\begin{bmatrix}
1 \\
0 \\
0 \\

\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
= 0
\]
\[
\begin{bmatrix}
0 \\
\cos x \\
\sin x
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
= 0
\]
which gives the following set of equations on variables $x, y, z$:
\[
x = 0
\]
\[
(\cos x) y + (\sin x) z = 0.
\]

The set of solutions
\[
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
\]
is:
\[
\left\{
\begin{bmatrix}
0 \\
-\frac{\sin x}{\cos x} t \\
t
\end{bmatrix}
\ \middle|\ t \in \mathbb{R}
\right\}.
\]"
21,"Recall that $A$ is orthogonal if and only if both conditions below are satisfied:
\begin{itemize}
    \item All column vectors are mutually orthogonal.
    \item All column vectors have unit length.
\end{itemize}

In Problem 1, we have already obtained the set of $\begin{bmatrix} y \\ z \end{bmatrix}$ satisfying the first bullet:
\[
\left\{ \begin{bmatrix} 0 \\ -\frac{\sin \theta}{\cos \theta} t \\ t \end{bmatrix} \ \middle|\ t \in \mathbb{R} \right\}.
\]

To satisfy the second bullet, we need:
\[
x^2 + y^2 + z^2 = 1 \implies
\left( -\frac{\sin \theta}{\cos \theta} t \right)^2 + t^2 = 1 \implies
t^2 \left( \frac{\sin^2 \theta}{\cos^2 \theta} + 1 \right) = 1 \implies
t^2 = \frac{\cos^2 \theta}{\cos^2 \theta}
\]
which means that $t = \cos \theta$ or $t = -\cos \theta$. Hence, there are only two $\begin{bmatrix} x \\ y \\ z \end{bmatrix}$ that can make $A$ orthogonal:
\[
\begin{bmatrix} 0 \\ -\sin \theta \\ \cos \theta \end{bmatrix}, \begin{bmatrix} 0 \\ \sin \theta \\ -\cos \theta \end{bmatrix}
\]"
22,"We aim to obtain three eigenvectors of $A$ � denote them as $v_1, v_2, v_3$ respectively � that are mutually orthogonal to each other and have lengths 1.

To start with, find the eigenvalues of $A$: $\lambda_1 = 1$ and $\lambda_2 = -1$.

Now, obtain the eigenspace of $\lambda_1$:
\[
\left\{ \begin{bmatrix} u \\ u \\ v \end{bmatrix} \ \middle|\ u, v \in \mathbb{R} \right\}.
\]

This set has dimension 2. We will first take from the set two eigenvectors $x_1$ and $x_2$ that are orthogonal to each other. For this purpose, first set $x_1$ to an arbitrary non-zero vector, e.g., $\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$.

Regarding $x_2 = \begin{bmatrix} u \\ u \\ v \end{bmatrix}$, we ensure orthogonality between $x_1$ and $x_2$ by requiring their dot product to be 0:
\[
\begin{bmatrix} 1 & 1 & 0 \end{bmatrix} \begin{bmatrix} u \\ u \\ v \end{bmatrix} = 0 \implies
u + u = 0 \implies
u = 0.
\]

Note that there is no constraint on $v$. We can set $v$ to be any value such that $x_2$ is not a zero-vector, e.g., $v = 1$ which gives $x_2 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$. Finally, normalize $x_1$ and $x_2$ to have length 1, which gives:
\[
v_1 = \begin{bmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \end{bmatrix} \quad \text{and} \quad v_2 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.
\]

Next, obtain the eigenspace of $\lambda_2$:
\[
\left\{ \begin{bmatrix} -t \\ 0 \\ t \end{bmatrix} \ \middle|\ t \in \mathbb{R} \right\}.
\]

This set has dimension 1. Take an arbitrary eigenvector from the set, e.g., $x_3 = \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}$. Normalizing this vector to have length 1 gives $v_3 = \begin{bmatrix} -1/\sqrt{2} \\ 0 \\ 1/\sqrt{2} \end{bmatrix}$.

Therefore:
\[
Q = \begin{bmatrix}
1/\sqrt{2} & 0 & -1/\sqrt{2} \\
1/\sqrt{2} & 0 & 0 \\
0 & 1 & 1/\sqrt{2}
\end{bmatrix}
\]
\[
B = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & -1
\end{bmatrix}.
\]"
23,"Since this matrix is triangular, the eigenvalues are $\lambda_1 = 2$ and $\lambda_2 = 4$. By solving $(A - \lambda I) \mathbf{x} = 0$ for each eigenvalue, we would find the following:
\[
\lambda_1 = 2 : \mathbf{v}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad \lambda_2 = 4 : \mathbf{v}_2 = \begin{bmatrix} 5 \\ 1 \\ 1 \end{bmatrix}.
\]

Every eigenvector of $A$ is a multiple of $\mathbf{v}_1$ or $\mathbf{v}_2$ which means there are not three linearly independent eigenvectors of $A$ and by Theorem 5, $A$ is not diagonalizable."
24,"Here the characteristic polynomial is given by
\[
c_A(x) = \det \begin{bmatrix} x-2 & 0 & 0 \\ -1 & x-2 & 1 \\ -1 & -3 & x+2 \end{bmatrix} = (x-2)(x-1)(x+1),
\]
so the eigenvalues are $\lambda_1 = 2$, $\lambda_2 = 1$, and $\lambda_3 = -1$. To find all eigenvectors for $\lambda_1 = 2$, compute
\[
\lambda_1 I - A = \begin{bmatrix} 2-2 & 0 & 0 \\ -1 & 2-2 & 1 \\ -1 & -3 & 2+2 \end{bmatrix} = \begin{bmatrix} 0 & 0 & 0 \\ -1 & 0 & 1 \\ -1 & -3 & 4 \end{bmatrix}.
\]

We want the (nonzero) solutions to $(\lambda_1 I - A) \mathbf{x} = 0$. The augmented matrix becomes
\[
\begin{bmatrix} 0 & 0 & 0 & 0 \\ -1 & 0 & 1 & 0 \\ -1 & -3 & 4 & 0 \end{bmatrix} \rightarrow \begin{bmatrix} 1 & 0 & -1 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}
\]
using row operations. Hence, the general solution to $(\lambda_1 I - A) \mathbf{x} = 0$ is $\mathbf{x} = t \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ where $t$ is arbitrary, so we can use $\mathbf{x}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$ as the basic eigenvector corresponding to $\lambda_1 = 2$. As the reader can verify, the Gaussian algorithm gives basic eigenvectors $\mathbf{x}_2 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}$ and $\mathbf{x}_3 = \begin{bmatrix} 0 \\ 1 \\ 3 \end{bmatrix}$ corresponding to $\lambda_2 = 1$ and $\lambda_3 = -1$, respectively. Note that to eliminate fractions, we could instead use $\mathbf{x}_3 = \begin{bmatrix} 0 \\ 1 \\ 3 \end{bmatrix}$ as the basic $\lambda_3$-eigenvector."
25,"The characteristic polynomial of $A$ is (adding twice row 1 to row 2):
\[
c_A(x) = \det \begin{bmatrix} x-1 & 0 & 1 \\ 0 & x-1 & -2 \\ 1 & -2 & x-5 \end{bmatrix} = (x-1)(x-6)
\]

Thus the eigenvalues are $\lambda = 0, 1$, and $6$, and corresponding eigenvectors are
\[
\mathbf{x}_1 = \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}, \quad \mathbf{x}_2 = \begin{bmatrix} 2 \\ 0 \\ 1 \end{bmatrix}, \quad \mathbf{x}_3 = \begin{bmatrix} -1 \\ -2 \\ 5 \end{bmatrix}
\]

respectively. Moreover, by what appears to be remarkably good luck, these eigenvectors are orthogonal. We have $\|\mathbf{x}_1\|^2 = 6$, $\|\mathbf{x}_2\|^2 = 5$, and $\|\mathbf{x}_3\|^2 = 30$, so
\[
P = \left[ \frac{1}{\sqrt{6}} \mathbf{x}_1 \quad \frac{1}{\sqrt{5}} \mathbf{x}_2 \quad \frac{1}{\sqrt{30}} \mathbf{x}_3 \right] = \frac{1}{\sqrt{30}} \begin{bmatrix} \sqrt{5} & 2\sqrt{6} & -1 \\ -2\sqrt{5} & 0 & -2 \\ \sqrt{5} & \sqrt{6} & 5 \end{bmatrix}
\]

is an orthogonal matrix. Thus $P^{-1} = P^T$ and
\[
P^T A P = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 6 \end{bmatrix}
\]

by the diagonalization algorithm."
26,"The characteristic polynomial is
\[
c_A(x) = \det \begin{bmatrix} x-8 & 2 & -2 \\ 2 & x-5 & -4 \\ -2 & -4 & x-5 \end{bmatrix} = (x-9)^2
\]

Hence the distinct eigenvalues are 0 and 9 of multiplicities 1 and 2, respectively, so $\dim(E_0) = 1$ and $\dim(E_9) = 2$ by Theorem 5.5.6 ($A$ is diagonalizable, being symmetric). Gaussian elimination gives
\[
E_0(A) = \operatorname{span}\{\mathbf{x}_1\}, \quad \mathbf{x}_1 = \begin{bmatrix} 1 \\ 2 \\ -2 \end{bmatrix},
\]
and
\[
E_9(A) = \operatorname{span}\left\{\begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \\ 1 \end{bmatrix}\right\}.
\]

The eigenvectors in $E_9$ are both orthogonal to $\mathbf{x}_1$ as Theorem 8.2.4 guarantees, but not to each other. However, the Gram-Schmidt process yields an orthogonal basis
\[
\{\mathbf{x}_2, \mathbf{x}_3\} \text{ of } E_9(A) \quad \text{where} \quad \mathbf{x}_2 = \begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix} \text{ and } \mathbf{x}_3 = \begin{bmatrix} 2 \\ 4 \\ 5 \end{bmatrix}.
\]

Normalizing gives orthonormal vectors $\frac{1}{\|\mathbf{x}_1\|} \mathbf{x}_1$, $\frac{1}{\sqrt{5}} \mathbf{x}_2$, $\frac{1}{\sqrt{45}} \mathbf{x}_3$, so
\[
P = \left[ \frac{1}{\|\mathbf{x}_1\|} \mathbf{x}_1 \quad \frac{1}{\sqrt{5}} \mathbf{x}_2 \quad \frac{1}{\sqrt{45}} \mathbf{x}_3 \right] = \frac{1}{\sqrt{45}} \begin{bmatrix} \sqrt{5} & -6 & 2 \\ 2\sqrt{5} & 3 & 4 \\ -2\sqrt{5} & 0 & 5 \end{bmatrix}
\]

is an orthogonal matrix such that $P^{-1} A P$ is diagonal.
It is worth noting that other, more convenient, diagonalizing matrices $P$ exist. For example,
\[
\mathbf{y}_2 = \begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix} \text{ and } \mathbf{y}_3 = \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix}
\]
lie in $E_9(A)$ and they are orthogonal. Moreover, they both have norm $\sqrt{3}$ (as does $\mathbf{x}_1$), so
\[
Q = \left[ \frac{1}{\sqrt{3}} \mathbf{x}_1 \quad \frac{1}{\sqrt{3}} \mathbf{y}_2 \quad \frac{1}{\sqrt{3}} \mathbf{y}_3 \right] = \frac{1}{\sqrt{3}} \begin{bmatrix} 1 & 2 & -2 \\ 2 & 2 & 1 \\ -2 & 1 & 1 \end{bmatrix}
\]

is a nicer orthogonal matrix with the property that $Q^{-1} A Q$ is diagonal."
27,"The characteristic polynomial is
\[
\det \begin{bmatrix}
-1 - \lambda & 2 & 2 \\
2 & -1 - \lambda & 2 \\
2 & 2 & -1 - \lambda
\end{bmatrix} = \lambda^3 + 3\lambda^2 - 9\lambda - 27 = (\lambda - 3)(\lambda + 3)^2.
\]

(All roots are integers, so they can be found by trial and error, among the divisors of 27.) The corresponding homogeneous systems for eigenvectors and their solutions are:
\[
\lambda = 3: \begin{bmatrix} -4 & 2 & 2 \\ 2 & -4 & 2 \\ 2 & 2 & -4 \end{bmatrix} \mathbf{X} = 0; \quad \mathbf{u}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix};
\]
\[
\lambda = -3: \begin{bmatrix} 2 & 2 & 2 \\ 2 & 2 & 2 \\ 2 & 2 & 2 \end{bmatrix} \mathbf{X} = 0; \quad \mathbf{u}_{2,3} = \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}.
\]

Thus, the matrix can be diagonalized, and it has diagonal form in the basis $\{\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3\}$. Since $A$ is symmetric, $\mathbf{u}_1$ is orthogonal to $\mathbf{u}_2, \mathbf{u}_3$, and we only need to orthogonalize $\mathbf{u}_2, \mathbf{u}_3$. (By the way, the fact that $A$ is symmetric also tells us that it is diagonalizable, i.e., we must find three independent eigenvectors!) Applying Gram-Schmidt to $\{\mathbf{u}_2, \mathbf{u}_3\}$, we replace $\mathbf{u}_3$ with $[-1 \, 2 \, 1]^T$. Finally, the matrix has diagonal form
\[
\begin{bmatrix}
3 & 0 & 0 \\
0 & -3 & 0 \\
0 & 0 & -3
\end{bmatrix}
\]
in the orthogonal basis
\[
\left\{ \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix} \right\}.
\]"
28,"The linear system \( A\mathbf{x} = \mathbf{b} \) given by
\begin{align*}
E_1 &: 4x_1 + x_2 - x_3 + x_4 = -2, \\
E_2 &: x_1 + 4x_2 - x_3 - x_4 = -1, \\
E_3 &: -x_1 - x_2 + 5x_3 + x_4 = 0, \\
E_4 &: x_1 - x_2 + x_3 + 3x_4 = 1,
\end{align*}
has the unique solution \(\mathbf{x} = (-0.75342, 0.041096, -0.28082, 0.69178)\).

To convert \( A\mathbf{x} = \mathbf{b} \) to the form \(\mathbf{x} = T\mathbf{x} + \mathbf{c}\), solve each equation \( E_i \) for \( x_i \) to obtain
\[
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
=
\begin{bmatrix}
0 & \frac{1}{4} & -\frac{1}{4} & \frac{1}{4} \\
-\frac{1}{4} & 0 & \frac{1}{4} & -\frac{1}{4} \\
\frac{1}{5} & -\frac{1}{5} & 0 & \frac{1}{5} \\
\frac{1}{3} & -\frac{1}{3} & \frac{1}{3} & 0
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
+
\begin{bmatrix}
-\frac{2}{4} \\
-\frac{1}{4} \\
0 \\
\frac{1}{3}
\end{bmatrix}.
\]

Then \( A\mathbf{x} - \mathbf{b} \) can be written in the form \(\mathbf{x} = T\mathbf{x} + \mathbf{c}\), with
\[
T = \begin{bmatrix}
0 & \frac{1}{4} & -\frac{1}{4} & \frac{1}{4} \\
-\frac{1}{4} & 0 & \frac{1}{4} & -\frac{1}{4} \\
\frac{1}{5} & -\frac{1}{5} & 0 & \frac{1}{5} \\
\frac{1}{3} & -\frac{1}{3} & \frac{1}{3} & 0
\end{bmatrix} \quad \text{and} \quad \mathbf{c} = \begin{bmatrix}
-\frac{2}{4} \\
-\frac{1}{4} \\
0 \\
\frac{1}{3}
\end{bmatrix}.
\]

For initial approximation, we let \(\mathbf{x}^{(0)} = (0, 0, 0, 0)^T\). Then \(\mathbf{x}^{(1)}\) is given by
\begin{align*}
x_1^{(1)} &= 0 + \frac{1}{4}(0) + -\frac{1}{4}(0) + \frac{1}{4}(0) + -\frac{2}{4} = -0.5, \\
x_2^{(1)} &= -\frac{1}{4}(0) + 0 + \frac{1}{4}(0) + -\frac{1}{4}(0) + -\frac{1}{4} = -0.25, \\
x_3^{(1)} &= \frac{1}{5}(0) + -\frac{1}{5}(0) + 0 + \frac{1}{5}(0) + 0 = 0, \\
x_4^{(1)} &= \frac{1}{3}(0) + -\frac{1}{3}(0) + \frac{1}{3}(0) + 0 + \frac{1}{3} = 1/3.
\end{align*}

The next iterate, \(\mathbf{x}^{(2)}\), is given by
\begin{align*}
x_1^{(2)} &= 0 + \frac{1}{4}(-0.5) + -\frac{1}{4}(0) + \frac{1}{4}(1/3) + -\frac{2}{4} = -0.52083, \\
x_2^{(2)} &= -\frac{1}{4}(-0.5) + 0 + \frac{1}{4}(0) + -\frac{1}{4}(1/3) + -\frac{1}{4} = -0.041667, \\
x_3^{(2)} &= \frac{1}{5}(-0.5) + -\frac{1}{5}(0) + 0 + \frac{1}{5}(1/3) + 0 = -0.21667, \\
x_4^{(2)} &= \frac{1}{3}(-0.5) + -\frac{1}{3}(0) + \frac{1}{3}(0) + 0 + \frac{1}{3} = 0.41667.
\end{align*}"
29," The following equations were used:
\begin{align*}
x_1^{(k)} &= -\frac{1}{4}x_2^{(k-1)} + \frac{1}{4}x_3^{(k-1)} + \frac{1}{4}x_4^{(k-1)} - \frac{1}{2}, \\
x_2^{(k)} &= -\frac{1}{4}x_1^{(k)} + \frac{1}{4}x_3^{(k-1)} - \frac{1}{4}x_4^{(k-1)} - \frac{1}{4}, \\
x_3^{(k)} &= \frac{1}{5}x_1^{(k)} - \frac{1}{5}x_2^{(k)} + \frac{1}{5}x_4^{(k-1)}, \\
x_4^{(k)} &= -\frac{1}{3}x_1^{(k)} + \frac{1}{3}x_2^{(k)} - \frac{1}{3}x_3^{(k)} + \frac{1}{3}.
\end{align*}

However, since for \( i > 1 \), \( x_1^{(k)}, \ldots, x_{i-1}^{(k)} \) have already been computed, these are probably better approximations to the actual solutions \( x_1, \ldots, x_{i-1} \) than \( x_1^{(k-1)}, \ldots, x_{i-1}^{(k-1)} \). Hence, Gauss-Seidel uses the most recently available approximations to \( x_1, \ldots, x_{i-1} \) in a calculation of the next iterate:
\begin{align*}
x_1^{(k)} &= -\frac{1}{4}x_2^{(k-1)} + \frac{1}{4}x_3^{(k-1)} + \frac{1}{4}x_4^{(k-1)} - \frac{1}{2}, \\
x_2^{(k)} &= -\frac{1}{4}x_1^{(k)} + \frac{1}{4}x_3^{(k-1)} - \frac{1}{4}x_4^{(k-1)} - \frac{1}{4}, \\
x_3^{(k)} &= \frac{1}{5}x_1^{(k)} - \frac{1}{5}x_2^{(k)} + \frac{1}{5}x_4^{(k-1)}, \\
x_4^{(k)} &= -\frac{1}{3}x_1^{(k)} + \frac{1}{3}x_2^{(k)} - \frac{1}{3}x_3^{(k)} + \frac{1}{3}.
\end{align*}

For initial approximation, we let \(\mathbf{x}^{(0)} = (0, 0, 0, 0)^T\). Then \(\mathbf{x}^{(1)}\) is given by
\begin{align*}
x_1^{(1)} &= 0 + \frac{1}{4}(0) + \frac{1}{4}(0) + \frac{1}{4}(0) - \frac{1}{2} = -0.5, \\
x_2^{(1)} &= -\frac{1}{4}(-0.5) + \frac{1}{4}(0) - \frac{1}{4}(0) - \frac{1}{4} = -0.125, \\
x_3^{(1)} &= \frac{1}{5}(-0.5) - \frac{1}{5}(-0.125) + \frac{1}{5}(0) = -0.125, \\
x_4^{(1)} &= -\frac{1}{3}(-0.5) + \frac{1}{3}(-0.125) - \frac{1}{3}(-0.125) + \frac{1}{3} = 0.5.
\end{align*}

The next iterate, \(\mathbf{x}^{(2)}\), is given by
\begin{align*}
x_1^{(2)} &= 0 + \frac{1}{4}(-0.125) + \frac{1}{4}(-0.125) + \frac{1}{4}(0.5) - \frac{1}{2} = -0.625, \\
x_2^{(2)} &= -\frac{1}{4}(-0.625) + \frac{1}{4}(-0.125) - \frac{1}{4}(0.5) - \frac{1}{4} = 0, \\
x_3^{(2)} &= \frac{1}{5}(-0.625) - \frac{1}{5}(0) + \frac{1}{5}(0.5) = -0.025, \\
x_4^{(2)} &= -\frac{1}{3}(-0.625) + \frac{1}{3}(0) - \frac{1}{3}(-0.025) + \frac{1}{3} = 0.41667.
\end{align*}
"
30,"Let the matrix \( A \) is defined as:
\[
A = \begin{bmatrix}
1 & 2 & 4 & 3 \\
3 & 5 & 12 & 9 \\
2 & 4 & 8 & 6
\end{bmatrix}.
\]

Now, it can also be written as:
\[
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} A \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 2 & 4 & 3 \\
3 & 5 & 12 & 9 \\
2 & 4 & 8 & 6
\end{bmatrix}.
\]

Now applying the row operations \( R_2 \to R_2 - 3R_1, R_3 \to R_3 - 2R_1 \), it is obtained that:
\[
\begin{bmatrix}
1 & 0 & 0 \\
-3 & 1 & 0 \\
-2 & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & 2 & 4 & 3 \\
3 & 5 & 12 & 9 \\
2 & 4 & 8 & 6
\end{bmatrix} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}.
\]

Now applying the row operations \( C_2 \to C_2 - 2C_1, C_3 \to C_3 - 4C_1 \) and \( C_4 \to C_4 - 3C_1 \), it is obtained that:
\[
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} \begin{bmatrix}
1 & -2 & -4 & -3 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix} \begin{bmatrix}
1 & 2 & 4 & 3 \\
3 & 5 & 12 & 9 \\
2 & 4 & 8 & 6
\end{bmatrix} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}.
\]

Now, it is reduced into the form \( P A Q = \Delta \).

\[
\Delta^{-1} = \begin{bmatrix}
1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}.
\]

Now, the G-inverse of the matrix \( A \) would be given by:
\[
G = P \Delta^{-1} Q = \begin{bmatrix}
-5 & 2 & 0 \\
3 & -1 & 0 \\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix}.
\]"
31,"Now, the matrix \( A'A \) is obtained:
\[
A'A = \begin{bmatrix}
1 & 0 & 1 \\
0 & -1 & 2 \\
1 & 2 & 0
\end{bmatrix} \begin{bmatrix}
1 & 0 & 2 \\
0 & -1 & 1 \\
1 & 2 & 0
\end{bmatrix} = \begin{bmatrix}
2 & 2 & 2 \\
2 & 5 & -1 \\
2 & -1 & 5
\end{bmatrix}.
\]

Now, the characteristic equation for \( A'A \) is given by:
\[
|A'A - \lambda I| = 0
\]

\[
\begin{vmatrix}
2 - \lambda & 2 & 2 \\
2 & 5 - \lambda & -1 \\
2 & -1 & 5 - \lambda
\end{vmatrix} = 0
\]

\[
(2 - \lambda)\{(5 - \lambda)(5 - \lambda) - 1\} + 2\{-2 - 2(5 - \lambda)\} + 2\{-2 - 2(5 - \lambda)\} = 0
\]

On solving it the characteristic equation is obtained as:
\[
36\lambda - 12\lambda^2 + \lambda^3 = 0
\]

Using Cayley-Hamilton theorem, it can be written as:
\[
36(A'A) - 12(A'A)^2 + (A'A)^3 = 0
\]

Here, it is observed that the first non zero coefficient (coefficient of lowest power of \(A'A\)) is 36, Therefore the matrix \( T \) is defined as:
\[
T = (-1/36)[-12I + A'A]
\]

Or,
\[
T = \begin{bmatrix}
5/18 & -1/18 & -1/18 \\
-1/18 & 7/36 & 1/36 \\
-1/18 & 1/36 & 7/36
\end{bmatrix}
\]

Now, the Moore-Penrose inverse of matrix \( A \) is given by:
\[
G = T A' = \begin{bmatrix}
5/18 & -1/18 & -1/18 \\
-1/18 & 7/36 & 1/36 \\
-1/18 & 1/36 & 7/36
\end{bmatrix} \begin{bmatrix}
1 & 0 & 1 \\
0 & -1 & 2 \\
1 & 2 & 0
\end{bmatrix} = \begin{bmatrix}
1/6 & 0 & 1/6 \\
0 & -1/6 & 1/3 \\
1/6 & 1/3 & 0
\end{bmatrix}
\]"
32,"let us calculate all eigenvalues and all eigenvectors of the matrix
\[
A = \begin{pmatrix}
4 & -5 & 7 \\
1 & -4 & 9 \\
-4 & 0 & 5
\end{pmatrix}.
\]

The characteristic equation has the form
\[
\begin{vmatrix}
4 - \lambda & -5 & 7 \\
1 & -4 - \lambda & 9 \\
-4 & 0 & 5 - \lambda
\end{vmatrix} = 0.
\]

Calculating this determinant, we get
\[
\lambda^3 - 5\lambda^2 + 17\lambda - 13 = 0.
\]

Evidently, $\lambda = 1$ is the root of equation . It is easy to see that
\[
\lambda^3 - 5\lambda^2 + 17\lambda - 13 = (\lambda - 1)(\lambda^2 - 4\lambda + 13).
\]

The equation $\lambda^2 - 4\lambda + 13 = 0$ has two roots: $\lambda = 2 \pm 3i$. Therefore,
\[
\lambda_1 = 1, \quad \lambda_2 = 2 + 3i, \quad \lambda_3 = 2 - 3i
\]
are all the eigenvalues of the matrix $A$. The coordinates of the eigenvector corresponding to $\lambda_1$ are the solution of the homogeneous system of linear equations
\begin{align}
3x_1 - 5x_2 + 7x_3 &= 0, \\
x_1 - 5x_2 + 9x_3 &= 0, \\
-4x_1 + 4x_3 &= 0.
\end{align}

We have
\[
\begin{vmatrix}
3 & -5 \\
1 & -5
\end{vmatrix} \neq 0.
\]
Hence the rank of the matrix of system (1)-(3) is equal to two, and this system has only one linearly independent solution. Take $x_3 = 1$ and find $x_1, x_2$ as a solution of system (1), (2). We get $x_1 = 1, x_2 = 2$. Thus the vector $(1, 2, 1)$ is a solution of the system of equations (1)-(3). Therefore the set of all eigenvectors corresponding to the eigenvalue $\lambda_1 = 1$ is the set of vectors having the form $c(1, 2, 1)$, where $c$ is an arbitrary nonzero complex number.

The coordinates of the eigenvector corresponding to $\lambda_2$ are the solution of the homogeneous system of linear equations
\begin{align}
(2 - 3i)x_1 - 5x_2 + 7x_3 &= 0, \\
x_1 - (6 + 3i)x_2 + 9x_3 &= 0, \\
-4x_1 + (3 - 3i)x_3 &= 0.
\end{align}

We have
\[
\begin{vmatrix}
2 - 3i & -5 \\
1 & -(6 + 3i)
\end{vmatrix} \neq 0.
\]
Hence the coordinates of an eigenvector are the solution of system (4), (5) for $x_3 = 1$. We get $x_1 = (3 - 3i)/4, x_2 = (5 - 3i)/4$. Therefore the set of all eigenvectors corresponding to the eigenvalue $\lambda_2$ is the set of vectors having the form $c(3 - 3i, 5 - 3i, 4)$, where $c$ is an arbitrary nonzero complex number. Analogous calculations show that the set of all eigenvectors corresponding to the eigenvalue $\lambda_3$ is the set of vectors having the form $c(3 + 3i, 5 + 3i, 4)$, where $c$ is an arbitrary nonzero complex number.

 all the eigenvalues are distinct and the corresponding eigenvectors form the basis of the space $\mathbb{C}^3$. This can be seen also from the fact that the determinant
\[
\begin{vmatrix}
1 & 2 & 1 \\
3 - 3i & 5 - 3i & 4 \\
3 + 3i & 5 + 3i & 4
\end{vmatrix},
\]
which is composed of the coordinates of the eigenvectors, is not equal to zero."
33,"Let us calculate the QR decomposition of a matrix

\[
A = \begin{pmatrix}
12 & -51 & 4 \\
6 & 167 & -68 \\
-4 & 24 & -41
\end{pmatrix}
\]

using Householder reflection. First, we need to find a reflection that transforms the first column of matrix $A$, the vector $x = a_1 = (12, 6, -4)^T$, to $\|x\| e_1 = \|a_1\| e_1 = (14, 0, 0)^T$.

Now,  we construct the vector

\[
v = x + \alpha e_1,
\]

where

\[
\alpha = -\text{sign}(x_1) \|x\|,
\]

and

\[
u = \frac{v}{\|v\|}.
\]

We observe that in our example $\|x\| = \|x\|_2 = \sqrt{12^2 + 6^2 + (-4)^2} = 14$,

\[
\alpha = -\text{sign}(12) \|x\| = -14 \text{ and the vector will be } x = a_1 = (12, 6, -4)^T.
\]

Therefore

\[
v = x + \alpha e_1 = (12, 6, -4)^T + (-14)(1, 0, 0)^T = (2)(-1, 3, -2)^T
\]

and thus $u = \frac{v}{\|v\|} = \frac{1}{\sqrt{14}} (-1, 3, -2)^T$. Then the first Householder matrix will be
\[
P_1 = I - \frac{2}{\sqrt{14} \sqrt{14}} \begin{pmatrix} -1 \\ 3 \\ -2 \end{pmatrix} (-1 \, 3 \, -2) = I - \frac{1}{7} \begin{pmatrix} 1 & -3 & 2 \\ -3 & 9 & -6 \\ 2 & -6 & 4 \end{pmatrix}
= \begin{pmatrix}
\frac{6}{7} & \frac{3}{7} & -\frac{2}{7} \\
\frac{3}{7} & -\frac{2}{7} & \frac{6}{7} \\
-\frac{2}{7} & \frac{6}{7} & \frac{3}{7}
\end{pmatrix}.
\]

We perform multiplication $P_1 A$ to get the matrix $A_1$:

\[
A_1 = P_1 A = \begin{pmatrix}
14 & 21 & -14 \\
0 & -49 & -14 \\
0 & 168 & -77
\end{pmatrix},
\]

which is almost a triangular matrix. We only need to zero the (3, 2) entry.

Take the (1, 1) minor of , and then apply the same process again to the matrix

\[
A' = M_{11} = \begin{pmatrix}
-49 & -14 \\
168 & -77
\end{pmatrix}.
\]

By the same method as above we first need to find a reflection that transforms the first column of matrix $A'$, vector $x = (-49, 168)^T$, to $\|x\| e_1 = (175, 0)^T$. Here, $\|x\| = \sqrt{(-49)^2 + 168^2} = 175$,

\[
\alpha = -\text{sign}(-49) \|x\| = 175 \text{ and } x = (-49, 168)^T.
\]

Therefore,

\[
v = x + \alpha e_1 = (-49, 168)^T + (175, 0)^T = (126, 168)^T,
\]

\[
\|v\| = \sqrt{126^2 + 168^2} = \sqrt{44100} = 210,
\]

\[
u = \frac{v}{\|v\|} = (126/210, 168/210)^T = (3/5, 4/5)^T.
\]

Then

\[
P_2' = I - 2 \begin{pmatrix} 3/5 \\ 4/5 \end{pmatrix} (3/5 \, 4/5) = I - 2 \begin{pmatrix} 9/25 & 12/25 \\ 12/25 & 16/25 \end{pmatrix} = \begin{pmatrix} 7/25 & -24/25 \\ -24/25 & -7/25 \end{pmatrix}.
\]

Finally, we obtain the matrix of the Householder transformation $P_2$ such that

\[
P_2 = \begin{pmatrix} 1 & 0 \\ 0 & P_2' \end{pmatrix}
\]

to get

\[
P_2 = \begin{pmatrix}
1 & 0 & 0 \\
0 & 7/25 & -24/25 \\
0 & -24/25 & -7/25
\end{pmatrix}.
\]

Now, we find

\[
Q = P = P_1^T P_2^T = \begin{pmatrix}
6/7 & 69/175 & -58/175 \\
3/7 & -158/175 & 6/175 \\
-2/7 & -6/35 & -33/35
\end{pmatrix}.
\]

Thus, we have performed the QR decomposition of the matrix $A$ with matrices $Q$ and $R$ given by

\[
Q = P_1^T P_2^T = \begin{pmatrix}
0.8571 & 0.3943 & -0.3314 \\
0.4286 & -0.9029 & 0.0343 \\
-0.2857 & 0.1714 & -0.9429
\end{pmatrix},
\]

\[
R = P_2 A_1 = P_2 P_1 A = Q^T A = \begin{pmatrix}
14 & 21 & -14 \\
0 & -175 & 70 \\
0 & 0 & 35
\end{pmatrix},
\]"
34,"the given matrix $A$

\[
A = \begin{pmatrix} 5 & 1 & 0 \\ 1 & 6 & 3 \\ 0 & 3 & 7 \end{pmatrix}
\]

is transformed to the similar tridiagonal matrix $A_1$ by using Householder Method. We perform tridiagonalization in a following steps:

\begin{itemize}
    \item 1. First compute $\alpha$ via (9.70) as
    \[
    \alpha = -\text{sgn}(a_{21}) \sqrt{\sum_{j=2}^n a_{j1}^2} = -\sqrt{(a_{21}^2 + a_{31}^2)} = -\sqrt{(1^2 + 0^2)} = -1.
    \]
    \item 2. Using $\alpha$ we find $r$ via (9.70) as
    \[
    r = \sqrt{\frac{1}{2} (a^2 - a_{11} \alpha)} = \sqrt{\frac{1}{2} ((-1)^2 - 1 \cdot (-1))} = 1.
    \]
    \item 3. By known $\alpha$ and $r$ construct vector $v^{(1)}$ as in (9.71). Using (9.71) we compute:
    \begin{align*}
        v_1 &= 0, \\
        v_2 &= \frac{a_{21} - \alpha}{2r} = \frac{1 - (-1)}{2 \cdot 1} = 1, \\
        v_3 &= \frac{a_{31}}{2r} = 0.
    \end{align*}
\end{itemize}

Now we have obtained the vector
\documentclass{article}
\usepackage{amsmath}

\begin{document}

\[
v^{(1)} = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}.
\]

We compute the first Householder matrix $P^{(1)}$ as

\[
P^{(1)} = I - 2 v^{(1)} (v^{(1)})^T
\]

and get

\[
P^{(1)} = \begin{pmatrix} 1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 1 \end{pmatrix}.
\]

The tridiagonal matrix $A^{(1)}$ is obtained as

\[
A^{(1)} = P^{(1)} A P^{(1)} = \begin{pmatrix} 5 & -1 & 0 \\ -1 & 6 & -3 \\ 0 & -3 & 7 \end{pmatrix}.
\]

\vspace{1cm}"
35,"the given matrix $A$ of the size 4-by-4

\[
A = \begin{pmatrix}
4 & 1 & -2 & 2 \\
1 & 2 & 0 & 1 \\
-2 & 0 & 3 & -2 \\
2 & 1 & -2 & -1
\end{pmatrix},
\]

is transformed to the similar tridiagonal matrix $A_2$ using Householder reflections. Similarly with the example above, we perform following steps:

\begin{itemize}
    \item 1. First compute $\alpha$ via  as
    \[
    \alpha = -\text{sgn}(a_{21}) \sqrt{\sum_{j=2}^n a_{j1}^2} = (-1) \cdot \sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2}
    \]
    \[
    = -1 \cdot (1^2 + (-2)^2 + 2^2) = (-1) \cdot \sqrt{1 + 4 + 4} = -\sqrt{9} = -3.
    \]
    \item 2. Using $\alpha$ we find $r$ as
    \[
    r = \sqrt{\frac{1}{2} (a^2 - a_{11} \alpha)} = \sqrt{\frac{1}{2} ((-3)^2 - 1 \cdot (-3))} = \sqrt{6}.
    \]
    \item 3. From $\alpha$ and $r$, construct vector $v^{(1)}$. we compute:
    \begin{align*}
        v_1^{(2)} &= v_2^{(2)} = 0, \\
        v_2^{(2)} &= \frac{a_{32} - \alpha}{2r} = \frac{2}{\sqrt{6}}, \\
        v_3^{(2)} &= \frac{a_{42}}{2r} = \frac{1}{\sqrt{6}}, \\
        v_4^{(2)} &= \frac{a_{42}}{2r} = \frac{1}{\sqrt{6}}.
    \end{align*}
\end{itemize}

Thus, we have found

\[
v^{(1)} = \begin{pmatrix} 0 \\ \frac{2}{\sqrt{6}} \\ \frac{1}{\sqrt{6}} \\ \frac{1}{\sqrt{6}} \end{pmatrix}.
\]

Now we can compute the first Householder matrix $P^{(1)}$

\[
P^{(1)} = I - 2 v^{(1)} (v^{(1)})^T = I - 2 \cdot \begin{pmatrix} 0 \\ \frac{2}{\sqrt{6}} \end{pmatrix} \cdot \left(0 \, \frac{2}{\sqrt{6}} \, \frac{1}{\sqrt{6}} \, \frac{1}{\sqrt{6}}\right)
\]

and get

\[
P^{(1)} = \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & -1/3 & 2/3 & 2/3 \\
0 & 2/3 & 2/3 & 1/3 \\
0 & 2/3 & 1/3 & 2/3
\end{pmatrix}.
\]

After that we compute matrix $A^{(1)}$ as

\[
A^{(1)} = P^{(1)} A P^{(1)}
\]

to get

\[
A^{(1)} = P^{(1)} A P^{(1)} = \begin{pmatrix}
4 & -3 & 0 & 0 \\
-3 & 10/3 & -5/3 & 0 \\
0 & -5/3 & -33/25 & 68/75 \\
0 & 4/3 & -4/3 & -1
\end{pmatrix}.
\]

Next, having found $A^{(1)}$ we need construct $A^{(2)}$ and $P^{(2)}$. Using formulas  for $k = 2$ we get:

\[
\alpha = -\text{sgn}(a_{(3,2)}) \sqrt{\sum_{j=3}^4 a_{j2}^2} = -\text{sgn}(1) \sqrt{a_{32}^2 + a_{42}^2} = -\sqrt{1 + \frac{16}{9}} = -\frac{5}{3},
\]

\[
r = \sqrt{\frac{1}{2} (a^2 - a_{32} \alpha)} = \sqrt{\frac{20}{9}}.
\]

\[
v_1^{(2)} = v_2^{(2)} = 0,
\]

\[
v_3^{(2)} = \frac{a_{3,2} - \alpha}{2r} = \frac{2}{\sqrt{5}},
\]

\[
v_4^{(2)} = \frac{a_{4,2}}{2r} = \frac{1}{\sqrt{5}},
\]

and thus new vector $v$ will be: $v^{(2)} = (0, 0, \frac{2}{\sqrt{5}}, \frac{1}{\sqrt{5}})^T$ and the new Householder matrix $P^{(2)}$ will be

\[
P^{(2)} = I - 2 v^{(2)} (v^{(2)})^T = I - 2 \begin{pmatrix} 0 \\ 0 \\ \frac{2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} \end{pmatrix} \cdot \left(0 \, 0 \, \frac{2}{\sqrt{5}} \, \frac{1}{\sqrt{5}}\right)
\]

\[
= \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 4/5 & 2/5 \\
0 & 0 & 2/5 & 1/5
\end{pmatrix} - 2 \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 4/5 & 2/5 \\
0 & 0 & 2/5 & 1/5
\end{pmatrix} = \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & -3/5 & -4/5 \\
0 & 0 & -4/5 & -3/5
\end{pmatrix}.
\]

Finally, we obtain the tridiagonal matrix $A^{(2)}$ as

\[
A^{(2)} = P^{(2)} A^{(1)} P^{(2)} = \begin{pmatrix}
4 & -3 & 0 & 0 \\
-3 & 10/3 & -5/3 & 0 \\
0 & -5/3 & -33/25 & 68/75 \\
0 & 0 & 68/75 & 149/75
\end{pmatrix}.
\]

We observe, that we have performed process of tridiagonalization in 2 steps. The final result is a tridiagonal symmetric matrix $A^{(2)}$ which is similar to the original one $A$"
36,"we will use the Householder reflection to get the upper Hessenberg matrix from the matrix

\[
A = \begin{pmatrix}
12 & -51 & 4 \\
6 & 167 & -68 \\
-4 & 24 & -41
\end{pmatrix}.
\]

To do that we need zero out the value of entry (3,1) of this matrix. First, we need to find the Householder reflection that transforms the first column of the matrix $A$, i.e. the vector $x = (6, -4)^T$, to the form $\|x\| e_1 = (\sqrt{6^2 + (-4)^2}, 0)^T = (2\sqrt{13}, 0)^T$. Now,

\[
u = x + \alpha e_1,
\]

and

\[
v = \frac{u}{\|u\|}.
\]

Here, $\alpha = -2/\sqrt{13}$ and $x = (6, -4)^T$. Therefore,

\[
u = (6 - 2/\sqrt{13}, -4)^T \approx (-1.21, -4)^T
\]

and $v = u / \|u\| \approx (-0.29, -0.96)^T$, and then

\[
Q_1 = I - 2 \begin{pmatrix} -0.29 \\ -0.96 \end{pmatrix} (-0.29 \, -0.96)
\]

\[
= I - \begin{pmatrix}
0.1682 & 0.5568 \\
0.5568 & 1.84
\end{pmatrix} = \begin{pmatrix}
0.8318 & -0.5568 \\
-0.5568 & -0.84
\end{pmatrix}.
\]

Now observe that $Q_1 A$ preserves the first row of the matrix $A$:

\[
Q_1 A = \begin{pmatrix}
1 & 0 & 0 \\
0 & 0.8318 & -0.5568 \\
0 & -0.5568 & -0.84
\end{pmatrix} \begin{pmatrix}
12 & -51 & 4 \\
6 & 167 & -68 \\
-4 & 24 & -41
\end{pmatrix}
\]

\[
= \begin{pmatrix}
12 & -51 & 4 \\
7.2180 & 125.5474 & -33.7336 \\
0.0192 & -113.1456 & 72.3024
\end{pmatrix}.
\]
and the matrix $Q_1 A Q_1^T$ preserves the first column of the matrix $Q_1 A$:

\[
A_1 = Q_1 A Q_1^T = \begin{pmatrix}
12 & -44.6490 & 25.0368 \\
7.2180 & 123.2132 & -41.5686 \\
0.0192 & -134.3725 & 2.2655
\end{pmatrix},
\]

which is upper Hessenberg matrix."
37,"we apply  on the bidiagonal reduction of the matrix

\[
A = \begin{pmatrix}
4 & 4 & 3 \\
3 & 6 & 1 \\
0 & 1 & 7
\end{pmatrix},
\]

using Householder transformation. We proceed in following steps.

First, we need to zero out the second entry in the first column of the matrix $A$, the vector $x = (4, 3, 0)^T$. We compute first $\alpha = -\text{sign}(x_1) \|x\| = -5$, and then the vectors $u = x + \alpha e_1 = (-1, 3, 0)^T$ and $v = u / \|u\| = (-1, 3, 0)^T / \sqrt{10}$. Compute the Householder matrix $P_1$ as

\[
P_1 = I - 2 v v^T = \begin{pmatrix}
0.8 & 0.6 & 0 \\
0.6 & -0.8 & 0 \\
0 & 0 & 1
\end{pmatrix}.
\]

Compute $P_1 A$ to zero out the two entries below 5 in the first column:

\[
P_1 A = \begin{pmatrix}
5 & 6.8 & 3 \\
0 & -2.4 & 1 \\
0 & 1 & 7
\end{pmatrix}.
\]

Now we want to zero out the (1,3) entry of matrix . To do that we take the minor

\[
M = \begin{pmatrix}
6.8 & 3 \\
-2.4 & 1
\end{pmatrix},
\]

and compute again for $x = (6.8, -2.4)^T$: the number $\alpha = -\text{sign}(x_1) \|x\| = -7.4324$ and then the vectors $u = x + \alpha e_1 = (-0.6324, -2.4)^T$, $v = u / \|u\| = (-0.06324, -0.9785)^T$. Compute the matrix

\[
V_1 = I - 2 v v^T = \begin{pmatrix}
0.9149 & 0.4037 \\
0.4037 & -0.9149
\end{pmatrix}.
\]

Construct $V_1$ such that

\[
V_1 = \begin{pmatrix}
1 & 0 & 0 \\
0 & V_1 & 0 \\
0 & 0 & 1
\end{pmatrix} = \begin{pmatrix}
1 & 0 & 0 \\
0 & 0.9149 & 0.4037 \\
0 & 0.4037 & -0.9149
\end{pmatrix}.
\]

Compute $P_1 A V_1$ to zero out the (1,3) entry:

\[
P_1 A V_1 = \begin{pmatrix}
5 & 7.4324 & 0.0005 \\
0 & -1.7921 & -1.8838 \\
0 & 3.7408 & -6.0006
\end{pmatrix}.
\]

It remains only to zero out the (3,2) entry of the matrix . We take the minor

\[
M = \begin{pmatrix}
-1.7921 & -1.8838 \\
3.7408 & -6.0006
\end{pmatrix},
\]

and compute for $x = (-1.7921, 3.7408)^T$: the number $\alpha = -\text{sign}(x_1) \|x\| = 4.1479$ and the vectors $u = x + \alpha e_1 = (2.3558, 3.7408)^T$, $v = u / \|u\| = (0.5329, 0.8462)^T$. Compute the matrix $P_2'$ of order 2:

\[
P_2' = I - 2 v v^T = \begin{pmatrix}
0.4320 & -0.9019 \\
-0.9019 & -0.4321
\end{pmatrix}.
\]

Construct $P_2$ such that the matrix $P_2'$ is inserted into the identity matrix of order 3:

\[
P_2 = \begin{pmatrix}
1 & 0 & 0 \\
0 & P_2' & 0 \\
0 & 0 & 1
\end{pmatrix} = \begin{pmatrix}
1.0000 & 0 & 0 \\
0 & 0.4320 & -0.9019 \\
0 & -0.9019 & -0.4321
\end{pmatrix}.
\]

Finally, multiply the matrix $P_2$ by the matrix $P_1 A V_1$ obtained  to get bidiagonal matrix:

\[
P_2 P_1 A V_1 = \begin{pmatrix}
5.0000 & 7.4324 & 0.0005 \\
-0.0000 & -4.1480 & 4.5981 \\
0.0000 & -0.0001 & 4.2918
\end{pmatrix}.
\]"
38,"we apply the above algorithm to make the tridiagonal reduction of the matrix

\[
A = \begin{pmatrix}
5 & 4 & 3 \\
4 & 6 & 1 \\
3 & 1 & 7
\end{pmatrix},
\]

using Householder transformation. To do that we proceed in following steps.

First we compute $\alpha$ as

\[
\alpha = -\text{sign}(a_{21}) \sqrt{\sum_{j=2}^n a_{j1}^2} = -\sqrt{a_{21}^2 + a_{31}^2} = -\sqrt{4^2 + 3^2} = -5.
\]

Using $\alpha$, we find $r$ as

\[
r = \sqrt{\frac{1}{2} (\alpha^2 - a_{21} \alpha)} = \sqrt{\frac{1}{2} ((-5)^2 - 4 \cdot (-5))} = \frac{3\sqrt{5}}{\sqrt{2}}.
\]

Then we compute the components of the vector $v$:

\begin{align*}
v_1 &= 0, \\
v_2 &= \frac{a_{21} - \alpha}{2r} = \frac{4 - (-5)}{2 \cdot \frac{3\sqrt{5}}{\sqrt{2}}} = \frac{3\sqrt{2}}{\sqrt{5}}, \\
v_3 &= \frac{a_{31}}{2r} = \frac{3}{2 \cdot \frac{3\sqrt{5}}{\sqrt{2}}} = \frac{\sqrt{2}}{\sqrt{5}},
\end{align*}

and we get

\[
v^{(1)} = \left(0, \frac{3\sqrt{2}}{\sqrt{5}}, \frac{\sqrt{2}}{\sqrt{5}}\right)^T.
\]

Now we compute the first Householder matrix $P^1 = I - 2 v^{(1)} (v^{(1)})^T$ to get

\[
P^1 = \begin{pmatrix}
1 & 0 & 0 \\
0 & -\frac{4}{5} & -\frac{3}{5} \\
0 & -\frac{3}{5} & \frac{4}{5}
\end{pmatrix}.
\]

Finally, we obtain the tridiagonal matrix $A^{(1)}$ as

\[
A^{(1)} = P^1 A P^1 = \begin{pmatrix}
5 & -5 & 0 \\
-5 & 7.32 & -0.76 \\
0 & -0.76 & 5.68
\end{pmatrix}.
\]
Tridiagonal and Bidiagonal Reduction

\[
A^{(1)} = P^1 A P^1 = \begin{pmatrix}
5 & -5 & 0 \\
-5 & 7.32 & -0.76 \\
0 & -0.76 & 5.68
\end{pmatrix}.
\]"
39,"We have $A^T A = \begin{bmatrix} 2 & -1 & 1 \\ -1 & 1 & 0 \\ 1 & 0 & 1 \end{bmatrix}$, so the characteristic polynomial is

\[
c_{A^T A}(x) = \det \begin{bmatrix} x - 2 & 1 & -1 \\ 1 & x - 1 & 0 \\ -1 & 0 & x - 1 \end{bmatrix} = (x - 3)(x - 1)x.
\]

Hence the eigenvalues of $A^T A$ (in descending order) are $\lambda_1 = 3$, $\lambda_2 = 1$ and $\lambda_3 = 0$ with, respectively, unit eigenvectors

\[
q_1 = \frac{1}{\sqrt{6}} \begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix}, \quad q_2 = \frac{1}{\sqrt{2}} \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}, \quad \text{and} \quad q_3 = \frac{1}{\sqrt{3}} \begin{bmatrix} -1 \\ 1 \\ 1 \end{bmatrix}.
\]

It follows that the orthogonal matrix $Q$ in Theorem 8.6.1 is

\[
Q = [q_1 \, q_2 \, q_3] = \frac{1}{\sqrt{6}} \begin{bmatrix} 2 & 0 & -\sqrt{2} \\ -1 & \sqrt{3} & -\sqrt{2} \\ 1 & \sqrt{3} & \sqrt{2} \end{bmatrix}.
\]

The singular values here are $\sigma_1 = \sqrt{3}$, $\sigma_2 = 1$ and $\sigma_3 = 0$, so $\text{rank}(A) = 2$�clear in this case�and the singular matrix is

\[
\Sigma_A = \begin{bmatrix} \sigma_1 & 0 & 0 \\ 0 & \sigma_2 & 0 \\ 0 & 0 & \sigma_3 \end{bmatrix} = \begin{bmatrix} \sqrt{3} & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}.
\]

So it remains to find the $2 \times 2$ orthogonal matrix $P$ in Theorem 8.6.1. This involves the vectors

\[
Aq_1 = \frac{\sqrt{6}}{2} \begin{bmatrix} 1 \\ -1 \end{bmatrix}, \quad Aq_2 = \frac{\sqrt{2}}{2} \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad \text{and} \quad Aq_3 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}.
\]
Normalize $Aq_1$ and $Aq_2$ to get

\[
P_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix} \quad \text{and} \quad P_2 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]

In this case, $\{P_1, P_2\}$ is already a basis of $\mathbb{R}^2$ (so the Gram-Schmidt algorithm is not needed), and we have the $2 \times 2$ orthogonal matrix

\[
P = [P_1 \, P_2] = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix}.
\]

Finally (by Theorem 8.6.1) the singular value decomposition for $A$ is

\[
A = P \Sigma A^T Q = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix} \begin{bmatrix} \sqrt{3} & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \sqrt{6} \end{bmatrix} \begin{bmatrix} 2 & -1 & 1 \\ 0 & \sqrt{3} & \sqrt{3} \\ -\sqrt{2} & -\sqrt{2} & \sqrt{2} \end{bmatrix}.
\]

Of course this can be confirmed by direct matrix multiplication."
40,"\( A^T A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \) with eigenvalues \( \lambda_1 = 1 \) and \( \lambda_2 = 0 \) and corresponding eigenvectors
\( \mathbf{q}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \) and \( \mathbf{q}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \). Hence \( Q = [\mathbf{q}_1 \, \mathbf{q}_2] = I_2 \). Also \( A \) has rank 1 with singular values
\( \sigma_1 = 1 \) and \( \sigma_2 = 0 \), so \( \Sigma_A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = -A \) and \( \Sigma_A^+ = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = A^T \) in this case.

Since \( A \mathbf{q}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \) and \( A \mathbf{q}_2 = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \), we have \( \mathbf{p}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \) which extends to an orthonormal
basis \( \{\mathbf{p}_1, \mathbf{p}_2, \mathbf{p}_3\} \) of \( \mathbb{R}^3 \) where (say) \( \mathbf{p}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} \) and \( \mathbf{p}_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \). Hence
\( P = [\mathbf{p}_1 \, \mathbf{p}_2 \, \mathbf{p}_3] = I \), so the SVD for \( A \) is \( A = P \Sigma_A Q^T \). Finally, the pseudoinverse of \( A \) is
\( A^+ = \Sigma_A^+ P^T = \Sigma_A^+ = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \). Note that \( A^+ = A^T \) in this case."
41,"First we compute the eigenvalues of \( S \). We find the characteristic polynomial by
expanding cofactors along the first column:
\[
p(\lambda) = \det \begin{pmatrix}
1 - \lambda & 0 & 2 \\
0 & -1 - \lambda & -2 \\
2 & -2 & -\lambda
\end{pmatrix}
\]
\[
= (1 - \lambda) \det \begin{pmatrix}
-1 - \lambda & -2 \\
-2 & -\lambda
\end{pmatrix} + 2 \det \begin{pmatrix}
0 & 2 \\
-1 - \lambda & -2
\end{pmatrix}
\]
\[
= (1 - \lambda) [(-1 - \lambda)(-\lambda) - 4] + 2 [-2(0 - 2)]
\]
\[
= (1 - \lambda) [(\lambda + 1)\lambda - 4] - 4(-1 - \lambda)
\]
\[
= (1 - \lambda)(\lambda^2 + \lambda - 4) - 4(1 + \lambda)
\]
\[
= \lambda^3 + \lambda^2 - 4\lambda - \lambda^2 + 4\lambda + 4\lambda + 4
\]
\[
= \lambda^3 + 9\lambda = -\lambda(\lambda - 3)(\lambda + 3).
\]

The eigenvalues are 0 and \(\pm 3\); we compute eigenvectors:

\(\lambda = 3\): \( S - 3I = \begin{pmatrix} -2 & 0 & 2 \\ 0 & -4 & -2 \\ 2 & -2 & -3 \end{pmatrix} \)
\[
\overset{\text{RREF}}{\longrightarrow} \begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & \frac{1}{2} \\ 0 & 0 & 0 \end{pmatrix} \quad \leadsto \quad v_1 = \frac{1}{3} \begin{pmatrix} 2 \\ -1 \\ 2 \end{pmatrix}
\]

\(\lambda = -3\): \( S + 3I = \begin{pmatrix} 4 & 0 & 2 \\ 0 & 2 & -2 \\ 2 & -2 & 3 \end{pmatrix} \)
\[
\overset{\text{RREF}}{\longrightarrow} \begin{pmatrix} 1 & 0 & \frac{1}{2} \\ 0 & 1 & -1 \\ 0 & 0 & 0 \end{pmatrix} \quad \leadsto \quad v_2 = \frac{1}{3} \begin{pmatrix} -1 \\ 1 \\ 2 \end{pmatrix}
\]

\(\lambda = 0\): \( S - 0I = \begin{pmatrix} 1 & 0 & 2 \\ 0 & -1 & -2 \\ 2 & -2 & 0 \end{pmatrix} \)
\[
\overset{\text{RREF}}{\longrightarrow} \begin{pmatrix} 1 & 0 & 2 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{pmatrix} \quad \leadsto \quad v_3 = \frac{1}{3} \begin{pmatrix} -2 \\ -2 \\ 1 \end{pmatrix}
\]

Hence \( S = Q D Q^T \) for
\[
Q = \frac{1}{3} \begin{pmatrix} 2 & -1 & -2 \\ -1 & 2 & -2 \\ 2 & 2 & 1 \end{pmatrix}, \quad D = \begin{pmatrix} 3 & 0 & 0 \\ 0 & -3 & 0 \\ 0 & 0 & 0 \end{pmatrix}.
\]"
42,"\textbf{Step 1. Compute its transpose \( A^T \) and \( A^T A \).}

Since
\[
A^T = \begin{bmatrix}
4 & 3 \\
0 & -5
\end{bmatrix},
\]
then,
\[
A^T A = \begin{bmatrix}
4 & 3 \\
0 & -5
\end{bmatrix} \begin{bmatrix}
4 & 0 \\
3 & -5
\end{bmatrix} = \begin{bmatrix}
25 & -15 \\
-15 & 25
\end{bmatrix}.
\]

\textbf{Step 2. Determine the eigenvalues of \( A^T A \) and sort them in descending order, in the absolute sense. Square roots these to obtain the singular values of \( A \).}

\[
A^T A - cI = \begin{bmatrix}
25 - c & -15 \\
-15 & 25 - c
\end{bmatrix},
\]
\[
\det(A^T A - cI) = (25 - c)(25 - c) - (-15)(-15) = 0
\]
\[
\text{characteristic equation} \quad \longrightarrow \quad c^2 - 50c + 400 = 0
\]

The quadratic equation gives two values. In decreasing order, these are
\[
\det \begin{bmatrix} 40 & 10 \\ 10 & 10 \end{bmatrix} \quad \longrightarrow \quad c_1 = 40, \, c_2 = 10
\]

\[
\text{eigenvalues} \quad \longrightarrow \quad \text{singular values} \quad \longrightarrow \quad s_1 = \sqrt{40} = 6.3245 > s_2 = \sqrt{10} = 3.1622
\]

\textbf{Step 3. Construct diagonal matrix \( S \) by placing singular values in descending order along its diagonal. Compute its inverse, \( S^{-1} \).}

\[
S = \begin{bmatrix}
6.3245 & 0 \\
0 & 3.1622
\end{bmatrix},
\]
\[
S^{-1} = \begin{bmatrix}
0.1581 & 0 \\
0 & 0.3162
\end{bmatrix}.
\]
\textbf{Step 4. Use the ordered eigenvalues from step 2 and compute the eigenvectors of \( A^T A \). Place these eigenvectors along the columns of \( V \) and compute its transpose, \( V^T \).}

for \( c_1 = 40 \)
\[
A^T A - cI = \begin{bmatrix} 25 - 40 & -15 \\ -15 & 25 - 40 \end{bmatrix} = \begin{bmatrix} -15 & -15 \\ -15 & -15 \end{bmatrix}
\]
\[
(A^T A - cI) \mathbf{x}_1 = \mathbf{0}
\]
\[
\begin{bmatrix} -15 & -15 \\ -15 & -15 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
\]
\[
-15 x_1 + -15 x_2 = 0
\]
\[
-15 x_1 + -15 x_2 = 0
\]

Solving for \( x_2 \), for either equation: \( x_2 = -x_1 \)
\[
\mathbf{x}_1 = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 \\ -x_1 \end{bmatrix}
\]

Dividing by its length,
\[
L = \sqrt{x_1^2 + x_2^2} = x_1 \sqrt{2}
\]
\[
\mathbf{x}_1 = \begin{bmatrix} x_1 / L \\ -x_1 / L \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \end{bmatrix} = \begin{bmatrix} 0.7071 \\ -0.7071 \end{bmatrix}
\]

for \( c_2 = 10 \)
\[
A^T A - cI = \begin{bmatrix} 25 - 10 & -15 \\ -15 & 25 - 10 \end{bmatrix} = \begin{bmatrix} 15 & -15 \\ -15 & 15 \end{bmatrix}
\]
\[
(A^T A - cI) \mathbf{x}_2 = \mathbf{0}
\]
\[
\begin{bmatrix} 15 & -15 \\ -15 & 15 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
\]
\[
15 x_1 + -15 x_2 = 0
\]
\[
-15 x_1 + 15 x_2 = 0
\]

Solving for \( x_2 \), for either equation: \( x_2 = x_1 \)
\[
\mathbf{x}_2 = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 \\ x_1 \end{bmatrix}
\]

Dividing by its length,
\[
L = \sqrt{x_1^2 + x_2^2} = x_1 \sqrt{2}
\]
\[
\mathbf{x}_2 = \begin{bmatrix} -x_1 / L \\ x_1 / L \end{bmatrix} = \begin{bmatrix} -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix} = \begin{bmatrix} -0.7071 \\ 0.7071 \end{bmatrix}
\]

\[
V = \begin{bmatrix} \mathbf{x}_1 & \mathbf{x}_2 \end{bmatrix} = \begin{bmatrix} 0.7071 & -0.7071 \\ -0.7071 & 0.7071 \end{bmatrix}
\]
\[
V^T = \begin{bmatrix} 0.7071 & -0.7071 \\ -0.7071 & 0.7071 \end{bmatrix}
\]

\textbf{Step 5. Compute \( U \) as \( U = A V S^{-1} \). To complete the proof, compute the full SVD using \( A = U S V^T \).}

\[
U = A V S^{-1} = \begin{bmatrix} 4 & 0 \\ 3 & -5 \end{bmatrix} \begin{bmatrix} 0.7071 & 0.7071 \\ 0.1581 & 0 \end{bmatrix} = \begin{bmatrix} 0.1118 & 0.2236 \\ 0.1118 & 0.2236 \end{bmatrix}
\]

\[
U = A V S^{-1} = \begin{bmatrix} 0.4472 & 0.8944 \\ 0.8944 & -0.4472 \end{bmatrix}
\]

\[
A = U S V^T = \begin{bmatrix} 0.4472 & 0.8944 \\ 0.8944 & -0.4472 \end{bmatrix} \begin{bmatrix} 6.3245 & 0 \\ 0 & 3.1622 \end{bmatrix} \begin{bmatrix} 0.7071 & -0.7071 \\ 0.7071 & 0.7071 \end{bmatrix}
\]

\[
A = U S V^T = \begin{bmatrix} 0.4472 & 0.8944 \\ 0.8944 & -0.4472 \end{bmatrix} \begin{bmatrix} 4.4721 & -4.4721 \\ 2.2360 & 2.2360 \end{bmatrix}
\]

\[
A = U S V^T = \begin{bmatrix} 3.9998 & 0 \\ 2.9999 & -4.9997 \end{bmatrix} \approx \begin{bmatrix} 4 & 0 \\ 3 & -5 \end{bmatrix}
\]"
43,"First we compute the singular values \(\sigma_i\) by finding the eigenvalues of \( A A^T \).
\[
A A^T = \begin{pmatrix} 17 & 8 \\ 8 & 17 \end{pmatrix}.
\]

The characteristic polynomial is \(\det(A A^T - \lambda I) = \lambda^2 - 34\lambda + 225 = (\lambda - 25)(\lambda - 9)\), so
the singular values are \(\sigma_1 = \sqrt{25} = 5\) and \(\sigma_2 = \sqrt{9} = 3\).

Now we find the right singular vectors (the columns of \( V \)) by finding an orthonormal
set of eigenvectors of \( A^T A \). It is also possible to proceed by finding the left singular
vectors (columns of \( U \)) instead. The eigenvalues of \( A^T A \) are 25, 9, and 0, and since
\( A^T A \) is symmetric we know that the eigenvectors will be orthogonal.
For \(\lambda = 25\), we have
\[
A^T A - 25I = \begin{pmatrix} -12 & 12 & 2 \\ 12 & -12 & -2 \\ 2 & -2 & 17\end{pmatrix}
\]
which row-reduces to \(\begin{pmatrix} 1 & -1 & 0 \\ 0 & 0 & 1 \\0&0&0 \end{pmatrix}\). A unit-length vector in the kernel of that matrix
is \( v_1 = \begin{pmatrix} 1/\sqrt{2} \\1/\sqrt{2} \\ 0 \end{pmatrix} \).

For \(\lambda = 9\) we have \( A^T A - 9I = \begin{pmatrix} 4 & 12 & 2 \\ 12 & 4 & -2 \\ 2 & -2 & -1 \end{pmatrix} \) which row-reduces to \(\begin{pmatrix} 1 & 0 & -1 \\ 0 & 1 & 1 \\ 0 & 0 & 0 \end{pmatrix}\).
A unit-length vector in the kernel is \( v_2 = \begin{pmatrix} 1/\sqrt{18} \\ -1/\sqrt{18} \\ 4/\sqrt{18} \end{pmatrix} \).

For the last eigenvector, we could compute the kernel of \( A^T A \) or find a unit vector
perpendicular to \( v_1 \) and \( v_2 \). To be perpendicular to \( v_1 = \begin{pmatrix} a \\ b \\ c \end{pmatrix} \) we need \(-a = b\).
From the condition that \( v_2^T v_3 = 0 \) becomes \( 2a/\sqrt{18} + 4c/\sqrt{18} = 0 \) or \(-a = 2c\). So
\( v_3 = \begin{pmatrix} a \\ -a \\ a/2 \end{pmatrix} \) and for it to be unit-length we need \( a = 2/3 \) so \( v_3 = \begin{pmatrix} 2/3 \\ -2/3 \\ -1/3 \end{pmatrix} \).

So at this point we know that
\[
A = U \Sigma V^T = U \begin{pmatrix} 5 & 0 & 0 \\ 0 & 3 & 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} & 0 \\ 1/\sqrt{18} & -1/\sqrt{18} & 4/\sqrt{18} \\ 2/3 & -2/3 & -1/3 \end{pmatrix}.
\]

Finally, we can compute \( U \) by the formula \( u_1 = A v_1 \), or \( u_2 = \frac{1}{2} A v_2 \). This gives
\[
U = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix}.
\]
So in its full glory the SVD is:
\[
A = U \Sigma V^T = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix} \begin{pmatrix} 5 & 0 & 0 \\ 0 & 3 & 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} & 0 \\ 1/\sqrt{18} & -1/\sqrt{18} & 4/\sqrt{18} \\ 2/3 & -2/3 & -1/3 \end{pmatrix}.
\]"
44,"By cofactor expansion, it is not hard to compute the characteristic polynomial \( f_A(t) = t^2(t - 1)^2 \). Hence we get two distinct eigenvalues \( \lambda_1 = 0 \) and \( \lambda_2 = 1 \) both with algebraic multiplicity 2. Now let us compute the \( K_{\lambda} \) for each \( \lambda \).

For \( \lambda_1 = 0 \), we have to solve \( A X = 0 \). We easily see that the eigenspace \( E_{\lambda_1} \) has dimension 1, generated by \( v_1 = \begin{pmatrix} 1 \\ -1 \\ 0 \\ 0 \end{pmatrix} \). Now \( A^2 = \begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 2 & 2 & -2 & -2 \\ 1 & 1 & 0 & 0 \end{pmatrix} \).

By solving \( A^2 X = 0 \), we get basis \( v_1, v_2 \) of \( K_{\lambda_1} \) where \( v_2 = \begin{pmatrix} 1 \\ 0 \\ 1 \\ 0 \end{pmatrix} \). Now we build cycle \( \{ A v_2, v_2 \}, \) we get \( A v_2 = \begin{pmatrix} 1 \\ -1 \\ 0 \\ 0 \end{pmatrix} = v_1 \).

Now for \( \lambda_2 = 1 \), we have \( (A - I) = \begin{pmatrix} 0 & 1 & 0 & 0 \\ -2 & -2 & 0 & 0 \\ -2 & -2 & 1 & 1 \\ 1 & 1 & -1 & -1 \end{pmatrix} \). The eigenspace has dimension 1 which is spanned by \( v_3 := \begin{pmatrix} 0 \\ 0 \\ 1 \\ -1 \end{pmatrix} \). We have
\[
(A - I)^2 = \begin{pmatrix} 2 & -2 & 0 & 0 \\ -2 & 2 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{pmatrix}.
\]

Hence \( N(A - I) = K_{\lambda_2} \) is spanned by \( v_3 \) and \( e_4 \). We build a cycle \( (A - I) e_4, e_4 \). We easily calculate that \( A e_4 = v_3 \).

In summary, the Jordan canonical form of \( A \) is
\[
J = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{pmatrix}.
\]

And
\[
S = (v_1, v_2, v_3, e_4) = \begin{pmatrix}
1 & 1 & 0 & 0 \\
-1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 \\
0 & 0 & -1 & 1
\end{pmatrix}.
\]"
45,"It is easy to check that \( W^\perp := \left\{ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \mid x + y + z = 0 \right\} \). We can pick two basis vectors of \( W^\perp \) to be \( v_2 = \begin{pmatrix} 1 \\ -1 \\ 0 \end{pmatrix} \) and \( v_1 = \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix} \). But \( v_1 \) and \( v_2 \) are not orthogonal. By Gram-Schmidt, we set \( w_1 = v_1 \).

\[
w_2 = v_2 - \frac{\langle v_2, w_1 \rangle}{\langle w_1, w_1 \rangle} w_1 = \begin{pmatrix} 1 \\ -1 \\ 0 \end{pmatrix} - \frac{1}{2} \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix} = \begin{pmatrix} \frac{1}{2} \\ -\frac{1}{2} \\ \frac{1}{2} \end{pmatrix}.
\]

Replace \( w_2 \) by \( \frac{w_2}{\|w_2\|} \), we obtain an orthonormal basis of \( W^\perp \): \( u_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix} \), \( u_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix} \).

Now we can use the formula of projection
\[
\text{Proj}_{W^\perp} v = \frac{\langle v, u_1 \rangle}{\langle u_1, u_1 \rangle} u_1 + \frac{\langle v, u_2 \rangle}{\langle u_2, u_2 \rangle} u_2.
\]

So the projection is
\[
0 \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix} + \frac{1}{3} \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix} = \frac{1}{3} \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}.
\]

Then
\[
\min_{w \in W^\perp} \|v - w\| = \|v - \text{Proj}_{W^\perp} v\| = \left\| \frac{2}{3} \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \right\| = \frac{2}{\sqrt{3}} = \frac{2\sqrt{3}}{3}.
\]"
46,"First, compute \( A^T A = \begin{pmatrix} 9 & -9 \\ -9 & 9 \end{pmatrix} \). The eigenvalues of \( A^T A \) are 18 and 0, with corresponding unit eigenvectors
\[
v_1 = \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \end{pmatrix}, \quad v_2 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}.
\]

These unit vectors form the columns of \( V \):
\[
V = [v_1 \, v_2] = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ -1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}.
\]

The singular values are \( \sigma_1 = \sqrt{18} = 3\sqrt{2} \) and \( \sigma_2 = 0 \). Since there is only one nonzero singular value, the ""matrix"" \( D \) may be written as a single number. That is, \( D = 3\sqrt{2} \). The matrix \( \Sigma \) is the same size as \( A \), with \( D \) in its upper left corner:
\[
\Sigma = \begin{pmatrix} D & 0 \\ 0 & 0 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 3\sqrt{2} & 0 \\ 0 & 0 \\ 0 & 0 \end{pmatrix}.
\]

To construct \( U \), first construct \( A v_1 \) and \( A v_2 \):
\[
A v_1 = \begin{pmatrix} 2/\sqrt{2} \\ -4/\sqrt{2} \\ 4/\sqrt{2} \end{pmatrix}, \quad A v_2 = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
\]

As a check on the calculations, verify that \( \|A v_1\| = \sigma_1 = 3\sqrt{2} \). Of course, \( A v_2 = 0 \) because \( \|A v_2\| = \sigma_2 = 0 \). The only column found for \( U \) so far is
\[
u_1 = \frac{1}{3\sqrt{2}} A v_1 = \begin{pmatrix} 1/3 \\ -2/3 \\ 2/3 \end{pmatrix}.
\]

The other columns of \( U \) are found by extending the set \(\{u_1\}\) to an orthonormal basis for \(\mathbb{R}^3\). In this case, we need two orthogonal unit vectors \( u_2 \) and \( u_3 \) that are orthogonal to \( u_1 \). (See Fig. 3.) Each vector must satisfy \( u_1^T x = 0 \), which is equivalent to the equation \( x_1 - 2x_2 + 2x_3 = 0 \). A basis for the solution set of this equation is
\[
w_1 = \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix}, \quad w_2 = \begin{pmatrix} -2 \\ 0 \\ 1 \end{pmatrix}.
\]

(Check that \( w_1 \) and \( w_2 \) are each orthogonal to \( u_1 \).) Apply the Gram-Schmidt process (with normalizations) to \(\{w_1, w_2\}\), and obtain
\[
u_2 = \begin{pmatrix} 2/\sqrt{5} \\ 1/\sqrt{5} \\ 0 \end{pmatrix}, \quad u_3 = \begin{pmatrix} -2/\sqrt{45} \\ 4/\sqrt{45} \\ 5/\sqrt{45} \end{pmatrix}.
\]

Finally, set \( U = [u_1 \, u_2 \, u_3] \), take \( \Sigma \) and \( V^T \) from above, and write
\[
A = \begin{pmatrix} 1 & -1 \\ -2 & 2 \\ 2 & -2 \end{pmatrix} = \begin{pmatrix} 1/3 & 2/\sqrt{5} & -2/\sqrt{45} \\ -2/3 & 1/\sqrt{5} & 4/\sqrt{45} \\ 2/3 & 0 & 5/\sqrt{45} \end{pmatrix} \begin{pmatrix} 3\sqrt{2} & 0 \\ 0 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{2} \\ 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}.
\]"
47,"\subsection*{Step 1}
We first need to find the eigenvalues of \( A^T A \). We compute that
\[
A^T A = \begin{pmatrix}
80 & 100 & 40 \\
100 & 170 & 140 \\
40 & 140 & 200
\end{pmatrix}.
\]

We know that at least one of the eigenvalues is 0, because this matrix can have rank at most 2. In fact, we can compute that the eigenvalues are \( \lambda_1 = 360 \), \( \lambda_2 = 90 \), and \( \lambda_3 = 0 \). Thus the singular values of \( A \) are \( \sigma_1 = \sqrt{360} = 6\sqrt{10} \), \( \sigma_2 = \sqrt{90} = 3\sqrt{10} \), and \( \sigma_3 = 0 \). The matrix \( \Sigma \) in a singular value decomposition of \( A \) has to be a \( 2 \times 3 \) matrix, so it must be
\[
\Sigma = \begin{pmatrix}
6\sqrt{10} & 0 & 0 \\
0 & 3\sqrt{10} & 0
\end{pmatrix}.
\]

\subsection*{Step 2}
To find a matrix \( V \) that we can use, we need to solve for an orthonormal basis of eigenvectors of \( A^T A \). One possibility is
\[
v_1 = \begin{pmatrix} 1/3 \\ 2/3 \\ 2/3 \end{pmatrix}, \quad v_2 = \begin{pmatrix} -2/3 \\ -1/3 \\ 2/3 \end{pmatrix}, \quad v_3 = \begin{pmatrix} 2/3 \\ -2/3 \\ 1/3 \end{pmatrix}.
\]

(There are seven other possibilities in which some of the above vectors are multiplied by \(-1\).) Then \( V \) is the matrix with \( v_1, v_2, v_3 \) as columns, that is
\[
V = \begin{pmatrix}
1/3 & -2/3 & 2/3 \\
2/3 & -1/3 & -2/3 \\
2/3 & 2/3 & 1/3
\end{pmatrix}.
\]

\subsection*{Step 3}
We now find the matrix \( U \). The first column of \( U \) is
\[
\sigma_1^{-1} A v_1 = \frac{1}{6\sqrt{10}} \begin{pmatrix} 18 \\ 6 \end{pmatrix} = \begin{pmatrix} 3/\sqrt{10} \\ 1/\sqrt{10} \end{pmatrix}.
\]

The second column of \( U \) is
\[
\sigma_2^{-1} A v_2 = \frac{1}{3\sqrt{10}} \begin{pmatrix} 9 \\ 9 \end{pmatrix} = \begin{pmatrix} 1/\sqrt{10} \\ -3/\sqrt{10} \end{pmatrix}.
\]

Since \( U \) is a \( 2 \times 2 \) matrix, we do not need any more columns. (If \( A \) had only one nonzero singular value, then we would need to add another column to \( U \) to make it an orthogonal matrix.) Thus
\[
U = \begin{pmatrix}
3/\sqrt{10} & 1/\sqrt{10} \\
1/\sqrt{10} & -3/\sqrt{10}
\end{pmatrix}.
\]

To conclude, we have found the singular value decomposition
\[
\begin{pmatrix}
4 & 11 & 14 \\
8 & 7 & -2
\end{pmatrix} = \begin{pmatrix}
3/\sqrt{10} & 1/\sqrt{10} \\
1/\sqrt{10} & -3/\sqrt{10}
\end{pmatrix} \begin{pmatrix}
6\sqrt{10} & 0 & 0 \\
0 & 3\sqrt{10} & 0
\end{pmatrix} \begin{pmatrix}
1/3 & -2/3 & 2/3 \\
2/3 & -1/3 & -2/3 \\
2/3 & 2/3 & 1/3
\end{pmatrix}^T.
\]"
48,"We begin by constructing the Gram matrix \( G = A^T A = \begin{pmatrix} 2 & 0 \\ 0 & 8 \end{pmatrix} \). Since \( G \) is symmetric, it can be orthogonally diagonalized with
\[
D = \begin{pmatrix} 8 & 0 \\ 0 & 2 \end{pmatrix}, \quad Q = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}.
\]

We now know that the maximum value of the quadratic form \( q_G(x) \) is 8, which occurs in the direction \( \begin{pmatrix} 0 \\ 1 \end{pmatrix} \). Since \( L_A(x) = \sqrt{q_G(x)} \), this tells us that the maximum value of \( L_A(x) \), the first singular value, is \( \sigma_1 = \sqrt{8} \) and that this occurs in the direction of the first right singular vector \( v_1 = \begin{pmatrix} 0 \\ 1 \end{pmatrix} \).

In the same way, we also know that the second singular value \( \sigma_2 = \sqrt{2} \) with associated right singular vector \( v_2 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \).

The first left singular vector \( u_1 \) is defined by \( A v_1 = \begin{pmatrix} 2 \\ 2 \end{pmatrix} = \sigma_1 u_1 \). Because \( \sigma_1 = \sqrt{8} \), we have \( u_1 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix} \). Notice that \( u_1 \) is a unit vector because \( \sigma_1^{-1} = \|A v_1\| \).

In the same way, the second left singular vector is defined by \( A v_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \sigma_2 u_2 \), which gives us \( u_2 = \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \end{pmatrix} \).

We then construct
\[
U = [u_1 \, u_2] = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix},
\]
\[
\Sigma = \begin{pmatrix} \sigma_1 & 0 \\ 0 & \sigma_2 \end{pmatrix} = \begin{pmatrix} \sqrt{8} & 0 \\ 0 & \sqrt{2} \end{pmatrix},
\]
\[
V = [v_1 \, v_2] = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}.
\]

We now have \( A V = U \Sigma \) because
\[
A V = [A v_1 \, A v_2] = [\sigma_1 u_1 \, \sigma_2 u_2] = U \Sigma.
\]

Because the right singular vectors, the columns of \( V \), are eigenvectors of the symmetric matrix \( G \), they form an orthonormal basis, which means that \( V \) is orthogonal. Therefore, we have \( (A V) V^T = A = U \Sigma V^T \). This gives the singular value decomposition
\[
A = \begin{pmatrix} 1 & 2 \\ -1 & 2 \end{pmatrix} = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix} \begin{pmatrix} \sqrt{8} & 0 \\ 0 & \sqrt{2} \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}^T.
\]"
49,"The associated Gram matrix is
\[
G = A^T A = \begin{pmatrix} 5 & 4 \\ 4 & 5 \end{pmatrix},
\]
which has an orthogonal diagonalization with
\[
D = \begin{pmatrix} 9 & 0 \\ 0 & 1 \end{pmatrix}, \quad Q = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix}.
\]

This gives singular values and vectors
\begin{align*}
\sigma_1 &= 3, & v_1 &= \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}, & u_1 &= \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}, \\
\sigma_2 &= 1, & v_2 &= \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \end{pmatrix}, & u_2 &= \begin{pmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix},
\end{align*}
and the singular value decomposition \( A = U \Sigma V^T \) where
\[
U = \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{2} \\ 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 3 & 0 \\ 0 & 1 \end{pmatrix}, \quad V = \begin{pmatrix} 1/\sqrt{2} & 1/\sqrt{2} \\ 1/\sqrt{2} & -1/\sqrt{2} \end{pmatrix}.
\]

This example is special because \( A \) is symmetric. With a little thought, it's possible to relate this singular value decomposition to an orthogonal diagonalization of \( A \) using the fact that \( G = A^T A = A^2 \)."
50,"Finding an orthogonal diagonalization of \( G = A^T A \) gives
\[
D = \begin{pmatrix} 144 & 0 & 0 \\ 0 & 9 & 0 \\ 0 & 0 & 0 \end{pmatrix}, \quad Q = \begin{pmatrix} 1/3 & 2/3 & 2/3 \\ 2/3 & -2/3 & 1/3 \\ 2/3 & 1/3 & -2/3 \end{pmatrix},
\]
which gives singular values \( \sigma_1 = \sqrt{144} = 12 \), \( \sigma_2 = \sqrt{9} = 3 \), and \( \sigma_3 = 0 \). The right singular vectors \( v_i \) appear as the columns of \( Q \) so that \( V = Q \).

We now find
\begin{align*}
A v_1 = \begin{pmatrix} 0 \\ -12 \end{pmatrix} &= 12 u_1, & u_1 &= \begin{pmatrix} 0 \\ -1 \end{pmatrix}, \\
A v_2 = \begin{pmatrix} 3 \\ 0 \end{pmatrix} &= 3 u_1, & u_1 &= \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \\
A v_3 = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\end{align*}

Notice that it's not possible to find a third left singular vector since \( A v_3 = 0 \). We therefore form the matrices
\[
U = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}, \quad \Sigma = \begin{pmatrix} 12 & 0 & 0 \\ 0 & 3 & 0 \end{pmatrix}, \quad V = \begin{pmatrix} 1/3 & 2/3 & 2/3 \\ 2/3 & -2/3 & 1/3 \\ 2/3 & 1/3 & -2/3 \end{pmatrix},
\]
which gives the singular value decomposition \( A = U \Sigma V^T \).

Notice that \( U \) is a \( 2 \times 2 \) orthogonal matrix because \( A \) has two rows, and \( V \) is a \( 3 \times 3 \) orthogonal matrix because \( A \) has three columns."
51,"We have
\[
A^2 = \begin{pmatrix}
1 + s & -s \\
s & 1 - s
\end{pmatrix}
\begin{pmatrix}
1 + s & -s \\
s & 1 - s
\end{pmatrix}
= \begin{pmatrix}
1 + 2s & -2s \\
2s & 1 - 2s
\end{pmatrix}
\]
\[
= 2 \begin{pmatrix}
1 + s & -s \\
s & 1 - s
\end{pmatrix}
- \begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
= 2A - I
\]
\[
A^3 = A(2A - I) = 2A^2 - A = 2(2A - I) - A = 3A - 2I
\]
\[
A^4 = A(3A - 2I) = 3A^2 - 2A = 4A - 3I
\]

By induction, we get
\[
A^n = nA + (1 - n)I
\]

Hence, \(p = n\) and \(q = 1 - n\).

We have
\[
e^A = I + \frac{A}{1!} + \frac{A^2}{2!} + \frac{A^3}{3!} + \cdots
\]
\[
= I + \frac{A}{1!} + \frac{2A + (1 - 2)I}{2!} + \frac{3A + (1 - 3)I}{3!} + \cdots
\]
\[
= A \left[ 1 + \frac{1}{1!} + \frac{2}{2!} + \frac{3}{3!} + \cdots \right] + I \left[ 1 + \frac{1 - 2}{2!} + \frac{1 - 3}{3!} + \cdots \right]
\]
\[
= eA + I \left[ \left( 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots \right) - \left( \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots \right) \right]
\]
\[
= eA + I (e - 1) - (e - 1) = eA.
\]"
52,"Since the inverse of a lower triangular matrix is also lower triangular, we write
\[
L^{-1} = (p_{ij}) \quad \text{and} \quad
\begin{bmatrix}
1 & & & 0 \\
-1/2 & 1 & \\
 & -2/3 &1& \\
 & \ddots \\
0 & & -(n-1)/n & 1
\end{bmatrix}
\begin{bmatrix}
p_{11} & & & 0 \\
p_{21} & p_{22} &  \\
\cdots &  & \cdots & \\
p_{n1} & p_{n2} & \cdots & p_{nn}
\end{bmatrix}
= I
\]

Comparing the elements on both sides, we obtain
\begin{align*}
p_{11} &= 1 ; \quad -\frac{1}{2} p_{11} + p_{21} = 0 \quad \text{or} \quad p_{21} = \frac{1}{2}, \\
p_{22} &= 1 ; \quad -\frac{2}{3} p_{21} + p_{31} = 0 \quad \text{or} \quad p_{31} = \frac{1}{3}, \\
-\frac{2}{3} p_{22} + p_{32} &= 0 \quad \text{or} \quad p_{32} = \frac{2}{3} ; p_{33} = 1 \text{ etc.}
\end{align*}

We find
\[
p_{ij} = j / i, \quad i \geq j.
\]

Hence,
\[
L^{-1} = \begin{bmatrix}
1 &  & & & 0& \\
1/2 & 1 & & & \\
1/3 & 2/3 & 1 \\
\vdots & \vdots & \vdots & \\
1/n & 2/n & \cdots & (n-1)/n & 1
\end{bmatrix}
\]"
53,"Applying elementary row transformations on the augmented matrix, we obtain

\[
\begin{pmatrix}
2 & 3 & 0 & 0 & \mid & 1 \\
2 & 4 & 1 & 0 & \mid & 2 \\
0 & 2 & 6 & A & \mid & 4 \\
0 & 0 & 4 & B & \mid & C
\end{pmatrix}
\]

\[
\leadsto
\begin{pmatrix}
2 & 3 & 0 & 0 & \mid & 1 \\
0 & 1 & 1 & 0 & \mid & 1 \\
0 & 2 & 6 & A & \mid & 4 \\
0 & 0 & 4 & B & \mid & C
\end{pmatrix}
\]

\[
\leadsto
\begin{pmatrix}
2 & 3 & 0 & 0 & \mid & 1 \\
0 & 1 & 1 & 0 & \mid & 1 \\
0 & 0 & 4 & A & \mid & 2 \\
0 & 0 & 0 & B - A & \mid & C - 2
\end{pmatrix}
\]

We conclude that
\begin{itemize}
    \item the solution exists and is unique if $B \neq A$,
    \item there is no solution if $B = A$ and $C \neq 2$,
    \item a one parameter family of solutions exists if $B = A$ and $C = 2$.
\end{itemize}

For $B \neq A$, the solution is
\begin{align*}
    x_1 &= (8A - 2B - 3AC) / (8(B - A)), \\
    x_2 &= (2B - 4A + AC) / (4(B - A)), \\
    x_3 &= (2B - AC) / (4(B - A)), \\
    x_4 &= (C - 2) / (B - A).
\end{align*}

For $B = A$ and $C = 2$, we have the solution
\[
\mathbf{x} = (-0.25, 0.5, 0.5, 0)^T + t (-0.375A, 0.25A - 0.25A, 1)^T,
\]
where $t$ is arbitrary."
54,"The difference equation is

\[
2x_{n-1} - 3x_n + x_{n+1} = 0, \quad n = 1, 2, \ldots, N-1
\]

with \(x_0 = -0.5\) and \(x_N = 0\). The solution of this constant coefficient difference equation is

\[
x_n = A 1^n + B 2^n.
\]

Substituting \(x_N = 0\), we get \(A = -B 2^N\). Hence

\[
x_n = B(2^n - 2^N).
\]

We determine \(B\) from the first difference equation \(-3x_1 + x_2 = 1\). We have

\[
-3B(2 - 2^N) + B(2^2 - 2^N) = 1.
\]

The solution is

\[
B = \frac{1}{2^N + 1 - 2}.
\]

Hence,

\[
x_n = \frac{2^n - 2^N}{2^N + 1 - 2} = \frac{2^n - 1 - 2^N - 1}{2^N - 1}, \quad n = 1, 2, \ldots, N-1.
\]"
55,"L = 
\begin{bmatrix}
l_{11} & 0 & 0 & 0 & 0 & 0 \\
l_{21} & l_{22} & 0 & 0 & 0 & 0 \\
l_{31} & l_{32} & l_{33} & 0 & 0 & 0 \\
l_{41} & l_{42} & l_{43} & l_{44} & 0 & 0 \\
l_{51} & l_{52} & l_{53} & l_{54} & l_{55} & 0 \\
l_{61} & l_{62} & l_{63} & l_{64} & l_{65} & l_{66}
\end{bmatrix}
\]

Using \(LL^T = A\) and comparing we get \(l_{ij}\). We obtain

\[
L = 
\begin{bmatrix}
p & 0 & 0 & 0 & 0 & 0 \\
0 & p & 0 & 0 & 0 & 0 \\
0 & 0 & 2.5 & 0 & 0 & 0 \\
0 & 0 & 0 & p & 0 & 0 \\
0 & 0 & 1.5 & 0 & 2 & 0 \\
7l/(2p) & 3l/(2p) & 0 & 1l/(2p) & 0 & q
\end{bmatrix}
\]

where \(p = \sqrt{5.5}\) and \(q = \sqrt{31/11}\).

We have
\[
LL^T x = b.
\]

Set
\[
L^T x = z.
\]

Solving \(Lz = b\), we get \(z = (1/p \quad 1/p \quad 0.4 \quad 1/p \quad 0.2 \quad 0)^T\).

Solving \(L^T x = z\), we get \(x = (2/11 \quad 2/11 \quad 0.1 \quad 2/11 \quad 0.1 \quad 0)^T\)."
56,"C^T A^{-1} B = C^T (LL^T)^{-1} B = C^T (L^T)^{-1} L^{-1} B.
\]

Since \(L\) is lower triangular, we have

\[
\begin{bmatrix}
1 & 0 \\
1 & 2 \\
1 & 2 & 3 \\
1 & 2 & 3 & 4
\end{bmatrix}
\begin{bmatrix}
l_{11} & 0 & 0 & 0 \\
l_{21} & l_{22} & 0 & 0 \\
l_{31} & l_{32} & l_{33} & 0 \\
l_{41} & l_{42} & l_{43} & l_{44}
\end{bmatrix}
= 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}.
\]

We find

\[
L^{-1} = 
\begin{bmatrix}
1 & 0 & 0 & 0 \\
-1/2 & 1/2 & 0 & 0 \\
0 & -1/3 & 1/3 & 0 \\
0 & 0 & -1/4 & 1/4
\end{bmatrix},
\]

and

\[
L^{-1} B = 
\begin{bmatrix}
1 & 2 & 3 & 4 \\
2 & 2 & 2 & 2 \\
4/3 & 4/3 & 4/3 & 4/3 \\
1 & 1 & 1 & 1
\end{bmatrix},
\quad
L^{-1} C = 
\begin{bmatrix}
1 \\
2 \\
3 \\
4
\end{bmatrix}.
\]

Hence,

\[
C^T A^{-1} B = (L^{-1} C)^T L^{-1} B = (13 \quad 14 \quad 15 \quad 16).
\]"
57,"The inverse of a lower triangular matrix is also a lower triangular matrix. Let the inverse of the given matrix \(A\) be \(L\). Using the identity \(AL = I\), we get

\[
\begin{bmatrix}
1 &  &  & & \\
x & 1 & &  & & 0 &\\
x^2 & x & 1 &  &  \\
x^3 & x^2 & x & 1 &  \\
\vdots &  & \vdots &  &  \\
x^{n-1} & x^{n-2} &  & \cdots & x^2 &x & 1
\end{bmatrix}
\begin{bmatrix}
l_{11} & 0 & 0 & \cdots & 0 \\
l_{21} & l_{22} & 0 & \cdots & 0 \\
l_{31} & l_{32} & l_{33} & \cdots & 0 \\
l_{41} & l_{42} & l_{43} & l_{44} & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots \\
l_{n1} & l_{n2} & l_{n3} & \cdots & l_{nn}
\end{bmatrix}
= 
\begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}.
\]

Comparing elements on both sides, we get
\[
l_{11} = 1, x l_{11} + l_{21} = 0, \quad \text{or} \quad l_{21} = -x, l_{22} = 1 \text{ etc.}
\]

We find that
\[
l_{ij} = 
\begin{cases} 
1, & \text{if } i = j \\
-x, & \text{if } i = j + 1 \\
0, & \text{otherwise}.
\end{cases}
\]

Hence, we obtain

\[
A^{-1} = 
\begin{bmatrix}
1 &  & 0 & \cdots & 0 \\
-x & 1 & 0 & \cdots & 0 \\
0 & -x & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & -x & 1
\end{bmatrix}.
\]"
58,"Using the Cholesky method, write
\[
A = LL^T = 
\begin{bmatrix}
l_{11} & 0 \\
l_{21} & l_{22}
\end{bmatrix}
\begin{bmatrix}
l_{11} & l_{21} \\
0 & l_{22}
\end{bmatrix}.
\]

Comparing the coefficients, we get
\[
l_{11}^2 = 2, l_{11} = \sqrt{2},
\]
\[
l_{21} = -1 / \sqrt{2}, l_{31} = 2 / \sqrt{2},
\]
\[
l_{22} = 1 / 2, l_{22} = 1 / \sqrt{2},
\]
\[
l_{32} = 0, l_{33} = 1.
\]

Hence,
\[
L = 
\begin{bmatrix}
1/\sqrt{2} & 0 &0\\
-1/\sqrt{2} & 1/\sqrt{2} &0\\
\sqrt{2} & 0&1
\end{bmatrix}.
\]

Since \(L^{-1}\) is also a lower triangular matrix, write

\[
\begin{bmatrix}
\sqrt{2} & 0 &0\\
-1/\sqrt{2} & \sqrt{2} &0\\
-\sqrt{2}& 0&1
\end{bmatrix}.
\begin{bmatrix}
l_{11}^* & 0 &0 \\
l_{21}^* & l_{22}^*&0\\
l_{31}^* & l_{32}^*&l_{33}^*\\
\end{bmatrix}
= 
\begin{bmatrix}
1 & 0 &0 \\
0 & 1 &0\\
0&0&1
\end{bmatrix}.
\]

We find
\[
L^{-1} = 
\begin{bmatrix}
1/\sqrt{2} & 0 &0\\
1/\sqrt{2} & \sqrt{2} &0\\
-1 & 0&1
\end{bmatrix}.
\]

Hence,
\[
A^{-1} = (LL^T)^{-1} = (L^T)^{-1} L^{-1} = (L^{-1})^T L^{-1} = 
\begin{bmatrix}
2 & 1 &-1 \\
1 & 2&0\\
-1&0&1
\end{bmatrix}.
\]"
59,"Let \(L = (l_{ij})\) where \(l_{ij} = 0\) for \(i < j\). Writing the given matrix as \(LL^T\), we obtain

\[
\begin{bmatrix}
1 & -1 & 0 & 0 & 0 \\
-1 & 2 & -1 & 0 & 0 \\
0 & -1 & 2 & -1 & 0 \\
0 & 0 & -1 & 2 & -1 \\
0 & 0 & 0 & -1 & 2
\end{bmatrix}
= 
\begin{bmatrix}
l_{11} & 0 & 0 & 0 & 0 \\
l_{21} & l_{22} & 0 & 0 & 0 \\
l_{31} & l_{32} & l_{33} & 0 & 0 \\
l_{41} & l_{42} & l_{43} & l_{44} & 0 \\
l_{51} & l_{52} & l_{53} & l_{54} & l_{55}
\end{bmatrix}
\begin{bmatrix}
l_{11} & l_{21} & l_{31} & l_{41} & l_{51} \\
0 & l_{22} & l_{32} & l_{42} & l_{52} \\
0 & 0 & l_{33} & l_{43} & l_{53} \\
0 & 0 & 0 & l_{44} & l_{54} \\
0 & 0 & 0 & 0 & l_{55}
\end{bmatrix}.
\]

On comparing corresponding elements on both sides and solving, we get
\begin{align*}
l_{11} &= 1, l_{21} = -1, l_{i1} = 0, i = 3, 4, 5, \\
l_{22} &= 1, l_{32} = -1, l_{i2} = 0, i = 4, 5, \\
l_{33} &= 1, l_{43} = -1, l_{i3} = 0, i = 5, \\
l_{44} &= 1, l_{54} = -1, \\
l_{55} &= 1.
\end{align*}

Hence,
\[
L = 
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
-1 & 1 & 0 & 0 & 0 \\
0 & -1 & 1 & 0 & 0 \\
0 & 0 & -1 & 1 & 0 \\
0 & 0 & 0 & -1 & 1
\end{bmatrix}.
\]"
60,"Let the matrix \(A\) be partitioned as
\[
A = \begin{bmatrix} B & \vert & C \\ 
-&\vert &-\\
E &|& D \end{bmatrix} = \begin{bmatrix} 1 & 1 & \vert& 1 \\ 4 & 3 &\vert& -1 \\ 
-&-&\vert &-\\
3 & 5 &\vert& 3 \end{bmatrix}, \quad A^{-1} = \begin{bmatrix} X &\vert& Y \\ 
-&\vert &-\\
Z & \vert&V\end{bmatrix}.
\]

Now,
\[
B^{-1} = \begin{bmatrix} 1 & 1 \\ 4 & 3 \end{bmatrix}^{-1} = -\begin{bmatrix} 3 & -1 \\ -4 & 1 \end{bmatrix}.
\]

\begin{align*}
D - EB^{-1}C &= 3 + \begin{bmatrix} 3 & 5\\ \end{bmatrix} \begin{bmatrix} 3 & -1 \\
-4 & 1 \\
\end{bmatrix} \begin{bmatrix} 1 \\ -1 \end{bmatrix} = -10 \\
V &= (D - EB^{-1}C)^{-1} = -\frac{1}{10} \\
Y &= -B^{-1}CV = \begin{bmatrix} 3 & -1 \\
-4 & 1 
\end{bmatrix} \begin{bmatrix} 1 \\ -1 \end{bmatrix}  = -\frac{1}{10} \begin{bmatrix}  4 \\-5 \end{bmatrix}  \\
Z &= -VEB^{-1} = -\frac{1}{10} \begin{bmatrix} 3 & 5 \end{bmatrix} \begin{bmatrix} 3 & -1 \\ -4 & 1 \end{bmatrix} = -\frac{1}{10} \begin{bmatrix}  11 & 2 \end{bmatrix} \\
X &= B^{-1} - B^{-1}CZ \\
&= \begin{bmatrix} -3 & 1  \\4 & -1 \\ \end{bmatrix} -\frac{1}{10} \begin{bmatrix} 3 & -1 \\
-4 & 1 \\ \end{bmatrix} \begin{bmatrix} 1 \\ -1 \end{bmatrix} \begin{bmatrix} -11 & 2 \end{bmatrix} \\
&= \begin{bmatrix} -3 & 1\\
4 & -1 \\ \end{bmatrix} -\frac{1}{10}   \begin{bmatrix} -44 & 8\\55 & -10 \end{bmatrix} = \begin{bmatrix} 14 & 0.2 \\
-15 & 0\end{bmatrix} \\
\end{bmatrix}
\end{align*}

Hence,
\[
A^{-1} = \begin{bmatrix} 14 & 0.2 & -0.4 \\ -1.5 & 0 & 0.5 \\
1.1 & -0.2 & -0.1 \end{bmatrix}.
\]

The solution of the given system of equations is
\[
x = \begin{bmatrix} 14 & 0.2 & -0.4 \\ -1.5 & 0 & 0.5 \\
1.1 & -0.2 & -0.1 \end{bmatrix}.\begin{bmatrix} 1 \\ 6 \\ 4 \end{bmatrix} = \begin{bmatrix} 1\\0.5 \\ -0.5 \end{bmatrix}.
\]"
61,"We partition the given matrix as
\[
A = \begin{bmatrix} 2 & 1 &\vert& 0 & 0 \\ 1 & 2 &\vert& 1 & 0 
\\ 
-&-&-&-&-\\
0 & 1 & \vert& 2 & 1 \\ 0 & 0 &\vert & 1 & 2 \end{bmatrix} = \begin{bmatrix} B & \vert&C \\ 
-&\vert&-\\E &\vert& D \end{bmatrix},
\]
and write the inverse matrix in the form
\[
A^{-1} = \begin{bmatrix} X & \vert&Y \\ -&\vert&-\\Z &\vert& V \end{bmatrix}.
\]

Using the fact that \(AA^{-1} = I\), we obtain
\[
\begin{bmatrix} B & C \\ E & D \end{bmatrix} \begin{bmatrix} X & Y \\ Z & V \end{bmatrix} = \begin{bmatrix} BX + CZ & BY + CV \\ EX + DZ & EY + DV \end{bmatrix} = \begin{bmatrix} I & 0 \\ 0 & I \end{bmatrix}.
\]

Hence,
\begin{align*}
BX + CZ &= I, \\
BY + CV &= 0, \\
EX + DZ &= 0, \\
EY + DV &= I.
\end{align*}

We find
\[
B^{-1} = \frac{1}{3} \begin{bmatrix} 2 & -1 \\ -1 & 2 \end{bmatrix}.
\]

Solving the above matrix equations, we get
\begin{align*}
V &= (D - EB^{-1}C)^{-1} = \begin{bmatrix} 4/3 & 1 \\ 1 & 2\end{bmatrix}^{-1} = \frac{3}{5} \begin{bmatrix} 2 & -1 \\ -1 & 4/3 \end{bmatrix}, \\
Y &= -B^{-1}CV = -\frac{1}{5} \begin{bmatrix} 2 & -1 \\
4 & -2 \end{bmatrix}, \\
Z &= -VEB^{-1} = -\frac{1}{5} \begin{bmatrix} -2 & 4 \\ 1 & -2\end{bmatrix}, \\
X &= B^{-1}(I - CZ) = \frac{1}{5} \begin{bmatrix} 4 & -3 \\ -3 & 6 \end{bmatrix}.
\end{align*}

Thus we obtain
\[
A^{-1} = \frac{1}{5} \begin{bmatrix} 4&-3&2&-1\\-3 & 6 & -4 & 2 \\ 2 & -4 & 6 & -3 \\ -1 & 2 & -3 & 4 \end{bmatrix}.
\]"
62,"The given matrix is symmetric. Hence, there exists an orthogonal similarity matrix \(S\) which reduces \(A\) to its diagonal form \(D\).

Let
\[
S = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}.
\]

Then,
\[
S^{-1}AS = \begin{bmatrix} \cos \theta & \sin \theta \\ -\sin \theta & \cos \theta \end{bmatrix} \begin{bmatrix} 1 & 0.1 \\ 0.1 & 1 \end{bmatrix} \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}
= \begin{bmatrix} 1 + 0.1 \sin 2\theta & 0.1 \cos 2\theta \\ 0.1 \cos 2\theta & 1 - 0.1 \sin 2\theta \end{bmatrix} = D.
\]

Since \(D\) is a diagonal matrix, we choose \(\theta\) such that \(0.1 \cos 2\theta = 0\), which gives \(\theta = \pi/4\).

Therefore,
\[
A = SDS^{-1} \quad \text{where} \quad D = \begin{bmatrix} 1.1 & 0 \\ 0 & 0.9 \end{bmatrix}.
\]

Hence,
\[
A^{10} = SD^{10}S^{-1}
= \frac{1}{2} \begin{bmatrix} 1 & -1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} a & 0 \\ 0 & b \end{bmatrix} \begin{bmatrix} 1 & 1 \\ -1 & 1 \end{bmatrix} = \frac{1}{2} \begin{bmatrix} a + b & a - b \\ a - b & a + b \end{bmatrix},
\]
where \(a = (1.1)^{10}\) and \(b = (0.9)^{10}\)."
63,"We find
\[
A^2 = AA = \frac{1}{9} \begin{bmatrix} -1 & 8 & -4 \\ 8 & -1 & -4 \\ -4 & -4 & -7 \end{bmatrix}.
\]

\[
A^4 = A^2 A^2 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = I.
\]

Hence,
\[
A^8 = A^4 A^4 = I,
\]

\[
A^{10} = A^8 A^2 = A^2 = \frac{1}{9} \begin{bmatrix} -1 & 8 & -4 \\ 8 & -1 & -4 \\ -4 & -4 & -7 \end{bmatrix}.
\]"
64,"Since $\dfrac{1}{4} \| A \| = \dfrac{1}{2}$, we have
\[
\left[ \ln \left( I + \dfrac{1}{4} A \right) \right] Y = \left[ \dfrac{A}{4} - \dfrac{(A/4)^2}{2} + \dfrac{(A/4)^3}{3} - \dfrac{(A/4)^4}{4} + \cdots \right] Y.
\]

We get,
\[
A Y = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \end{bmatrix} = 3 \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]

Since
\[
A^2 Y = 3(2) \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad A^3 Y = 3(2)^2 \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad \ldots,
\]
we can generalize:
\[
A^m Y = 3(2)^{m-1} \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]

Therefore,
\[
\left[ \ln \left( I + \dfrac{1}{4} A \right) \right] Y = \left[ \dfrac{A}{4} - \dfrac{(A/4)^2}{2} + \dfrac{(A/4)^3}{3} - \dfrac{(A/4)^4}{4} + \cdots \right] Y.
\]

Substituting the known expression for $A^m Y$,
\[
\left[ \ln \left( I + \dfrac{1}{4} A \right) \right] Y = \dfrac{3}{2} \left( \dfrac{1}{2} - \dfrac{1}{8} + \dfrac{1}{24} - \dfrac{1}{64} + \cdots \right) \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]

Recognizing the Taylor series for $\ln(1 + x)$ with $x = \dfrac{1}{2}$:
\[
\ln(1 + x) = x - \dfrac{x^2}{2} + \dfrac{x^3}{3} - \dfrac{x^4}{4} + \cdots,
\]
we get:
\[
\left[ \ln \left( I + \dfrac{1}{4} A \right) \right] Y = \dfrac{3}{2} \ln \left( 1 + \dfrac{1}{2} \right) \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \dfrac{3}{2} \ln \left( \dfrac{3}{2} \right) \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]

Numerically,
\[
\dfrac{3}{2} \ln \left( \dfrac{3}{2} \right) \approx 0.6082.
\]

Thus,
\[
\left[ \ln \left( I + \dfrac{1}{4} A \right) \right] Y \approx 0.6082 \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
\]"
65,"The given matrix \(A\) is symmetric. Hence, \(\|A\|_2 = \rho(A)\).

The eigenvalues of \(A\) are given by
\[
|A - \lambda I| = \begin{vmatrix}
2 - \lambda & -1 & -1 & 1 \\
-1 & 2 - \lambda & 1 & -1 \\
-1 & 1 & 2 - \lambda & -1 \\
1 & -1 & -1 & 2 - \lambda
\end{vmatrix} = (1 - \lambda)^2 (\lambda^2 - 6\lambda + 5) = 0,
\]
which gives \(\lambda = 1, 1, 1, 5\). Hence, \(\|A\|_2 = \rho(A) = 5\).

 For \(\lambda = 5\), we have the eigensystem
\[
\begin{bmatrix}
-3 & -1 & -1 & 1 \\
-1 & -3 & 1 & -1 \\
-1 & 1 & -3 & -1 \\
1 & -1 & -1 & -3
\end{bmatrix} \mathbf{x} = 0.
\]

Solving this system, we get \(\mathbf{x} = [1, -1, -1, 1]^T\). Normalizing, such that \(\|\mathbf{x}\|_2 = (\sum x_i^2)^{1/2} = 1\), we obtain the eigenvector as
\[
\mathbf{x} = \frac{1}{\sqrt{2}} [1/2, -1/2, -1/2, 1/2]^T.
\]"
66,"We have
\[
A^{-1} = -\frac{1}{8} \begin{bmatrix}
-31 & 44 & -17 \\
44 & -56 & 20 \\
-17 & 20 & -7
\end{bmatrix} = \begin{bmatrix}
\frac{31/8}{44/8} & -\frac{44/8}{56/8} & \frac{17/8}{20/8} \\
-\frac{44/8}{56/8} & \frac{56/8}{20/8} & -\frac{20/8}{7/8} \\
\frac{17/8}{20/8} & -\frac{20/8}{7/8} & \frac{7/8}{}
\end{bmatrix}.
\]

\[
\|A\|_\infty = \text{maximum absolute row sum norm for } A = \max(14, 29, 50) = 50,
\]

\[
\|A^{-1}\|_\infty = \text{maximum absolute row sum norm for } A^{-1} = \max\left(\frac{31}{8} + \frac{44}{8} + \frac{17}{8}, \frac{44}{8} + \frac{56}{8} + \frac{20}{8}, \frac{17}{8} + \frac{20}{8} + \frac{7}{8}\right)
= \max\left(\frac{92}{8}, \frac{120}{8}, \frac{44}{8}\right) = 15.
\]

Therefore,
\[
\kappa(A) = \|A\|_\infty \|A^{-1}\|_\infty = 50 \cdot 15 = 750.
\]

 The given matrix is real and symmetric. Therefore, \(\kappa(A) = \lambda^* / \lambda^{**}\), where \(\lambda^*\) and \(\lambda^{**}\) are the largest and smallest eigenvalues in modulus of \(A\).

The characteristic equation of \(A\) is given by
\[
|A - \lambda I| = \begin{vmatrix}
1 - \lambda & 4 & 9 \\
4 & 9 - \lambda & 16 \\
9 & 16 & 25 - \lambda
\end{vmatrix} = -\lambda^3 + 35\lambda^2 + 94\lambda - 8 = 0.
\]

A root lies in (0, 0.1). Using the Newton-Raphson method}
A root lies in \((0, 0.1)\). Using the Newton-Raphson method,
\[
\lambda_{k+1} = \lambda_k - \frac{\lambda_k^3 - 35\lambda_k^2 + 94\lambda_k + 8}{3\lambda_k^2 - 70\lambda_k - 94}, \quad k = 0, 1, 2, \ldots
\]
with \(\lambda_0 = 0.1\), we get \(\lambda_1 = 0.09268\), \(\lambda_2 = 0.08257\), \(\lambda_3 = 0.08257\). The root correct to five places is \(0.08257\). Dividing the characteristic equation by \((x - 0.08257)\), we get the deflated polynomial as
\[
x^2 - 34.91743x - 96.888313 = 0
\]
whose roots are \(37.50092\), \(-2.58349\). Hence,
\[
\kappa(A) = \frac{37.50092}{0.08257} \approx 454.17.
\]"
67,"For the matrix \(A\), \(\text{cond}(A) = \|A\| \|A^{-1}\|\).

Here, we have
\[
A(\alpha) = \begin{bmatrix} 0.1\alpha & 0.1\alpha \\ 10 & 1.5 \end{bmatrix},
\]
and
\[
A^{-1}(\alpha) = \frac{1}{0.05\alpha} \begin{bmatrix} 1.5 & -0.1\alpha \\ -10 & 0.1\alpha \end{bmatrix}.
\]

Using maximum norm, we get
\[
\|A(\alpha)\| = \max [0.2 |\alpha|, 2.5],
\]
\[
\|A^{-1}(\alpha)\| = \max \left[ \frac{2 |\alpha| + 30}{|\alpha|}, \frac{2 |\alpha| + 20}{|\alpha|} \right] = \frac{2 |\alpha| + 30}{|\alpha|}.
\]

We have,
\[
\text{cond}(A(\alpha)) = \frac{1}{|\alpha|} [2 |\alpha| + 30] \max [0.2 |\alpha|, 2.5].
\]

We want to determine \(\alpha\) such that \(\text{cond}(A(\alpha))\) is minimum. We have
\[
\text{cond}(A(\alpha)) = \max \left[ 0.4 |\alpha| + 6, 5 + \frac{75}{|\alpha|} \right] = \text{minimum}.
\]

Choose \(\alpha\) such that
\[
0.4 |\alpha| + 6 = 5 + \frac{75}{|\alpha|},
\]
which gives \(|\alpha| = 12.5\). The minimum value of \(\text{cond}(A(\alpha)) = 11\)."
68,"The solution of the system of equations $Ax = \mathbf{b}$, is $\mathbf{x} = A^{-1}\mathbf{b}$.

if $\hat{\mathbf{x}} = \mathbf{x} + \delta \mathbf{x}$ is the solution when the disturbance $\delta \mathbf{b} = [\epsilon_1, \epsilon_2]^T$ is present on the right hand side, we obtain
\[
\hat{\mathbf{x}} = A^{-1}(\mathbf{b} + \delta \mathbf{b}).
\]

Therefore, we get,
\[
\delta \mathbf{x} = A^{-1} \delta \mathbf{b}, \quad \text{or} \quad \|\delta \mathbf{x}\| \leq \|A^{-1}\| \|\delta \mathbf{b}\|.
\]

Since,
\[
A^{-1} = \frac{1}{5} \begin{bmatrix}
1 & 2 \\
2 & -1
\end{bmatrix},
\]

we have
\[
\|A^{-1}\| \rho(A^{-1}) = \sqrt{0.2}.
\]

We also have
\[
\|\delta \mathbf{b}\| \leq \sqrt{2} \epsilon, \quad \text{where} \quad \epsilon = \max \{|\epsilon_1|, |\epsilon_2|\}.
\]

Hence, we obtain $\|\delta \mathbf{x}\| \leq \sqrt{0.4} \epsilon = \sqrt{0.4} \cdot 10^{-4}."
69,"The exact solution is $\mathbf{x} = [1 \quad 1]^T$. For $\mathbf{y} = [2 \quad 0]^T$, the residual is
\begin{align*}
\mathbf{r} &= A\mathbf{y} - \mathbf{b} \\
&= \begin{bmatrix} 1 & 1.001 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 0 \end{bmatrix} - \begin{bmatrix} 2.001 \\ 2 \end{bmatrix} = \begin{bmatrix} -0.001 \\ 0 \end{bmatrix} \\
\|\mathbf{r}\| &= 0.001, \quad \|\mathbf{x}\| = \sqrt{2}, \quad \|\mathbf{b}\| \approx 2.829,
\end{align*}
\begin{align*}
\|\mathbf{x} - \mathbf{y}\| &= \sqrt{2}, \\
\frac{\|\mathbf{x} - \mathbf{y}\|}{\|\mathbf{x}\|} &= \frac{\sqrt{2}}{\sqrt{2}} = 1. \quad \text{Also}, \quad \frac{\|\mathbf{r}\|}{\|\mathbf{b}\|} = \frac{0.001}{2.829} \approx 0.00035.
\end{align*}

Even though, $\|\mathbf{r}\| / \|\mathbf{b}\|$ is very small, $\mathbf{y}$ is not a solution of the problem."
70,"Let $\hat{\mathbf{x}}$ be the computed solution, when the right hand side vector is in error by $\delta \mathbf{b}$. Writing $\mathbf{x} = \mathbf{x} + \delta \mathbf{x}$, we have
\[
\hat{\mathbf{x}} = A^{-1}(\mathbf{b} + \delta \mathbf{b}) = A^{-1}\mathbf{b} + A^{-1}\delta \mathbf{b}.
\]

Hence,
\[
\delta \mathbf{x} = A^{-1}\delta \mathbf{b}.
\]

We find
\[
A^{-1} = 12 \begin{bmatrix}
6 & -20 & 15 \\
-20 & 75 & -60 \\
15 & -60 & 50
\end{bmatrix}.
\]

Hence,
\begin{align*}
\delta x_1 &= 12 [6 \delta b_1 - 20 \delta b_2 + 15 \delta b_3], \\
\delta x_2 &= 12 [-20 \delta b_1 + 75 \delta b_2 - 60 \delta b_3], \\
\delta x_3 &= 12 [15 \delta b_1 - 60 \delta b_2 + 50 \delta b_3], \\
|\delta x_1| &\leq 12(41\epsilon) = 492\epsilon, \\
|\delta x_2| &\leq 12(155\epsilon) = 1860\epsilon, \\
|\delta x_3| &\leq 12(125\epsilon) = 1500\epsilon.
\end{align*}

The error for the sum of the components, $y = x_1 + x_2 + x_3$, is given by
\[
\delta y = \delta x_1 + \delta x_2 + \delta x_3 = 12(6\delta b_1 - 5 \delta b_2 + 5 \delta b_3).
\]

Hence, the error bound is obtained as
\[
|\Delta y| \leq 12(1 + 5 + 5) \max_i |\delta b_i| \leq 132\epsilon.
\]"
71,"The given matrix is symmetric. Consider the eigenvalue problem $(A - \lambda I)\mathbf{x} = 0$.

The three term recurrence relation satisfying this equation is
\[
x_{j-1} - \lambda x_j + x_{j+1} = 0
\]
with $x_0 = 0$ and $x_7 = 0$. Setting $x_j = 2 \cos \theta$ and $x_j = \xi^j$, we get
\[
1 - 2 (\cos \theta) \xi + \xi^2 = 0
\]
whose solution is $\xi = \cos \theta \pm i \sin \theta = e^{\pm i\theta}$. Hence, the solution is
\[
x_j = C \cos j\theta + D \sin j\theta.
\]

Using the boundary conditions, we get
\begin{align*}
x_0 &= 0 = C \\
x_7 &= 0 = D \sin (7\theta) = \sin k\pi.
\end{align*}

We get
\[
\theta = \frac{k\pi}{7}, \quad k = 1, 2, 3, 4, 5, 6.
\]

The eigenvalues of $A$ are $2 \cos (\pi/7)$, $2 \cos (2\pi/7)$, $2 \cos (3\pi/7)$, $2 \cos (4\pi/7)$, $2 \cos (5\pi/7)$ and $2 \cos (6\pi/7)$. The smallest eigenvalue in magnitude of $A$ is $2 \cos (3\pi/7) = 2 |\cos (4\pi/7)|$. Hence,
\[
\rho(A^{-1}) = \frac{1}{2 \cos (3\pi/7)}.
\]"
72,"Since the given matrix \(A\) is an Hermitian matrix, its eigenvalues are real. By Gershgorin theorem, we have
\[
|\lambda| \leq \max [\sqrt{5} + 1, 2 + \sqrt{2}, \sqrt{5} + \sqrt{2}] = \sqrt{5} + \sqrt{2}.
\]

Hence, the eigenvalues lie in the interval \([-(\sqrt{5} + \sqrt{2}), (\sqrt{5} + \sqrt{2})]\), i.e., in the interval \((-3.65, 3.65)\).
The Euclidean norm of \(A\) is}
The Euclidean norm of \(A\) is
\[
\|A\| = \left( \sum |a_{ij}|^2 \right)^{1/2} = (1 + 5 + 4 + 2 + 5 + 2)^{1/2} = \sqrt{19}.
\]"
73,"We have
\[
AB = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} \beta_1 & 1 \\ 0 & \beta_2 \end{bmatrix} = \begin{bmatrix} \beta_1 & 1 + \beta_2 \\ \beta_1 & 1 + \beta_2 \end{bmatrix}
\]
which has eigenvalues 0 and \( 1 + \beta_1 + \beta_2 \).

Now, \(\rho(AB) = |1 + \beta_1 + \beta_2|\).

Hence, for \( |1 + \beta_1 + \beta_2| < 1 \), \((AB)^k \to 0\) as \( k \to \infty \)."
74,"The eigenvalues of \( A \) are \( \lambda_1 = 0 \), \( \lambda_2 = 2\sqrt{13} \), \( \lambda_3 = -2\sqrt{13} \).

Let \( S \) be the matrix having its columns as eigenvectors corresponding to the eigenvalues of \( A \). Then, we have
\[
S^{-1} A S = D, \quad \text{and} \quad S D S^{-1} = A.
\]
where \( D \) is the diagonal matrix with the eigenvalues of \( A \) as the diagonal entries.

We have, when \( m \) is odd,
\[
S^{-1} A^m S = D^m = \begin{bmatrix}
0 & 0 & 0 \\
0 & (2\sqrt{13})^m & 0 \\
0 & 0 & (-2\sqrt{13})^m
\end{bmatrix}
\]

\[
= (2\sqrt{13})^{m-1} \begin{bmatrix}
0 & 0 & 0 \\
0 & 2\sqrt{13} & 0 \\
0 & 0 & -2\sqrt{13}
\end{bmatrix} = (2\sqrt{13})^{m-1} D.
\]

Hence,
\[
A^m = (2\sqrt{13})^{m-1} S D S^{-1} = (2\sqrt{13})^{m-1} A.
\]
Now,
\[
f(A) = e^A - e^{-A} = 2 \left[ A + \frac{1}{3!} A^3 + \frac{1}{5!} A^5 + \cdots \right]
\]

\[
= 2 \left[ A + \frac{(2\sqrt{13})^2}{3!} A + \frac{(2\sqrt{13})^4}{5!} A + \cdots \right]
\]

\[
= 2 \left[ 1 + \frac{(2\sqrt{13})^2}{3!} + \frac{(2\sqrt{13})^4}{5!} + \cdots \right] A
\]

\[
= \frac{2}{2\sqrt{13}} \left[ \frac{(2\sqrt{13})^3}{3!} + \frac{(2\sqrt{13})^5}{5!} + \cdots \right] A
\]

\[
= \frac{1}{\sqrt{13}} \sinh(2\sqrt{13}) A.
\]"
75,"We have the equation
\[
T^{-1} A T = D
\]
or
\[
A T = T D
\]
where \( D = \text{diag}[\lambda_1, \lambda_2, \lambda_3] \), and \( \lambda_1, \lambda_2, \lambda_3 \) are the eigenvalues of \( A \).

We have,
\[
\begin{bmatrix}
1 & -2 & 3 \\
6 & -13 & 18 \\
4 & -10 & 14
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 1 \\
3 & 3 & 4 \\
2 & 2 & 3
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 1 \\
3 & 3 & 4 \\
2 & 2 & 3
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & 0 \\
0 & \lambda_2 & 0 \\
0 & 0 & \lambda_3
\end{bmatrix}.
\]

Comparing the corresponding elements on both sides, we obtain \( \lambda_1 = 1 \), \( \lambda_2 = -1 \), \( \lambda_3 = 2 \).

Since \( T \) transforms \( A \) to the diagonal form, \( T \) is the matrix of the corresponding eigenvectors. Hence, the eigenvalues are \( 1, -1, 2 \) and the corresponding eigenvectors are \( [1 \, 3 \, 2]^T \), \( [0 \, 3 \, 2]^T \), and \( [1 \, 4 \, 3]^T \) respectively."
76,"The eigenvalues of \(A\) are \(1, -1, 0\) and the matrix of eigenvectors is
\[
S = \begin{bmatrix}
0 & 1 & 1/2 \\
1 & -1 & -1 \\
1/2 & 0 & 0
\end{bmatrix}
\quad \text{and} \quad
S^{-1} = \begin{bmatrix}
0 & 0 & 2 \\
2 & 1 & -2 \\
-2 & -2 & 4
\end{bmatrix}.
\]

We have \(S^{-1}(A + \epsilon B) S = S^{-1} A S + \epsilon S^{-1} B S = D + \epsilon P\)
where \(D\) is a diagonal matrix with \(1, -1, 0\) on the diagonal and
\[
P = S^{-1} B S = \begin{bmatrix}
1 & -4 & -3 \\
-1/2 & 2 & 3/2 \\
2 & -8 & -6
\end{bmatrix}.
\]

The eigenvalues of \(A + \epsilon B, (\epsilon < 1)\) lie in the union of the disks
\[
|\lambda(\epsilon) - \lambda_i(0)| \leq \epsilon \cdot \text{cond}_\infty(S) \|P\|_\infty.
\]

Since, \(\text{cond}_\infty(S) = \|S\|_\infty \|S^{-1}\|_\infty = 24\) and \(\|P\|_\infty = 16\), we have the union of the disks as
\[
|\lambda(\epsilon) - \lambda_i(0)| \leq 384\epsilon
\]
where \(\lambda_1(0) = 1\), \(\lambda_2(0) = -1\) and \(\lambda_3(0) = 0\).

A more precise result is obtained using the Gershgorin theorem. We have the union of disks as
\[
|\lambda(\epsilon) - \lambda_i(0) - \epsilon p_{ii}| \leq \epsilon \sum_{j=1, j \neq i}^3 |p_{ij}|, \quad \text{or} \quad |\lambda(\epsilon) - 1 - \epsilon| \leq 7\epsilon,
\]

\[
|\lambda(\epsilon) + 1 - 2\epsilon| \leq 2\epsilon, \quad \text{and} \quad |\lambda(\epsilon) + 6\epsilon| \leq 10\epsilon.
\]

The eigenvalues of \(A\) are real and \(\epsilon B\) represents a perturbation. Hence, we assume that the eigenvalues of \(A + \epsilon B\) are also real. We now have the bounds for the eigenvalues as
\begin{align*}
-6\epsilon &\leq \lambda_1(\epsilon) - \lambda_1(0) \leq 8\epsilon, \\
0 &\leq \lambda_2(\epsilon) - \lambda_2(0) \leq 4\epsilon, \\
-16\epsilon &\leq \lambda_3(\epsilon) - \lambda_3(0) \leq 4\epsilon.
\end{align*}

Alternatively, we have that the eigenvalues lie in the interval
\[
-16\epsilon \leq \lambda(\epsilon) - \lambda_i(0) \leq 8\epsilon.
\]"
77,"The eigenvalues of \(A\) are \(1/2, 5/2\) and \(-1\). The corresponding eigenvectors are found to be
\([1 \, -1 \, 0]^T\), \([3 \, 1 \, 0]^T\), and \([0 \, 0 \, 1]^T\).

Hence, the matrix
\[
S = \begin{bmatrix}
1 & 3 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\]
reduces \(A\) to its diagonal form.

We have
\[
S^{-1} \tilde{A} S = S^{-1} (A + 10^{-2} B) S = S^{-1} A S + 10^{-2} S^{-1} B S
\]
where
\[
B = \begin{bmatrix}
-1 & -1 & 1 \\
-1 & 1 & -1 \\
1 & -1 & 1
\end{bmatrix}.
\]

We also have
\[
S^{-1} = \frac{1}{4} \begin{bmatrix}
1 & -3 & 0 \\
1 & 1 & 0 \\
0 & 0 & 4
\end{bmatrix}, \quad S^{-1} B S = \begin{bmatrix}
2 & 2 & 1 \\
0 & 0 & 0 \\
2 & 2 & 1
\end{bmatrix}.
\]

Therefore,
\[
S^{-1} \tilde{A} S = \begin{bmatrix}
1/2 & 0 & 0 \\
0 & 5/2 & 0 \\
0 & 0 & -1
\end{bmatrix} + 10^{-2} \begin{bmatrix}
2 & 2 & 1 \\
0 & 0 & 0 \\
2 & 2 & 1
\end{bmatrix}.
\]

By Gershgorin's theorem, we obtain that the eigenvalues of \(\tilde{A}\) lies in the union of the circles
\[
\left| \tilde{\lambda} - \left(\frac{1}{2} + 2 \times 10^{-2}\right) \right| \leq 3 \times 10^{-2},
\]

\[
\left| \tilde{\lambda} - \frac{5}{2} \right| = 0,
\]

\[
\left| \tilde{\lambda} - (-1 + 10^{-2}) \right| \leq 4 \times 10^{-2}
\]
which are disjoint bounds.

Hence, we have
\begin{align*}
\lambda_1 &= \frac{1}{2}, |\tilde{\lambda}_1 - \lambda_1| \leq 5 \times 10^{-2}, \\
\lambda_2 &= \frac{5}{2}, |\tilde{\lambda}_2 - \lambda_2| = 0, \\
\lambda_3 &= -1, |\tilde{\lambda}_3 - \lambda_3| \leq 5 \times 10^{-2}.
\end{align*}"
78,"From the given equation
\[
y_n = (\mathbf{I} + \alpha A + \alpha^2 A^2) y_{n-1}
\]
we get
\[
y_n = (\mathbf{I} + \alpha A + \alpha^2 A^2)^n y_0
\]
where \(y_0\) is arbitrary.

Hence, \(\lim_{n \to \infty} y_n \to 0\) if and only if \(\rho(\mathbf{I} + \alpha A + \alpha^2 A^2) < 1\).

The eigenvalues of
\[
A = \begin{bmatrix}
3/2 & 1/2 \\
1/2 & 3/2
\end{bmatrix}
\]
are 1 and 2. Hence, the eigenvalues of \(\mathbf{I} + \alpha A + \alpha^2 A^2\) are \(1 + \alpha + \alpha^2\) and \(1 + 2\alpha + 4\alpha^2\). We require that
\[
|1 + \alpha + \alpha^2| < 1, \quad \text{and} \quad |1 + 2\alpha + 4\alpha^2| < 1.
\]

The first inequality gives
\[
-1 < 1 + \alpha + \alpha^2 < 1, \quad \text{or} \quad -2 < \alpha(1 + \alpha) < 0.
\]

This gives,
\[
\alpha < 0, \alpha + 1 > 0, \quad \text{or} \quad \alpha \in (-1, 0).
\]

The second inequality gives
\[
-1 < 1 + 2\alpha + 4\alpha^2 < 1, \quad \text{or} \quad -2 < 2\alpha (1 + 2\alpha) < 0.
\]

This given, \(\alpha < 0, 1 + 2\alpha > 0, \text{or} \alpha \in (-1/2, 0)\).

Hence, the required interval is \((-1/2, 0)\)."
79,"Let \(\overline{\mathbf{x}}\) be a computed solution of \(\mathbf{Ax} = \mathbf{b}\) and let \(\mathbf{r} = \mathbf{b} - \mathbf{A}\overline{\mathbf{x}}\) be the residual. Then,
\[
\mathbf{A}(\mathbf{x} - \overline{\mathbf{x}}) = \mathbf{Ax} - \mathbf{A}\overline{\mathbf{x}} = \mathbf{b} - \mathbf{A}\overline{\mathbf{x}} = \mathbf{r}, \quad \text{or} \quad \mathbf{A}\delta\mathbf{x} = \mathbf{r}.
\]

Inverting \(\mathbf{A}\), we have
\[
\delta\mathbf{x} = \mathbf{A}^{-1} \mathbf{r} \approx \mathbf{Br}.
\]

The next approximation to the solution is then given by \(\mathbf{x} = \overline{\mathbf{x}} + \delta\mathbf{x}\).

We have in the present problem
\begin{align*}
\mathbf{x}_0 &= \mathbf{Bb} = [-20 \, 182 \, -424 \, 283]^T, \\
\mathbf{r} &= \mathbf{b} - \mathbf{A}\mathbf{x}_0 = [-0.2667 \, -0.2 \, -0.1619 \, -0.1369]^T, \\
\delta\mathbf{x} &= \mathbf{Br} = [-0.0294 \, -2.0443 \, 3.9899 \, -3.0793]^T, \\
\mathbf{x} &= \mathbf{x}_0 + \delta\mathbf{x} = [-20.0294 \, 179.9557 \, -420.0101 \, 279.9207]^T \\
&\approx [-20 \, 180 \, -420 \, 280]^T
\end{align*}
since an integer solution is required. It can be verified that this is the exact solution."
80,"The given iteration scheme is
\[
\mathbf{x}^{(n+1)} = \mathbf{x}^{(n)} + \alpha (\mathbf{A} \mathbf{x}^{(n)} - \mathbf{y}) = (\mathbf{I} + \alpha \mathbf{A}) \mathbf{x}^{(n)} - \alpha \mathbf{y}.
\]

Setting \(n = 0, 1, 2, \ldots\), we obtain
\[
\mathbf{x}^{(n+1)} = \mathbf{q}^{n+1} \mathbf{x}^{(0)} - \alpha [\mathbf{I} + \mathbf{q} + \cdots + \mathbf{q}^n] \mathbf{y}
\]
where
\[
\mathbf{q} = \mathbf{I} + \alpha \mathbf{A}.
\]

The iteration scheme will converge if and only if \(\rho(\mathbf{I} + \alpha \mathbf{A}) < 1\).

The eigenvalues of \([\mathbf{I} + \alpha \mathbf{A}]\) are \(\lambda_1 = 1 + \alpha\) and \(\lambda_2 = 1 + 4\alpha\).

We choose \(\alpha\) such that
\[
|1 + \alpha| = |1 + 4\alpha|
\]
which gives
\[
\alpha = -0.4.
\]"
81,"Gauss-Seidel method, in error format, is given by
\[
(\mathbf{D} + \mathbf{L}) \mathbf{v}^{(k)} = \mathbf{r}^{(k)}, \quad \text{where} \quad \mathbf{v}^{(k)} = \mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}, \quad \mathbf{r}^{(k)} = \mathbf{b} - \mathbf{A} \mathbf{x}^{(k)}.
\]

We have the following approximations.

\begin{align*}
\mathbf{r}^{(0)} &= \begin{bmatrix} 15 \\ 10 \\ 2.1 \end{bmatrix}, \quad
\begin{bmatrix} 4 & 0 & 0 \\ 1 & 3 & 0 \\ 3 & 2 & 6 \end{bmatrix} \mathbf{v}^{(0)} = \begin{bmatrix} 15 \\ 10 \\ 2.1 \end{bmatrix}, \quad \mathbf{v}^{(0)} = \begin{bmatrix} 0.375 \\ 0.2083 \\ 0.0931 \end{bmatrix} \quad (\text{By forward substitution}),
\\
\mathbf{x}^{(1)} &= \mathbf{x}^{(0)} + \mathbf{v}^{(0)} = \begin{bmatrix} 0.1 \\ 0.8 \\ 0.5 \end{bmatrix} + \begin{bmatrix} 0.375 \\ 0.2083 \\ 0.0931 \end{bmatrix} = \begin{bmatrix} 0.475 \\ 1.0083 \\ 0.5931 \end{bmatrix}, \quad \mathbf{r}^{(1)} = \begin{bmatrix} -0.5097 \\ -0.0093 \\ -0.0002 \end{bmatrix};
\end{align*}

\begin{align*}
\begin{bmatrix} 4 & 0 & 0 \\ 1 & 3 & 0 \\ 3 & 2 & 6 \end{bmatrix} \mathbf{v}^{(1)} &= \begin{bmatrix} -0.5097 \\ -0.0093 \\ -0.0002 \end{bmatrix}, \quad \mathbf{v}^{(1)} = \begin{bmatrix} -0.1274 \\ 0.0115 \\ 0.0598 \end{bmatrix};
\\
\mathbf{x}^{(2)} &= \mathbf{x}^{(1)} + \mathbf{v}^{(1)} = \begin{bmatrix} 0.3476 \\ 1.0198 \\ 0.6529 \end{bmatrix}, \quad \mathbf{r}^{(2)} = \begin{bmatrix} -0.0829 \\ -0.0599 \\ 0.0002 \end{bmatrix};
\end{align*}

\begin{align*}
\begin{bmatrix} 4 & 0 & 0 \\ 1 & 3 & 0 \\ 3 & 2 & 6 \end{bmatrix} \mathbf{v}^{(2)} &= \begin{bmatrix} -0.0829 \\ -0.0599 \\ 0.0002 \end{bmatrix}, \quad \mathbf{v}^{(2)} = \begin{bmatrix} -0.0207 \\ -0.0131 \\ 0.0148 \end{bmatrix}, \quad \mathbf{x}^{(3)} = \mathbf{x}^{(2)} + \mathbf{v}^{(2)} = \begin{bmatrix} 0.3269 \\ 1.0067 \\ 0.6677 \end{bmatrix}.
\end{align*}

\subsection*{Direct method}
We write
\begin{align*}
x_1^{(k+1)} &= \frac{1}{4} [4 - 2x_2^{(k)} - x_3^{(k)}], \quad x_2^{(k+1)} = \frac{1}{3} [4 - x_1^{(k+1)} - x_3^{(k)}], \\
x_3^{(k+1)} &= \frac{1}{6} [7 - 3x_1^{(k+1)} - 2x_2^{(k+1)}].
\end{align*}

Using \(\mathbf{x}^{(0)} = [0.1 \, 0.8 \, 0.5]^T\), we obtain the following approximations:
\begin{align*}
x_1^{(1)} &= 0.475, \quad x_2^{(1)} = 1.0083, \quad x_3^{(1)} = 0.5931, \\
x_1^{(2)} &= 0.3476, \quad x_2^{(2)} = 1.0198, \quad x_3^{(2)} = 0.6529, \\
x_1^{(3)} &= 0.3269, \quad x_2^{(3)} = 1.0067, \quad x_3^{(3)} = 0.6677.
\end{align*}"
82,"The Jacobi method for the given system is
\[
\mathbf{x}_{n+1} = -\begin{bmatrix} 0 & k \\ 2k & 0 \end{bmatrix} \mathbf{x}_n + \mathbf{b} = \mathbf{Mx}_n + \mathbf{b}.
\]

The necessary and sufficient condition for convergence of the Jacobi method is \(\rho(\mathbf{M}) < 1\).

The eigenvalues of \(\mathbf{M}\) are given by the equation
\[
\lambda^2 - 2k\lambda = 0.
\]

Hence,
\[
\rho(\mathbf{M}) = \sqrt{2} |k|.
\]

The required condition is therefore
\[
\sqrt{2} |k| < 1 \quad \text{or} \quad |k| < 1/\sqrt{2}.
\]

The optimal relaxation factor is
\[
\omega_{\text{opt}} = \frac{2}{1 + \sqrt{1 - \mu^2}} = \frac{2}{1 + \sqrt{1 - 2k^2}}
\]
\[
= \frac{2}{1 + \sqrt{7/8}} \approx 1.033 \quad \text{for} \quad k = 0.25.
\]"
83,"The iteration matrix of the Jacobi method is
\[
\mathbf{H} = -\mathbf{D}^{-1}(\mathbf{L} + \mathbf{U}) = -(\mathbf{L} + \mathbf{U})
\]
\[
= -\begin{bmatrix}
0 & 2 & -2 \\
1 & 0 & 1 \\
2 & 2 & 0
\end{bmatrix}.
\]

The characteristic equation of \(\mathbf{H}\) is
\[
|\lambda \mathbf{I} - \mathbf{H}| = \begin{vmatrix}
\lambda & -2 & 2 \\
-1 & \lambda & -1 \\
-2 & -2 & \lambda
\end{vmatrix} = \lambda^3 = 0.
\]

The eigenvalues of \(\mathbf{H}\) are \(\lambda = 0, 0, 0\) and \(\rho(\mathbf{H}) < 1\). The iteration converges.

The iteration matrix of the Gauss-Seidel method is
\[
\mathbf{H} = -(\mathbf{D} + \mathbf{L})^{-1} \mathbf{U}
\]
\[
= -\begin{bmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 2 & 2 & 1 \end{bmatrix}^{-1} \begin{bmatrix} 0 & 2 & -2 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}
\]
\[
= -\begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ -2 & -2 & 1 \end{bmatrix} \begin{bmatrix} 0 & 2 & -2 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix} = -\begin{bmatrix} 0 & 2 & -2 \\ 0 & -2 & -3 \\ 0 & 0 & -2 \end{bmatrix}.
\]

The eigenvalues of \(\mathbf{H}\) are \(\lambda = 0, 2, 2\) and \(\rho(\mathbf{H}) > 1\).
The iteration diverges."
84,"We solve the system of equations directly.
\begin{align*}
x^{(k+1)} &= \frac{1}{2} [1 + y^{(k)}], \\
y^{(k+1)} &= \frac{1}{2} [x^{(k+1)} + z^{(k)}], \\
z^{(k+1)} &= \frac{1}{2} [y^{(k+1)} + w^{(k)}], \\
w^{(k+1)} &= \frac{1}{2} [1 + z^{(k+1)}].
\end{align*}

With \(\mathbf{x}^{(0)} = [0.5 \, 0.5 \, 0.5 \, 0.5]^T\), we obtain the following approximate values.
\begin{align*}
\mathbf{x}^{(1)} &= [0.75 \, 0.625 \, 0.5625 \, 0.78125]^T, \\
\mathbf{x}^{(2)} &= [0.8125 \, 0.6875 \, 0.7344 \, 0.8672]^T, \\
\mathbf{x}^{(3)} &= [0.84375 \, 0.7859 \, 0.8203 \, 0.9140]^T.
\end{align*}

The Gauss-Seidel method is \(\mathbf{x}^{(k+1)} = \mathbf{H} \mathbf{x}^{(k)} + \mathbf{c}\), where
\[
\mathbf{H} = -(\mathbf{D} + \mathbf{L})^{-1} \mathbf{U} = \begin{bmatrix}
-\frac{1}{2} & 0 & 0 & 0 \\
\frac{1}{4} & -\frac{1}{2} & 0 & 0 \\
0 & \frac{1}{4} & -\frac{1}{2} & 0 \\
0 & 0 & \frac{1}{4} & -\frac{1}{2}
\end{bmatrix} \begin{bmatrix}
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1 \\
0 & 0 & 0 & 0
\end{bmatrix} = \begin{bmatrix}
0 & \frac{1}{2} & 0 & 0 \\
-\frac{1}{4} & 0 & \frac{1}{2} & 0 \\
0 & -\frac{1}{4} & 0 & \frac{1}{2} \\
0 & 0 & -\frac{1}{4} & 0
\end{bmatrix},
\]
\[
\mathbf{c} = (\mathbf{D} + \mathbf{L})^{-1} \mathbf{b} = \frac{1}{16} \begin{bmatrix} 8 & 4 & 2 & 9 \end{bmatrix}^T.
\]

The eigenvalues of \(\mathbf{H}\) are given by the equation
\[
|\mathbf{H} - \lambda \mathbf{I}| = \lambda^4 - \frac{3}{4} \lambda^2 + \frac{1}{16} = 0,
\]
whose solution is \(\lambda = 0, 0, (3 \pm \sqrt{5})/8\). The eigenvalues lie in the interval
\[
[a, b] = \left[ \frac{3 - \sqrt{5}}{8}, \frac{3 + \sqrt{5}}{8} \right].
\]

We have \(\rho(\mathbf{H}_{\text{GS}}) = \frac{3 + \sqrt{5}}{8}\) and rate of convergence \((G-S) = -\log_{10} \left( \frac{3 + \sqrt{5}}{8} \right) = 0.1841\).

We have
\[
\gamma = \frac{2}{2 - \alpha - b} = \frac{2}{2 - (3/4)} = \frac{8}{5} = 1.6, \text{ and}
\]
\[
\mathbf{H}_\gamma = \gamma \mathbf{H} + (1 - \gamma) \mathbf{I} = -0.6 \mathbf{I} + 1.6 \mathbf{H} = \begin{bmatrix}
-0.6 & 0.8 & 0 & 0 \\
-0.4 & -0.6 & 0.8 & 0 \\
0 & -0.4 & -0.6 & 0.8 \\
0 & 0 & -0.4 & -0.6
\end{bmatrix}.
\]

The extrapolation iteration scheme is given by \(\mathbf{x}^{(k+1)} = \mathbf{H}_\gamma \mathbf{x}^{(k)} + \gamma \mathbf{c}\).
With
\begin{align*}
\mathbf{x}^{(0)} &= [0.5 \, 0.5 \, 0.5 \, 0.5]^T, \text{ we get} \\
\mathbf{x}^{(1)} &= [0.9 \, 0.7 \, 0.6 \, 0.95]^T, \\
\mathbf{x}^{(2)} &= [0.82 \, 0.74 \, 0.89 \, 0.9]^T, \\
\mathbf{x}^{(3)} &= [0.9 \, 1.036 \, 0.872 \, 0.99]^T.
\end{align*}

We also have \(\rho(\mathbf{H}_\gamma) = 1 - |\gamma| d\), where \(d\) is the distance of 1 from \([a, b] = \left[ \frac{3 - \sqrt{5}}{8}, \frac{3 + \sqrt{5}}{8} \right]\)
which is equal to 0.3455. Hence,
\[
\rho(\mathbf{H}_\gamma) = 1 - (1.6) (0.3455) = 0.4472,
\]
and rate of convergence = \(-\log_{10} (0.4472) = 0.3495\). The maximum absolute errors in the Gauss-Seidel method and the extrapolation are respectively (after three iterations) 0.2109 and 0.1280."
85,"We write the iteration method in the form
\[
\mathbf{x}^{(n+1)} = \mathbf{M} \mathbf{x}^{(n)} + \mathbf{c}.
\]

For Jacobi method, we have
\[
\mathbf{M}_J = -\mathbf{D}^{-1}(\mathbf{L} + \mathbf{U}).
\]

For Gauss-Seidel method, we have
\[
\mathbf{M}_{GS} = -(\mathbf{D} + \mathbf{L})^{-1} \mathbf{U}.
\]

The iteration method converges if and only if \(\rho(\mathbf{M}) < 1\).

For Jacobi method, we find
\[
\mathbf{M}_J = -\begin{bmatrix}
1/4 & 0 & 0 \\
0 & 1/5 & 0 \\
0 & 0 & 1/10
\end{bmatrix}
\begin{bmatrix}
0 & 0 & 2 \\
0 & 0 & 2 \\
5 & 4 & 0
\end{bmatrix}
= \begin{bmatrix}
0 & 0 & -1/2 \\
0 & 0 & -2/5 \\
-1/2 & -2/5 & 0
\end{bmatrix}.
\]

The eigenvalues of \(\mathbf{M}_J\) are \(\mu = 0\) and \(\mu = \pm \sqrt{0.41}\).

For Gauss-Seidel method, we find
\[
\mathbf{M}_{GS} = -\begin{bmatrix}
4 & 0 & 0 \\
0 & 5 & 0 \\
5 & 4 & 10
\end{bmatrix}^{-1}
\begin{bmatrix}
0 & 0 & 2 \\
0 & 0 & 2 \\
0 & 0 & 0
\end{bmatrix}
= -\frac{1}{200}
\begin{bmatrix}
4 & 0 & 0 \\
0 & 5 & 0 \\
5 & 4 & 10
\end{bmatrix}^{-1}
\begin{bmatrix}
0 & 0 & 2 \\
0 & 0 & 2 \\
0 & 0 & 0
\end{bmatrix}
= -\frac{1}{200}
\begin{bmatrix}
0 & 0 & 100 \\
0 & 0 & 0 \\
0 & 0 & -82
\end{bmatrix}.
\]

Eigenvalues of \(\mathbf{M}_{GS}\) are 0, 0, 0.41.
Hence, the convergence factor (rate of convergence) for Gauss-Seidel method is
\[
v = -\log_{10} \rho(\mathbf{M}_{GS}) = -\log_{10} (0.41) \approx 0.387.
\]

then 
\[
\omega_{\text{opt}} = \frac{2}{\mu} (1 - \sqrt{1 - \mu^2}), \quad \text{where} \quad \mu = \rho(\mathbf{M}_J)
\]
\[
= \frac{2}{0.41} (1 - \sqrt{1 - 0.41}) \approx 1.132.
\]

The SOR method becomes
\[
\mathbf{x}^{(n+1)} = \mathbf{M} \mathbf{x}^{(n)} + \mathbf{c}
\]
where
\[
\mathbf{M} = (\mathbf{D} + \omega_{\text{opt}} \mathbf{L})^{-1} [(1 - \omega_{\text{opt}}) \mathbf{D} - \omega_{\text{opt}} \mathbf{U}]
\]
\[
\begin{bmatrix}
4 & 0 & 0 \\
0 & 5 & 0 \\
5.660 & 4.528 & 10
\end{bmatrix}^{-1}
\begin{bmatrix}
-0.528 & 0 & -2.264 \\
0 & -0.660 & -2.264 \\
0 & 0 & -1.320
\end{bmatrix}
\]

\[
\frac{1}{200}
\begin{bmatrix}
50 & 0 & 0 \\
0 & 40 & 0 \\
-28.3 & -18.112 & 20
\end{bmatrix}
\begin{bmatrix}
-0.528 & 0 & -2.264 \\
0 & -0.660 & -2.264 \\
0 & 0 & -1.320
\end{bmatrix}
\]

\[
\begin{bmatrix}
-0.1320 & 0 & -0.5660 \\
0 & -0.1320 & -0.4528 \\
0.0747 & 0.0598 & 0.3944
\end{bmatrix}
\]

\[
c = w_{\text{opt}} (D + w_{\text{opt}} L)^{-1} b
\]

\[
\frac{1.132}{200}
\begin{bmatrix}
50 & 0 & 0 \\
0 & 40 & 0 \\
-28.3 & -18.112 & 20
\end{bmatrix}
\begin{bmatrix}
4 \\
-3 \\
2
\end{bmatrix}
=
\begin{bmatrix}
1.132 \\
-0.6792 \\
-0.1068
\end{bmatrix}
\]"
86,"For the given system of equations, we obtain :

\[
\mathbf{x}^{(n+1)} = -\begin{bmatrix}
\frac{1}{4} & 0 & 0 \\
0 & \frac{1}{5} & 0 \\
0 & 0 & \frac{1}{3}
\end{bmatrix}
\begin{bmatrix}
0 & 1 & 2 \\
3 & 0 & 1 \\
1 & 1 & 0
\end{bmatrix}
\mathbf{x}^{(n)} +
\begin{bmatrix}
\frac{1}{4} & 0 & 0 \\
0 & \frac{1}{5} & 0 \\
0 & 0 & \frac{1}{3}
\end{bmatrix}
\begin{bmatrix}
4 \\
7 \\
3
\end{bmatrix}
\]

\[
= -\begin{bmatrix}
0 & \frac{1}{4} & \frac{1}{2} \\
-\frac{3}{5} & 0 & \frac{1}{5} \\
\frac{1}{3} & \frac{1}{3} & 0
\end{bmatrix}
\mathbf{x}^{(n)} + \begin{bmatrix}
\frac{7}{5} \\
1 \\
1
\end{bmatrix}
\]

Starting with $\mathbf{x}^{(0)} = 0$, we get

\begin{align*}
\mathbf{x}^{(1)} &= (1 \quad 1.4 \quad 1)^T, \\
\mathbf{x}^{(2)} &= (0.15 \quad 0.6 \quad 0.2)^T, \\
\mathbf{x}^{(3)} &= (0.75 \quad 1.27 \quad 0.75)^T.
\end{align*}

Gauss-Seidel Iteration Scheme

\[
\mathbf{x}^{(n+1)} = -\begin{bmatrix}
4 & 0 & 0 \\
3 & 5 & 0 \\
1 & 1 & 3
\end{bmatrix}^{-1}
\begin{bmatrix}
0 & 1 & 2 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
\mathbf{x}^{(n)} +
\begin{bmatrix}
4 & 0 & 0 \\
3 & 5 & 0 \\
1 & 1 & 3
\end{bmatrix}^{-1}
\begin{bmatrix}
4 \\
7 \\
3
\end{bmatrix}
\]

\[
= -\frac{1}{60}\begin{bmatrix}
0 & 15 & 30 \\
0 & -9 & -6 \\
0 & -2 & -8
\end{bmatrix}
\mathbf{x}^{(n)} + \frac{1}{60}\begin{bmatrix}
60 \\
48 \\
24
\end{bmatrix}
\]

Starting with $\mathbf{x}^{(0)} = 0$, we get

\begin{align*}
\mathbf{x}^{(1)} &= (1.0 \quad 0.8 \quad 0.4)^T, \quad \mathbf{x}^{(2)} = (0.6 \quad 0.96 \quad 0.48)^T, \\
\mathbf{x}^{(3)} &= (0.52 \quad 0.992 \quad 0.496)^T.
\end{align*}

Exact solution of the given system of equations is $[0.5 \quad 1 \quad 0.5]^T$.

The Jacobi iteration matrix is

\[
\mathbf{M}_J = \begin{bmatrix}
0 & -\frac{1}{4} & -\frac{1}{2} \\
-\frac{3}{5} & 0 & -\frac{1}{5} \\
\frac{1}{3} & -\frac{1}{3} & 0
\end{bmatrix}
\]

The characteristic equation of $\mathbf{M}_J$ is given by

\[
60\lambda^3 - 23\lambda + 7 = 0.
\]

The equation has one real root in $(-0.8, 0)$ and a complex pair. The real root can be obtained by the Newton-Raphson method

\[
\lambda_{k+1} = \lambda_k - \frac{60\lambda_k^3 - 23\lambda_k + 7}{180\lambda_k^2 - 23}, \quad k = 0, 1, 2, \ldots
\]

Starting with $\lambda_0 = -0.6$, we obtain the successive approximations to the root as $-0.7876, -0.7402, -0.7361, -0.7361$. The complex pair are the roots of $60\lambda^2 - 44.1660\lambda + 9.5106 = 0$, which is obtained as $0.3681 \pm 0.1518i$. The magnitude of this pair is 0.3981. Hence, $\rho(\mathbf{M}_J) = 0.7876$ and $v = -\log_{10}(0.7876) = 0.1037$.

The Gauss-Seidel iteration matrix is

\[
\mathbf{M}_{GS} = \begin{bmatrix}
0 & -\frac{1}{4} & -\frac{1}{2} \\
0 & \frac{3}{20} & \frac{1}{10} \\
0 & \frac{1}{30} & \frac{2}{15}
\end{bmatrix}
\]

The characteristic equation of $\mathbf{M}_{GS}$ is obtained as

\[
60\lambda^3 - 17\lambda^2 + \lambda = 0
\]

whose roots are 0, $1/12$, $1/5$. Hence,

\[
\rho(\mathbf{M}_{GS}) = 0.2,
\]

and

\[
v_{GS} = -\log_{10}(0.2) = 0.6990.
\]"
87,"Choose the permutation matrix as}
\[
P = \begin{bmatrix}
0 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
\]

\subsection*{Then,}
\[
PAP^T = P \begin{bmatrix}
2 & -1 & 0 & 0 & 0 \\
-1 & 2 & -1 & 0 & 0 \\
0 & -1 & 2 & -1 & 0 \\
0 & 0 & -1 & 2 & -1 \\
0 & 0 & 0 & -1 & 2 \\
\end{bmatrix} P^T
\]



\[
= P \begin{bmatrix}
0 & -1 & 0 & 2 & 0 \\
0 & 2 & -1 & -1 & 0 \\
-1 & -1 & 2 & 0 & 0 \\
2 & 0 & -1 & 0 & -1 \\
-1 & 0 & 0 & 0 & 2 \\
\end{bmatrix} = \begin{bmatrix}
2 & 0 & \vert&-1 & 0 & -1 \\
0 & 2 & \vert&-1 & -1 & 0 \\
-&-&-&-&-&-&\\
-1 & -1 &\vert& 2 & 0 & 0 \\
0 & -1 & \vert&0 & 2 & 0 \\
-1 & 0 & \vert&0 & -1 & 2 \\
\end{bmatrix}
\]

Hence, the matrix \( A \) has 'property \( A' \). The Jacobi iteration matrix is

\[
H_J = \begin{bmatrix}
0 & -1 &\vert& -1/2 & 0 & -1/2 \\
0 & 0 &\vert& -1/2 & -1/2 & 0 \\
-&-&-&-&-&-&\\
-1/2 & -1/2 & \vert&0 & 0 & 0 \\
0 & -1/2 &\vert& 0 & 0 & 0\\
-1/2 & 0 &\vert& 0 & 0 & 0 \\
\end{bmatrix}
\]

The eigenvalues of \( H_J \) are \( \mu^* = 0, \pm 1/2, \pm \sqrt{3}/2 \). Therefore,

\[
\rho(H_J) = \sqrt{3}/2 = \mu.
\]

\[
\omega_{opt} = \frac{2}{1 + \sqrt{1 - \mu^2}} = \frac{4}{3}.
\]"
88,"The iteration matrix of the Jacobi method is given by
\[
\mathbf{M}_J = -\begin{bmatrix} 1/3 & 0 & 0 \\ 0 & 1/3 & 0 \\ 0 & 0 & 1/2 \end{bmatrix}^{-1} \begin{bmatrix} 0 & -2 & 0 \\ 2 & 0 & -1 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & -2/3 & 0 \\ -2/3 & 0 & 1/3 \\ 0 & 1/2 & 0 \end{bmatrix}
\]

Eigenvalues of \(\mathbf{M}_J\) are \(\lambda = 0, \pm \sqrt{11}/18\). Therefore
\[
\rho(\mathbf{M}_J) = \mu = \sqrt{11}/18.
\]

The optimal relaxation parameter for SOR method is obtained as
\[
\omega_{\text{opt}} = \frac{2}{1 + \sqrt{1 - \mu^2}} \approx 1.23183,
\]

\[
\rho(\text{SOR}) = \omega_{\text{opt}} - 1 \approx 0.23183.
\]

Hence, rate of convergence of SOR method is
\[
v(\text{SOR}) = -\log_{10} (0.23183) \approx 0.6348.
\]

The SOR iteration scheme can be written as
\[
\mathbf{x}^{n+1} = \mathbf{M} \mathbf{x}^{(n)} + \mathbf{c}
\]

where, with \(\omega = \omega_{\text{opt}} = 1.23183\), we have
\[
\mathbf{M} = (\mathbf{D} + \omega \mathbf{L})^{-1} [(1 - \omega) \mathbf{D} - \omega \mathbf{U}]
\]

\[
= \begin{bmatrix} 3 & 0 & 0 \\ 2.4636 & 3 & 0 \\ 0 & -1.2318 & 2 \end{bmatrix}^{-1} \begin{bmatrix} -0.6954 & -2.4636 & 0 \\ 0 & -0.6954 & 1.2318 \\ 0 & 0 & -0.4636 \end{bmatrix}
\]

\[
= \frac{1}{18} \begin{bmatrix} 6 & 0 & 0 \\ -4.9272 & 6 & 0 \\ -3.0347 & 3.6954 & 9 \end{bmatrix}^{-1} \begin{bmatrix} -0.6954 & -2.4636 & 0 \\ 0 & -0.6954 & 1.2318 \\ 0 & 0 & -0.4636 \end{bmatrix}
\]

\[
= \begin{bmatrix} -0.2318 & -0.8212 & 0 \\ 0.1904 & 0.4426 & 0.4106 \\ 0.1172 & 0.2726 & 0.0211 \end{bmatrix}
\]

and
\[
\mathbf{c} = \omega (\mathbf{D} + \omega \mathbf{L})^{-1} \mathbf{b}
\]

\[
= \frac{1.2318}{18} \begin{bmatrix} 6 & 0 & 0 \\ -4.9272 & 6 & 0 \\ -3.0347 & 3.6954 & 9 \end{bmatrix}^{-1} \begin{bmatrix} 4.5 \\ 5 \\ -0.5 \end{bmatrix} = \begin{bmatrix} 0.18477 \\ 0.5357 \\ 0.0220 \end{bmatrix}
\]

Hence, we have the iteration scheme
\[
\mathbf{x}^{(k+1)} = \begin{bmatrix} -0.2318 & -0.8212 & 0 \\ 0.1904 & 0.4426 & 0.4106 \\ 0.1172 & 0.2726 & 0.0211 \end{bmatrix} \mathbf{x}^{(k)} + \begin{bmatrix} 0.18477 \\ 0.5357 \\ 0.0220 \end{bmatrix}, \quad k = 0, 1, 2, \ldots
\]

Starting with \(\mathbf{x}^{(0)} = 0\), we obtain the following iterations:
\begin{align*}
\mathbf{x}^{(1)} &= [1.8477 \quad 0.5357 \quad 0.0220]^T \\
\mathbf{x}^{(2)} &= [0.9795 \quad 1.1336 \quad 0.3850]^T \\
\mathbf{x}^{(3)} &= [0.6898 \quad 1.3820 \quad 0.4539]^T \\
\mathbf{x}^{(4)} &= [0.5529 \quad 1.4651 \quad 0.4891]^T \\
\mathbf{x}^{(5)} &= [0.5164 \quad 1.4902 \quad 0.4965]^T
\end{align*}

\[
(\mathbf{D} + \omega_{\text{opt}} \mathbf{L}) \mathbf{v}^{(k+1)} = \omega_{\text{opt}} \mathbf{r}^{(k)}
\]

or
\[
(\mathbf{D} + 1.2318 \mathbf{L}) \mathbf{v}^{(k+1)} = 1.2318 \mathbf{r}^{(k)}
\]

where \(\mathbf{v}^{(k+1)} = \mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\), and \(\mathbf{r}^{(k)} = \mathbf{b} - \mathbf{A} \mathbf{x}^{(k)}\). We have
\[
\begin{bmatrix} 3 & 0 & 0 \\ 2.4636 & 3 & 0 \\ 0 & -1.2318 & 2 \end{bmatrix} \mathbf{v}^{(k+1)} = 1.2318 \begin{bmatrix} 4.5 - 3x^{(k)} - 2y^{(k)} \\ 5.0 - 2x^{(k)} - 3y^{(k)} + z^{(k)} \\ -0.5 + y^{(k)} - 2z^{(k)} \end{bmatrix}
\]

With \(\mathbf{x}^{(0)} = 0\), we obtain the following iterations. The equations are solved by forward substitution.

\begin{align*}
\text{First iteration:} \quad \mathbf{v}^{(1)} = \mathbf{x}^{(1)} &= [1.8477 \quad 0.5357 \quad 0.0220]^T \\
\text{Second iteration:} \quad \begin{bmatrix} 3 & 0 & 0 \\ 2.4636 & 3 & 0 \\ 0 & -1.2318 & 2 \end{bmatrix} \mathbf{v}^{(2)} &= \begin{bmatrix} -2.6046 \\ -0.3455 \\ -0.0102 \end{bmatrix} \\
\text{which gives,} \quad \mathbf{v}^{(2)} &= [-0.8682 \quad 0.5978 \quad 0.3631]^T, \\
\text{and} \quad \mathbf{x}^{(2)} = \mathbf{v}^{(2)} + \mathbf{x}^{(1)} &= [0.9795 \quad 1.1336 \quad 0.3850]^T \\
\text{Third iteration:} \quad \begin{bmatrix} 3 & 0 & 0 \\ 2.4636 & 3 & 0 \\ 0 & -1.2318 & 2 \end{bmatrix} \mathbf{v}^{(3)} &= \begin{bmatrix} -0.8690 \\ 0.0315 \\ -0.1684 \end{bmatrix} \\
\text{which gives,} \quad \mathbf{v}^{(3)} &= [-0.2897 \quad 0.2484 \quad 0.0688]^T, \\
\text{and} \quad \mathbf{x}^{(3)} = \mathbf{v}^{(3)} + \mathbf{x}^{(2)} &= [0.6898 \quad 1.3820 \quad 0.4539]^T \\
\text{Fourth iteration:} \quad \begin{bmatrix} 3 & 0 & 0 \\ 2.4636 & 3 & 0 \\ 0 & -1.2318 & 2 \end{bmatrix} \mathbf{v}^{(4)} &= \begin{bmatrix} -0.4104 \\ -0.0880 \\ -0.0319 \end{bmatrix} \\
\text{which gives,} \quad \mathbf{v}^{(4)} &= [-0.1368 \quad 0.0830 \quad 0.0352]^T, \\
\text{and} \quad \mathbf{x}^{(4)} = \mathbf{v}^{(4)} + \mathbf{x}^{(3)} &= [0.5530 \quad 1.4649 \quad 0.4891]^T \\
\text{Fifth iteration:} \quad \begin{bmatrix} 3 & 0 & 0 \\ 2.4636 & 3 & 0 \\ 0 & -1.2318 & 2 \end{bmatrix} \mathbf{v}^{(5)} &= \begin{bmatrix} -0.1094 \\ -0.0143 \\ -0.0164 \end{bmatrix} \\
\text{which gives,} \quad \mathbf{v}^{(5)} &= [-0.0365 \quad 0.0252 \quad 0.0073]^T, \\
\text{and} \quad \mathbf{x}^{(5)} = \mathbf{v}^{(5)} + \mathbf{x}^{(4)} &= [0.5164 \quad 1.4902 \quad 0.4965]^T
\end{align*}

Exact solution is \(\mathbf{x} = [0.5 \quad 1.5 \quad 0.5]^T\)."
89,"The largest off-diagonal element is \( a_{13} = a_{31} = 2 \). The other two elements in this \( 2 \times 2 \) submatrix are \( a_{11} = 1 \) and \( a_{33} = 1 \).

\[
\theta = \frac{1}{2} \tan^{-1} \left( \frac{4}{0} \right) = \pi/4
\]

\[
\mathbf{S}_1 = \begin{bmatrix}
1/\sqrt{2} & 0 & -1/\sqrt{2} \\
0 & 1 & 0 \\
1/\sqrt{2} & 0 & 1/\sqrt{2}
\end{bmatrix}
\]

\subsection*{The first rotation gives}
\[
\mathbf{B}_1 = \mathbf{S}_1^{-1} \mathbf{A} \mathbf{S}_1
\]

\[
= \begin{bmatrix}
1/\sqrt{2} & 0 & 1/\sqrt{2} \\
0 & 1 & 0 \\
-1/\sqrt{2} & 0 & 1/\sqrt{2}
\end{bmatrix} \begin{bmatrix}
1 & \sqrt{2} & 2 \\
\sqrt{2} & 3 & \sqrt{2} \\
2 & \sqrt{2} & 1
\end{bmatrix} \begin{bmatrix}
1/\sqrt{2} & 0 & -1/\sqrt{2} \\
0 & 1 & 0 \\
1/\sqrt{2} & 0 & 1/\sqrt{2}
\end{bmatrix}
\]

\[
= \begin{bmatrix}
3 & 2 & 0 \\
2 & 3 & 0 \\
0 & 0 & -1
\end{bmatrix}
\]

The largest off-diagonal element in magnitude in \(\mathbf{B}_1\) is \( a_{12} = a_{21} = 2 \). The other elements are \( a_{11} = 3 \), \( a_{22} = 3 \).

\[
\theta = \frac{1}{2} \tan^{-1} \left( \frac{4}{0} \right) = \pi/4
\]

\[
\mathbf{S}_2 = \begin{bmatrix}
1/\sqrt{2} & -1/\sqrt{2} & 0 \\
1/\sqrt{2} & 1/\sqrt{2} & 0 \\
0 & 0 & 1
\end{bmatrix}
\]

{The second rotation gives}
\[
\mathbf{B}_2 = \mathbf{S}_2^{-1} \mathbf{B}_1 \mathbf{S}_2 = \begin{bmatrix}
5 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & -1
\end{bmatrix}
\]

{We have the matrix of eigenvectors as}
\[
\mathbf{S} = \mathbf{S}_1 \mathbf{S}_2 = \begin{bmatrix}
1/\sqrt{2} & 0 & -1/\sqrt{2} \\
0 & 1 & 0 \\
1/\sqrt{2} & 0 & 1/\sqrt{2}
\end{bmatrix} \begin{bmatrix}
1/\sqrt{2} & -1/\sqrt{2} & 0 \\
1/\sqrt{2} & 1/\sqrt{2} & 0 \\
0 & 0 & 1
\end{bmatrix}
\]

\[
= \begin{bmatrix}
1/2 & -1/2 & -1/\sqrt{2} \\
1/\sqrt{2} & 1/\sqrt{2} & 0 \\
1/2 & 1/2 & 1/\sqrt{2}
\end{bmatrix}
\]

The eigenvalues are 5, 1, -1 and the corresponding eigenvectors are the columns of \(\mathbf{S}\)."
90,"This question illustrates the fact that in the Jacobi method, zeros once created may be disturbed and thereby the number of iterations required are increased. We have the following results.

\subsection*{First rotation}
Largest off diagonal element in magnitude \( a_{12} = 3 \),

\[
\tan 2\theta = -\frac{2 a_{12}}{a_{11} - a_{22}} = -\frac{6}{0}, \quad \theta = \pi/3
\]

\[
\mathbf{S}_1 = \begin{bmatrix}
0.707106781 & -0.707106781 & 0 \\
0.707106781 & 0.707106781 & 0 \\
0 & 0 & 1
\end{bmatrix}
\]

\[
\mathbf{A}_1 = \mathbf{S}_1^{-1} \mathbf{A} \mathbf{S}_1 = \mathbf{S}_1^T \mathbf{A} \mathbf{S}_1
\]

\[
= \begin{bmatrix}
5.0 & 0 & 2.121320343 \\
0 & -1.0 & 0.707106781 \\
2.121320343 & 0.707106781 & 1.0
\end{bmatrix}
\]

\subsection*{Second rotation}
Largest off diagonal element in magnitude \( a_{13} \),

\[
\tan 2\theta = \frac{2 a_{13}}{a_{11} - a_{33}} .
\]

We get
\[
\theta = 0.407413458,
\]

\[
\mathbf{S}_2 = \begin{bmatrix}
0.918148773 & 0 & -0.396235825 \\
0 & 1.0 & 0 \\
0.396235825 & 0 & 0.918148773
\end{bmatrix}
\]

\[
\mathbf{A}_2 = \mathbf{S}_2^T \mathbf{A}_1 \mathbf{S}_2
\]

\[
= \begin{bmatrix}
5.925475938 & 0.280181038 & 0.0 \\
0.280181038 & -1.0 & 0.649229223 \\
0.0 & 0.649229223 & 0.08452433
\end{bmatrix}
\]

Notice now that the zero in the (1, 2) position is disturbed. After six iterations, we get

\[
\mathbf{A}_6 = \mathbf{S}_6^T \mathbf{A}_5 \mathbf{S}_6
\]

\[
= \begin{bmatrix}
5.9269228 & -0.000089 & 0.0 \\
-0.00089 & -1.31255436 & 0.0 \\
0 & 0 & 0.38563102
\end{bmatrix}
\]

Hence, the approximate eigenvalues are 5.92692, -1.31255 and 0.38563. The orthogonal matrix of eigenvectors is given by \(\mathbf{S} = \mathbf{S}_1 \mathbf{S}_2 \mathbf{S}_3 \mathbf{S}_4 \mathbf{S}_5 \mathbf{S}_6\). We find that the corresponding eigenvectors are

\begin{align*}
\mathbf{x}_1 &= [-0.61853 \quad -0.67629 \quad -0.40007]^T, \\
\mathbf{x}_2 &= [0.54566 \quad -0.73605 \quad 0.40061]^T, \\
\mathbf{x}_3 &= [0.56540 \quad -0.29488 \quad -0.82429]^T.
\end{align*}"
91,"Perform the orthogonal rotation with respect to \( a_{22}, a_{23}, a_{32}, a_{33} \) submatrix. We get

\[
\tan \theta = \frac{a_{13}}{a_{12}} = \frac{3}{2}, \quad \cos \theta = \frac{2}{\sqrt{13}}, \quad \sin \theta = \frac{3}{\sqrt{13}}.
\]

Hence,
\[
\mathbf{B} = \mathbf{S}^{-1} \mathbf{M} \mathbf{S} = \mathbf{S}^T \mathbf{M} \mathbf{S}
\]

\[
= \begin{bmatrix}
1 & 0 & 0 \\
0 & 2/\sqrt{3} & 3/\sqrt{3} \\
0 & -3/\sqrt{13} & 2/\sqrt{13}
\end{bmatrix} \begin{bmatrix}
1 & 2 & 3 \\
2 & 1 & -1 \\
3 & -1 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0 \\
0 & 2/\sqrt{3} & -3/\sqrt{13} \\
0 & 3/\sqrt{3} & 2/\sqrt{13}
\end{bmatrix}
\]

\[
= \begin{bmatrix}
\sqrt{3} & 1/\sqrt{3} & 0 \\
1/\sqrt{3} & 5/\sqrt{3} & 0 \\
0 & 0 & 25/\sqrt{3}
\end{bmatrix}
\]

is the required tridiagonal form."
92,"Using the Given's method, we have
\[
\tan \theta = \frac{a_{13}}{a_{12}} = 1 \quad \text{or} \quad \theta = \frac{\pi}{4}
\]

\[
\mathbf{A}_1 = \mathbf{S}^{-1} \mathbf{A} \mathbf{S}
\]

\[
= \begin{bmatrix}
1 & 0 & 0 \\
0 & 1/\sqrt{2} & 1/\sqrt{2} \\
0 & -1/\sqrt{2} & 1/\sqrt{2}
\end{bmatrix} \begin{bmatrix}
1 & 2 & 2 \\
2 & 1 & 2 \\
2 & 2 & 1
\end{bmatrix} \begin{bmatrix}
1 & 0 & 0 \\
0 & 1/\sqrt{2} & -1/\sqrt{2} \\
0 & 1/\sqrt{2} & 1/\sqrt{2}
\end{bmatrix}
\]

\[
= \begin{bmatrix}
1 & 2\sqrt{2} & 0 \\
2\sqrt{2} & 3 & 0 \\
0 & 0 & -1
\end{bmatrix}
\]

which is the required tridiagonal form.

The characteristic equation of \(\mathbf{A}_1\) is given by
\[
f_n = |\lambda \mathbf{I} - \mathbf{A}_1| = \begin{vmatrix}
\lambda - 1 & -2\sqrt{2} & 0 \\
-2\sqrt{2} & \lambda - 3 & 0 \\
0 & 0 & \lambda + 1
\end{vmatrix} = 0
\]

The Sturm sequence \(\{ f_n \}\) is defined as
\begin{align*}
f_0 &= 1, \\
f_1 &= \lambda - 1, \\
f_2 &= (\lambda - 3) f_1 - (-2\sqrt{2})^2 f_0 = \lambda^2 - 4\lambda - 5, \\
f_3 &= (\lambda + 1) f_2 - (0)^2 f_1 = (\lambda + 1)(\lambda + 1)(\lambda - 5).
\end{align*}

Since \( f_3(-1) = 0 \) and \( f_3(5) = 0 \), the eigenvalues of \(\mathbf{A}\) are \(-1, -1\) and 5. The largest eigenvalue in magnitude is 5.

The eigenvector corresponding to \(\lambda = 5\) of \(\mathbf{A}_1\) is \(\mathbf{v}_1 = [1 \quad \sqrt{2} \quad 0]^T\).

Hence, the corresponding eigenvector of \(\mathbf{A}\) is
\[
\mathbf{v} = \mathbf{S} \mathbf{v}_1 = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}^T.
\]"
93,"Choose \(\mathbf{w}_2^T = [0 \quad x_2 \quad x_3]\) such that \(x_2^2 + x_3^2 = 1\). The parameters in the first Householder transformation are obtained as follows:

\[
s_1 = \sqrt{a_{12}^2 + a_{13}^2} = \sqrt{5},
\]

\[
x_2 = \frac{1}{2} \left[ 1 + \frac{a_{12}}{s_1} \text{sign}(a_{12}) \right] = \frac{1}{2} \left( 1 + \frac{2}{\sqrt{5}} \right) = \frac{\sqrt{5} + 2}{2\sqrt{5}},
\]

\[
x_3 = \frac{a_{13} \text{sign}(a_{12})}{2 s_1 x_2} = -\frac{1}{2 s_1 x_2},
\]

\[
x_2 x_3 = -\frac{1}{2\sqrt{5}},
\]

\[
\mathbf{P}_2 = \mathbf{I} - 2 \mathbf{w}_2 \mathbf{w}_2^T = \begin{bmatrix}
1 & 0 & 0 \\
0 & -2/\sqrt{5} & 1/\sqrt{5} \\
0 & 1/\sqrt{5} & 2/\sqrt{5}
\end{bmatrix}.
\]

The required Householder transformation is

\[
\mathbf{A}_2 = \mathbf{P}_2 \mathbf{A} \mathbf{P}_2 = \begin{bmatrix}
1 & -\sqrt{5} & 0 \\
-\sqrt{5} & -3/5 & -6/5 \\
0 & -6/5 & 13/5
\end{bmatrix}.
\]

Using the Given's method, we obtain the Sturm's sequence as
\begin{align*}
f_0 &= 1, \\
f_1 &= \lambda - 1, \\
f_2 &= \lambda^2 - \frac{2}{5} \lambda - \frac{28}{5}, \\
f_3 &= \lambda^3 - 3\lambda^2 - 6\lambda + 16.
\end{align*}

Let \( V(\lambda) \) denote the number of changes in sign in the Sturm sequence. We have the following table giving \( V(\lambda) \)

\begin{tabular}{cccccc}
\toprule
\(\lambda\) & \(f_0\) & \(f_1\) & \(f_2\) & \(f_3\) & \(V(\lambda)\) \\
\midrule
\(-3\) & \(+\) & \(- \) & \(+\) & \(- \) & 3 \\
\(-2\) & \(+\) & \(- \) & \(- \) & \(+\) & 2 \\
\(-1\) & \(+\) & \(- \) & \(- \) & \(+\) & 2 \\
\(0\) & \(+\) & \(- \) & \(- \) & \(+\) & 2 \\
\(1\) & \(+\) & \(+\) & \(- \) & \(+\) & 2 \\
\(2\) & \(+\) & \(+\) & \(- \) & 0 & 1 \\
\(3\) & \(+\) & \(+\) & \(+\) & \(- \) & 1 \\
\(4\) & \(+\) & \(+\) & \(+\) & \(+\) & 0 \\
\bottomrule
\end{tabular}

Since \( f_3 = 0 \) for \(\lambda = 2\), \(\lambda = 2\) is an eigenvalue. The remaining two eigenvalues lie in the intervals \((-3, -2)\) and \((3, 4)\). Repeated bisection and application of the Sturm's theorem gives the eigenvalues as \(\lambda_2 = -2.372\) and \(\lambda_3 = 3.372\). Exact eigenvalues are \(2, (1 \pm \sqrt{33})/2\)."
94,"\subsection*{First transformation:}
\[
\mathbf{w}_2 = [0 \quad x_2 \quad x_3 \quad x_4]^T,
\]

\[
s_1 = \sqrt{a_{12}^2 + a_{13}^2 + a_{14}^2} = \sqrt{3},
\]

\[
x_2 = \frac{1}{2} \left[ 1 + \frac{(-1)}{3} \right] = \frac{2}{3}, \quad x_2 = \frac{\sqrt{2}}{\sqrt{3}},
\]

\[
x_3 = \frac{-2}{2(3) \frac{\sqrt{2}}{\sqrt{3}}} = -\frac{1}{\sqrt{6}}, \quad x_4 = -\frac{1}{\sqrt{6}},
\]

\[
\mathbf{P}_2 = \mathbf{I} - 2 \mathbf{w}_2 \mathbf{w}_2^T = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -2/3 & -2/3 & 2/3 \\
0 & -2/3 & 1/3 & 2/3 \\
0 & 2/3 & 2/3 & 1/3
\end{bmatrix},
\]

\[
\mathbf{A}_2 = \mathbf{P}_2 \mathbf{A} \mathbf{P}_2 = \begin{bmatrix}
4 & 3 & 0 & 0 \\
3 & 16/3 & 2/3 & 1/3 \\
0 & 2/3 & 16/3 & -1/3 \\
0 & 1/3 & -1/3 & 4/3
\end{bmatrix}.
\]

\subsection*{Second transformation:}
\[
\mathbf{w}_3 = [0 \quad 0 \quad x_3 \quad x_4]^T,
\]

\[
s_1 = \sqrt{a_{23}^2 + a_{24}^2} = \frac{\sqrt{5}}{3},
\]

\[
x_3 = \frac{1}{2} \left[ 1 + \frac{2/3}{\sqrt{5}/3} \right] = \frac{\sqrt{5} + 2}{2\sqrt{5}} = a,
\]

\[
x_4 = 1 - x_3^2 = 1 - \frac{\sqrt{5} + 2}{2\sqrt{5}} = \frac{\sqrt{5} - 2}{2\sqrt{5}} = \frac{1}{20a},
\]

\[
\mathbf{P}_3 = \mathbf{I} - 2 \mathbf{w}_3 \mathbf{w}_3^T = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 - 2a & -1/\sqrt{5} \\
0 & 0 & -1/\sqrt{5} & 1 - 1/(10a)
\end{bmatrix},
\]

\[
\mathbf{A}_3 = \mathbf{P}_3 \mathbf{A}_2 \mathbf{P}_3 = \begin{bmatrix}
4 & 3 & 0 & 0 \\
3 & 16/3 & -5/(3\sqrt{5}) & 0 \\
0 & -5/(3\sqrt{5}) & 16/3 & 9/5 \\
0 & 0 & 9/5 & 12/5
\end{bmatrix}
\]

is the required tridiagonal form."
95,"Using the Rutishauser method. Apply the procedure until the elements of the lower triangular part are less than 0.005 in magnitude. We have the following decompositions:

\[
A_1 = A = L_1 U_1 = \begin{bmatrix}
1 & 0 \\
1/3 & 1
\end{bmatrix} \begin{bmatrix}
3 & 1 \\
0 & 2/3
\end{bmatrix}
\]

\[
A_2 = U_1 L_1 = \begin{bmatrix}
10/3 & 1 \\
2/9 & 2/3
\end{bmatrix}
\]

\[
= L_2 U_2 = \begin{bmatrix}
1 & 0 \\
1/15 & 1
\end{bmatrix} \begin{bmatrix}
10/3 & 1 \\
0 & 3/5
\end{bmatrix}
\]

\[
A_3 = U_2 L_2 = \begin{bmatrix}
17/5 & 1 \\
1/25 & 3/5
\end{bmatrix}
\]

\[
= L_3 U_3 = \begin{bmatrix}
1 & 0 \\
1/85 & 1
\end{bmatrix} \begin{bmatrix}
17/5 & 1 \\
0 & 10/17
\end{bmatrix}
\]
\[
A_4 = U_3 L_3 = \begin{bmatrix}
\frac{58}{17} & 1 \\
\frac{2}{289} & \frac{10}{17}
\end{bmatrix}
\]

\[
= L_4 U_4 = \begin{bmatrix}
1 & 0 \\
\frac{1}{493} & 1
\end{bmatrix} \begin{bmatrix}
\frac{58}{17} & 1 \\
0 & \frac{289}{493}
\end{bmatrix}
\]

\[
A_5 = U_4 L_4 = \begin{bmatrix}
3.4138 & 1 \\
0.0012 & 0.5862
\end{bmatrix}
\]

To the required accuracy the eigenvalues are 3.4138 and 0.5862. The exact eigenvalues are \(2 \pm \sqrt{2}\)."
96,"Starting with \( \mathbf{v}_0 = [1 \, 1 \, 1 \, 1]^T \) and using the algorithm for power method we obtain
\[
\mathbf{y}_1 = A \mathbf{v}_0 = [4 \, 3 \, 3 \, 4]^T,
\]
\[
\mathbf{v}_1 = \frac{\mathbf{y}_1}{n_1} = [1 \, 3/4 \, 3/4 \, 1]^T,
\]
\[
\mathbf{y}_2 = A \mathbf{v}_1 = [7/2 \, 11/4 \, 11/4 \, 7/2]^T,
\]
\[
\mathbf{v}_2 = \frac{\mathbf{y}_2}{n_2} = [1 \, 11/14 \, 11/14 \, 1]^T,
\]
\[
\mathbf{y}_3 = A \mathbf{v}_2 = [25/7 \, 39/14 \, 39/14 \, 25/7]^T,
\]
\[
\mathbf{v}_3 = \frac{\mathbf{y}_3}{n_3} = [1 \, 39/50 \, 39/50 \, 1]^T,
\]
\[
\ldots
\]
\[
\mathbf{y}_5 = A \mathbf{v}_4 = [317/89 \, 495/178 \, 495/178 \, 317/89]^T,
\]
\[
\mathbf{v}_5 = [1 \, 495/634 \, 495/634 \, 1]^T,
\]
\[
\mathbf{y}_6 = A \mathbf{v}_5 = [1129/317 \, 1763/634 \, 1763/634 \, 1129/317]^T,
\]
\[
\mathbf{v}_6 = [1 \, 1763/2258 \, 1763/2258 \, 1]^T,
\]

After six iterations, the ratios
\[
(\mathbf{y}_6)_r / (\mathbf{v}_5)_r, r = 1, 2, 3, 4
\]
are 3.5615, 3.5616, 3.5616 and 3.5615. Hence, the largest eigenvalue in magnitude is 3.5615. The corresponding eigenvector is [1 \, 0.7808 \, 0.7808 \, 1]^T."
97,"Starting with \( \mathbf{v}_0 = [1 \, 1 \, 1]^T \) and using power method we obtain the following:
\[
\mathbf{y}_1 = A \mathbf{v}_0 = [5 \, 22 \, 5]^T,
\]
\[
\mathbf{v}_1 = \frac{\mathbf{y}_1}{m_1} = [5/22 \, 1 \, 5/22]^T,
\]
\[
\mathbf{y}_2 = A \mathbf{v}_1 = [21/11 \, 225/11 \, 21/11]^T,
\]
\[
\mathbf{v}_2 = \frac{\mathbf{y}_2}{m_2} = [21/225 \, 1 \, 21/225]^T,
\]
\[
\ldots
\]
\[
\mathbf{y}_7 = A \mathbf{v}_6 = [1.24806 \, 20.12412 \, 1.24824]^T,
\]
\[
\mathbf{v}_7 = \frac{\mathbf{y}_7}{m_7} = [0.06202 \, 1 \, 0.06202]^T,
\]
\[
\mathbf{y}_8 = A \mathbf{v}_7 = [1.24806 \, 20.12404 \, 1.24806]^T,
\]
\[
\mathbf{v}_8 = \frac{\mathbf{y}_8}{m_8} = [0.06202 \, 1 \, 0.06202]^T,
\]

After 8 iterations, the ratios \( (\mathbf{y}_8)_r / (\mathbf{v}_7)_r, r = 1, 2, 3 \) are 20.1235, 20.1240 and 20.1235. The largest eigenvalue in magnitude correct to 3 decimal places is 20.124 and the corresponding eigenvector is
\[
[0.062 \, 1 \, 0.062]^T.
\]"
98,"Starting with \( \mathbf{v}_0 = [1 \, 1 \, 1 \, 1 \, 1]^T \) and using the power method, we obtain the following:
\[
\mathbf{y}_1 = A \mathbf{v}_0 = [2 \, 2 \, 3 \, 2 \, 3]^T,
\]
\[
\mathbf{v}_1 = \frac{\mathbf{y}_1}{m_1} = [0.666667 \, 0.666667 \, 1 \, 0.666667 \, 1]^T,
\]
\[
\mathbf{y}_2 = A \mathbf{v}_1 = [1.666667 \, 2 \, 2.333334 \, 1.666667 \, 2.333334]^T,
\]
\[
\mathbf{v}_2 = \frac{\mathbf{y}_2}{m_2} = [0.714286 \, 0.857143 \, 1 \, 0.714286 \, 1]^T,
\]
\[
\ldots
\]
\[
\mathbf{y}_{13} = A \mathbf{v}_{12} = [1.675145 \, 2 \, 2.481239 \, 1.675145 \, 2.481239]^T,
\]
\[
\mathbf{v}_{13} = \frac{\mathbf{y}_{13}}{m_{13}} = [0.675124 \, 0.806049 \, 1 \, 0.675124 \, 1]^T,
\]
\[
\mathbf{y}_{14} = A \mathbf{v}_{13} = [1.675124 \, 2 \, 2.481173 \, 1.675124 \, 2.481173]^T,
\]
\[
\mathbf{v}_{14} = \frac{\mathbf{y}_{14}}{m_{14}} = [0.675124 \, 0.806070 \, 1 \, 0.675134 \, 1]^T,
\]

After 14 iterations, the ratios \( (\mathbf{y}_{14})_r / (\mathbf{v}_{13})_r, r = 1, 2, 3, 4, 5 \) are 2.481209, 2.481238, 2.481173, 2.481238 and 2.481173. Hence, the largest eigenvalue in magnitude may be taken as 2.4812."
99,"The smallest eigenvalue in magnitude of \( A \) is the largest eigenvalue in magnitude of \( A^{-1} \). We have
\[
A^{-1} = \begin{bmatrix}
3/4 & 1/2 & 1/4 \\
1/2 & 1 & 1/2 \\
1/4 & 1/2 & 3/4
\end{bmatrix}
\]

Using
\[
\mathbf{y}^{(k+1)} = A^{-1} \mathbf{v}^{(k)}, \, k = 0, 1, \ldots
\]
and
\[
\mathbf{v}^{(0)} = [1, \, 1, \, 1]^T, \, \text{we obtain}
\]
\[
\mathbf{y}^{(1)} = [1.5, \, 2, \, 1.5]^T, \, \mathbf{v}^{(1)} = [0.75, \, 1, \, 0.75]^T
\]
\[
\mathbf{y}^{(2)} = [1.25, \, 1.75, \, 1.25]^T, \, \mathbf{v}^{(2)} = [0.7143, \, 1, \, 0.7143]^T
\]
\[
\mathbf{y}^{(3)} = [1.2143, \, 1.7143, \, 1.2143]^T, \, \mathbf{v}^{(3)} = [0.7083, \, 1, \, 0.7083]^T
\]
\[
\mathbf{y}^{(4)} = [1.2083, \, 1.7083, \, 1.2083]^T, \, \mathbf{v}^{(4)} = [0.7073, \, 1, \, 0.7073]^T
\]

After four iterations, we obtain the ratios as
\[
\mu = \frac{[\mathbf{y}^{(4)}]_r}{[\mathbf{v}^{(3)}]_r} = (1.7059, \, 1.7083 \, 1.7059).
\]

Therefore, \(\mu = 1.71\) and \(\lambda = 1 / \mu \approx 0.5848\). Since \(|A - 0.5848 I| \approx 0\), \(\lambda = 0.5848\) is the required eigenvalue. The corresponding eigenvector is \([0.7073, \, 1, \, 0.7073]^T\).

The smallest eigenvalue of \(A\) is \(2 - \sqrt{2} = 0.5858\).

Alternatively, we can write
\[
A \mathbf{y}^{(k+1)} = \mathbf{v}^{(k)}, \, k = 0, 1, \ldots
\]
or
\[
\begin{bmatrix}
1 & 0 & 0 \\
-1/2 & 1 & 0 \\
0 & -2/3 & 1
\end{bmatrix}
\begin{bmatrix}
2 & -1 & 0 \\
0 & 3/2 & -1 \\
0 & 0 & 4/3
\end{bmatrix}
\mathbf{y}^{(k+1)} = \mathbf{v}^{(k)}
\]

Writing the above system as
\[
L \mathbf{z}^{(k)} = \mathbf{v}^{(k)} \quad \text{and} \quad U \mathbf{y}^{(k+1)} = \mathbf{z}^{(k)}
\]
we obtain for
\[
\mathbf{v}^{(0)} = [1, \, 1, \, 1]^T, \, \mathbf{z}^{(0)} = [1, \, 1.5, \, 2]^T, \, \mathbf{y}^{(1)} = [1.5, \, 2, \, 1.5]^T.
\]
We obtain the same successive iterations as before."
100,"The eigenvalue of \(A\) which is nearest to 3 is the smallest eigenvalue (in magnitude) of \(A - 3I\). Hence it is the largest eigenvalue (in magnitude) of \((A - 3I)^{-1}\). We have
\[
A - 3I = \begin{bmatrix}
-1 & -1 & 0 \\
-1 & -1 & -1 \\
0 & -1 & -1
\end{bmatrix}, \quad (A - 3I)^{-1} = \begin{bmatrix}
-1 & 1 & -1 \\
1 & -1 & 0 \\
1 & 0 & 0
\end{bmatrix}
\]

Using \(\mathbf{y}^{(k+1)} = (A - 3I)^{-1} \mathbf{v}^{(k)}, k = 0, 1, \ldots\) and \(\mathbf{v}^{(0)} = [1, \, 1, \, 1]^T\), we obtain
\[
\mathbf{y}^{(1)} = [0, \, -1, \, 0]^T, \quad \mathbf{v}^{(1)} = [0, \, -1, \, 0]^T
\]
\[
\mathbf{y}^{(2)} = [1, \, -1, \, 1]^T, \quad \mathbf{v}^{(2)} = [1, \, -1, \, 1]^T
\]
\[
\mathbf{y}^{(3)} = [2, \, -3, \, 2]^T, \quad \mathbf{v}^{(3)} = [0.6667, \, -1, \, 0.6667]^T
\]
\[
\mathbf{y}^{(4)} = [1.6667, \, -2.3334, \, 1.6667]^T, \quad \mathbf{v}^{(4)} = [0.7143, \, -1, \, 0.7143]^T
\]
\[
\mathbf{y}^{(5)} = [1.7143, \, -2.4286, \, 1.7143]^T
\]

After five iterations, we obtain the ratios as
\[
\mu = \frac{[\mathbf{y}^{(5)}]_r}{[\mathbf{v}^{(4)}]_r} = [2.4000, \, 2.43, \, 2.4000].
\]

Therefore, \(\mu = 2.4\) and \(\lambda = 3 \pm (1 / \mu) = 3 \pm 0.42\). Since \(\lambda = 2.58\) does not satisfy \(|A - 2.58 I| = 0\), the correct eigenvalue nearest to 3 is 3.42 and the corresponding eigenvector is \([0.7143, \, -1, \, 0.7143]^T\). The exact eigenvalues of \(A\) are \(2 + \sqrt{2} = 3.42\), \(2\) and \(2 - \sqrt{2} \approx 0.59\)."
